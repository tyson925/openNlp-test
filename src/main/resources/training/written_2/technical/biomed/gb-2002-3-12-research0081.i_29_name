
  
    
      
        Rationale
        The information held in genomic sequence is encoded and
        highly compressed; to extract biologically interesting data
        we must decrypt this primary data computationally. This
        generates results that provide a measure of biologically
        relevant characteristics, such as coding potential or
        sequence similarity, present in the sequence. Because of
        the amount of sequence to be examined and the volume of
        <ENAMEX TYPE="ORGANIZATION">data</ENAMEX> generated, these results must be automatically
        processed and carefully filtered.
        There are essentially <NUMEX TYPE="CARDINAL">three</NUMEX> different strategies for
        whole-genome analysis. The <NUMEX TYPE="ORDINAL">first</NUMEX> is a purely automatic
        synthesis from a combination of analyses to predict gene
        models. The <NUMEX TYPE="ORDINAL">second</NUMEX> aggregates analyses contributed by the
        research <ENAMEX TYPE="PER_DESC">community</ENAMEX> that the <ENAMEX TYPE="PER_DESC">user</ENAMEX> is then required to
        integrate visually on a public website. The <NUMEX TYPE="ORDINAL">third</NUMEX> is
        curation by <ENAMEX TYPE="PER_DESC">experts</ENAMEX> using a full trail of evidence to
        support an integrated assessment. Several <ENAMEX TYPE="ORG_DESC">groups</ENAMEX> charged
        with rapidly providing a dispersed <ENAMEX TYPE="PER_DESC">community</ENAMEX> with genome
        annotations have chosen the purely computational route;
        examples are <ENAMEX TYPE="PRODUCT">Ensembl</ENAMEX> [ <ENAMEX TYPE="LAW">1</ENAMEX>] and the <ENAMEX TYPE="ORGANIZATION">National Center for</ENAMEX>
        Biotechnology Information (NCBI) [ <ENAMEX TYPE="LAW">2</ENAMEX>]. Approaches using
        aggregation adapt well to the dynamics of collaborative
        <ENAMEX TYPE="ORG_DESC">groups</ENAMEX> which are focused on sharing results as they accrue;
        examples are the <ENAMEX TYPE="ORGANIZATION">University of California</ENAMEX> <ENAMEX TYPE="GPE">Santa Cruz</ENAMEX> (UCSC)
        genome browser [ <ENAMEX TYPE="LAW">3</ENAMEX>] and the <ENAMEX TYPE="ORGANIZATION">Distributed Annotation System</ENAMEX>
        (<ENAMEX TYPE="ORGANIZATION">DAS</ENAMEX>) [ <ENAMEX TYPE="LAW">4</ENAMEX>]. For <ENAMEX TYPE="ANIMAL">organisms</ENAMEX> with well established and
        cohesive communities the demand is for carefully reviewed
        and qualified <ENAMEX TYPE="FAC_DESC">annotations</ENAMEX>; this approach was adopted by
        <NUMEX TYPE="CARDINAL">three</NUMEX> of the oldest <ENAMEX TYPE="PER_DESC">genome-community</ENAMEX> databases, <ENAMEX TYPE="ORGANIZATION">SGD</ENAMEX> for 
        Saccharomyces cerevisiae [ <ENAMEX TYPE="LAW">5</ENAMEX>], ACeDB
        for 
        Caenorhabditis
        <ENAMEX TYPE="ORGANIZATION">elegans</ENAMEX> (documentation, code and data available from
        <ENAMEX TYPE="ORGANIZATION">anonymous FTP</ENAMEX> servers at [ <ENAMEX TYPE="LAW">6</ENAMEX>]) and <ENAMEX TYPE="ORGANIZATION">FlyBase</ENAMEX> for 
        Drosophila melanogaster [ <ENAMEX TYPE="LAW">7</ENAMEX>].
        We decided to examine every <ENAMEX TYPE="SUBSTANCE">gene</ENAMEX> and feature of the 
        Drosophila genome and manually
        improve the quality of the annotations [ <ENAMEX TYPE="LAW">8</ENAMEX>]. The
        prerequisites for this are: <NUMEX TYPE="ORDINAL">first</NUMEX>, a computational pipeline
        and a database capable of both monitoring the <ENAMEX TYPE="FAC_DESC">pipeline</ENAMEX>'s
        progress and storing the raw analysis; <NUMEX TYPE="ORDINAL">second</NUMEX>, an
        additional database to provide the <ENAMEX TYPE="PER_DESC">curators</ENAMEX> with a
        complete, compact and salient collection of evidence and to
        store the <ENAMEX TYPE="ORG_DESC">annotations</ENAMEX> generated by the <ENAMEX TYPE="PER_DESC">curators</ENAMEX>; and <NUMEX TYPE="ORDINAL">third</NUMEX>,
        an editing tool for the <ENAMEX TYPE="PER_DESC">curators</ENAMEX> to create and edit
        <ENAMEX TYPE="ORGANIZATION">annotations</ENAMEX> based on this evidence. This paper discusses
        our solution for the <NUMEX TYPE="ORDINAL">first</NUMEX> <NUMEX TYPE="CARDINAL">two</NUMEX> requirements. The editing
        tool used, <ENAMEX TYPE="ORGANIZATION">Apollo</ENAMEX>, is described in an accompanying paper [
        <NUMEX TYPE="CARDINAL">9</NUMEX>].
        Our primary design requirement was flexibility. This was
        to ensure that the <ENAMEX TYPE="FAC_DESC">pipeline</ENAMEX> could easily be tuned to the
        needs of the <ENAMEX TYPE="PER_DESC">curators</ENAMEX>. We use <NUMEX TYPE="CARDINAL">two</NUMEX> distinct databases with
        different schemata to decouple the <ENAMEX TYPE="PER_DESC">management</ENAMEX> of the
        sequence workflow from the sequence annotation data itself.
        Our long-term goal is to provide a set of open-source
        software tools to support large-scale genome
        <ENAMEX TYPE="ORGANIZATION">annotation</ENAMEX>.
      
      
        Sequence datasets
        The sequence datasets are the primary input into the
        pipeline. These fall into <NUMEX TYPE="CARDINAL">three</NUMEX> categories: the 
        <ENAMEX TYPE="NATIONALITY">D.</ENAMEX> melanogaster genomic sequence;
        expressed sequences from 
        <ENAMEX TYPE="NATIONALITY">D.</ENAMEX> melanogaster ; and informative
        sequences from other <ENAMEX TYPE="ANIMAL">species</ENAMEX>.
        <ENAMEX TYPE="PRODUCT">Release 3</ENAMEX> of the 
        <ENAMEX TYPE="NATIONALITY">D.</ENAMEX> melanogaster genomic sequence was
        generated using bacterial artificial chromosome (BAC)
        clones that formed a complete tiling <ENAMEX TYPE="FAC_DESC">path</ENAMEX> across the
        genome, as well as whole-genome <ENAMEX TYPE="PRODUCT_DESC">shotgun sequencing</ENAMEX> reads [
        <NUMEX TYPE="CARDINAL">10</NUMEX>]. This genomic sequence was 'frozen' when, during
        sequence finishing, there was sufficient improvement in the
        quality to justify a new 'release'. This provided a stable
        underlying sequence for annotation.
        In general, the accuracy and scalability of
        <ENAMEX TYPE="PERSON">gene</ENAMEX>-prediction and similarity-search programs is such that
        computing on <NUMEX TYPE="MONEY">20 million</NUMEX> <ENAMEX TYPE="ORG_DESC">base</ENAMEX> (<ENAMEX TYPE="PRODUCT">Mb</ENAMEX>) chromosome <ENAMEX TYPE="PRODUCT_DESC">arms</ENAMEX> is
        <ENAMEX TYPE="PERSON">ill-advised</ENAMEX>, and we therefore cut the finished genomic
        sequence into smaller <ENAMEX TYPE="ORG_DESC">segments</ENAMEX>. Ideally, we would have
        broken the genome down into sequence <ENAMEX TYPE="ORG_DESC">segments</ENAMEX> containing
        individual <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> or a small number of genes. Before the
        <NUMEX TYPE="ORDINAL">first</NUMEX> round of annotation, however, this was not possible
        for the simple reason that the position of the <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> was as
        yet unknown. Therefore, we began the process of annotation
        using a non-biological breakdown of the sequence. We
        considered two possibilities for the initial sequence
        segments, either individual <ENAMEX TYPE="ORGANIZATION">BACs</ENAMEX> or the <ENAMEX TYPE="ORG_DESC">segments</ENAMEX> that
        comprise the public database accessions. We rejected the
        use of individual <ENAMEX TYPE="ORGANIZATION">BAC</ENAMEX> sequences and chose to use the
        GenBank accessions as the main sequence <ENAMEX TYPE="ORG_DESC">unit</ENAMEX> for our
        <ENAMEX TYPE="ORGANIZATION">genomic</ENAMEX> <ENAMEX TYPE="FAC_DESC">pipeline</ENAMEX> because the <ENAMEX TYPE="ORGANIZATION">BACs</ENAMEX> are physical clones with
        physical breaks, while the <ENAMEX TYPE="ORGANIZATION">GenBank</ENAMEX> accession can
        subsequently be refined to respective biological <ENAMEX TYPE="ORG_DESC">entities</ENAMEX>.
        At <NUMEX TYPE="MONEY">around 270 kilobases</NUMEX> (kb), these are manageable by most
        analysis programs and provide a convenient <ENAMEX TYPE="ORG_DESC">unit</ENAMEX> of work for
        the <ENAMEX TYPE="PER_DESC">curators</ENAMEX>. To minimize the problem of genes straddling
        these arbitrary <ENAMEX TYPE="ORG_DESC">units</ENAMEX> we first fed the <ENAMEX TYPE="ORGANIZATION">BAC</ENAMEX> sequences into a
        lightweight version of the full annotation <ENAMEX TYPE="FAC_DESC">pipeline</ENAMEX> that
        estimated the positions of genes. We then projected the
        coordinates of these predicted <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> from the <ENAMEX TYPE="ORGANIZATION">BAC</ENAMEX> clones
        onto the full-arm sequence <ENAMEX TYPE="ORG_DESC">assembly</ENAMEX>. This step was followed
        by the use of another in-house software tool to divide up
        the <ENAMEX TYPE="ORG_DESC">arm</ENAMEX> sequence, trying to simultaneously optimize two
        constraints: to avoid the creation of gene models that
        straddle the boundaries between <NUMEX TYPE="CARDINAL">two</NUMEX> accessions; and to
        maintain a close correspondence to the pre-existing Release
        <NUMEX TYPE="CARDINAL">2</NUMEX> accessions in <ENAMEX TYPE="ORGANIZATION">GenBank/EMBL/DDBJ</ENAMEX> [ <TIMEX TYPE="DATE">11, 12, 13</TIMEX>]. During the
        annotation process, if a <ENAMEX TYPE="PER_DESC">curator</ENAMEX> discovered that a unit
        broke a gene, they requested an appropriate extension of
        the accession prior to further annotation. In hindsight we
        have realized that we should have focused solely on
        minimizing gene breaks because further adjustments by
        GenBank were still needed to ensure that, as much as
        possible, <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> remained on the same sequence
        <ENAMEX TYPE="PERSON">accession</ENAMEX>.
        To re-annotate a genome in sufficient detail, an
        extensive set of additional sequences is necessary to
        generate sequence alignments and search for homologous
        <ENAMEX TYPE="ORGANIZATION">sequences</ENAMEX>. In the case of this project, these sequence
        datasets included assembled full-insert cDNA sequences,
        expressed sequence tags (ESTs), and cDNA sequence reads
        from 
        <ENAMEX TYPE="NATIONALITY">D.</ENAMEX> melanogaster as well as peptide,
        cDNA, and <TIMEX TYPE="TIME">EST</TIMEX> sequences from other <ENAMEX TYPE="ANIMAL">species</ENAMEX>. The sequence
        datasets we used are listed in Figure <NUMEX TYPE="CARDINAL">1and</NUMEX> described more
        fully in [ <ENAMEX TYPE="LAW">8</ENAMEX>].
      
      
        Software for task monitoring and scheduling the
        computational pipeline
        There are <NUMEX TYPE="CARDINAL">three</NUMEX> major infrastructure components of the
        pipeline: the database, the <ENAMEX TYPE="ORGANIZATION">Perl</ENAMEX> module (named <ENAMEX TYPE="ORGANIZATION">Pipeline</ENAMEX>),
        and sufficient computational power, allocated by a
        job-management system. The database is crucial because it
        maintains a persisting record reflecting the current state
        of all the tasks that are in progress. Maintaining the
        <ENAMEX TYPE="PERSON">jobs</ENAMEX>, job parameters and job output in a database avoids
        some of the inherent limitations of a file-system approach.
        It is easier to update, provides a built-in querying
        language and offers many other data-management tools that
        make the system more robust. We used a <ENAMEX TYPE="ORGANIZATION">MySQL</ENAMEX> [ <TIMEX TYPE="DATE">14</TIMEX>] database
        to manage the large number of analyses run against the
        genome, transcriptome and proteome (see below).
        <ENAMEX TYPE="ORGANIZATION">MySQL</ENAMEX> is an open-<ENAMEX TYPE="PER_DESC">source</ENAMEX> 'structured query language'
        (<ENAMEX TYPE="ORGANIZATION">SQL</ENAMEX>) database that, despite having a limited set of
        features, has the advantage of being fast, free, and simple
        to maintain. <ENAMEX TYPE="ORGANIZATION">SQL</ENAMEX> is a database query language that was
        adopted as an industry standard in <TIMEX TYPE="DATE">1986</TIMEX>. An SQL database
        manages data as a collection of tables. Each table has a
        fixed set of columns (also called fields) and usually
        <ENAMEX TYPE="ORGANIZATION">corresponds</ENAMEX> to a particular concept in the domain being
        modeled. Tables can be cross-referenced by using primary
        and foreign key fields. The database tables can be queried
        using the <ENAMEX TYPE="ORGANIZATION">SQL</ENAMEX> language, which allows the dynamic
        combination of data from different tables [ <TIMEX TYPE="DATE">15</TIMEX>]. A
        collection of these tables is called a database schema, and
        a particular instantiation of that schema with the tables
        populated is a database. The <ENAMEX TYPE="ORGANIZATION">Perl</ENAMEX> modules provide an
        application <ENAMEX TYPE="PER_DESC">programmer interface</ENAMEX> (API) that is used to
        launch and monitor jobs, retrieve results and support other
        interactions with the database.
        There are <NUMEX TYPE="CARDINAL">four</NUMEX> basic abstractions that all components of
        the pipeline system operate upon: a sequence, a job, an
        analysis and a batch. A 'sequence' is defined as a string
        of amino or <ENAMEX TYPE="SUBSTANCE">nucleic acids</ENAMEX> held either in the database or as
        an entry in a FASTA file (usually both). A 'job' is an
        instance of a particular program being run to analyze a
        particular sequence, for example running <ENAMEX TYPE="ORGANIZATION">BLASTX</ENAMEX> to compare
        one sequence to a peptide set is considered a single job.
        Jobs can be chained together. If job A is dependent on the
        output of job B, then the pipeline <ENAMEX TYPE="ORG_DESC">software</ENAMEX> will not launch
        job A until job B is complete. This situation occurs, for
        example, with programs that require masked sequence as
        input. An 'analysis' is a collection of jobs using the same
        program and parameters against a set of sequences. Lastly,
        a 'batch' is a collection of analyses a user launches
        simultaneously. <ENAMEX TYPE="PERSON">Jobs</ENAMEX>, analyses and batches all have a
        <ENAMEX TYPE="ORGANIZATION">'status</ENAMEX>' attribute that is used to track their progress
        through the <ENAMEX TYPE="FAC_DESC">pipeline</ENAMEX> (Figure <NUMEX TYPE="CARDINAL">2</NUMEX>).
        The <NUMEX TYPE="CARDINAL">three</NUMEX> applications that use the <ENAMEX TYPE="ORGANIZATION">Perl API</ENAMEX> are the
        <ENAMEX TYPE="ORGANIZATION">pipe_launcher</ENAMEX> script, the flyshell interactive command line
        <ENAMEX TYPE="PERSON">interpreter</ENAMEX>, and the internet front end [ <TIMEX TYPE="DATE">16</TIMEX>]. Both
        <ENAMEX TYPE="ORGANIZATION">pipe_launcher</ENAMEX> and flyshell provide pipeline <ENAMEX TYPE="ORG_DESC">users</ENAMEX> with a
        variety of powerful ways to launch and monitor jobs,
        <ENAMEX TYPE="PERSON">analyses</ENAMEX> and batches. These tools are useful to those with
        a basic understanding of Unix and bioinformatics tools, as
        well as those with a good knowledge of object-oriented
        <ENAMEX TYPE="ORGANIZATION">Peri</ENAMEX>. The web front end is used for monitoring the progress
        of the jobs in the pipeline.
        The <ENAMEX TYPE="FAC_DESC">pipe_launcher</ENAMEX> application is a command-line tool
        used to launch jobs. Users create configuration files that
        specify input data sources and any number of analyses to be
        performed on each of these data sources, along with the
        arguments for each of the analyses. Most of these
        specifications can be modified with command line options.
        This allows each <ENAMEX TYPE="PER_DESC">user</ENAMEX> to create a library of configuration
        files for sending off large batches of jobs that can be
        altered with command-line arguments when necessary.
        <ENAMEX TYPE="PERSON">Pipe_launcher</ENAMEX> returns the batch identifier generated by the
        database to the <ENAMEX TYPE="PER_DESC">user</ENAMEX>. To monitor jobs in progress, the
        batch identifier can be used in a variety of commands, such
        as 'monitor', <ENAMEX TYPE="FAC">'batch'</ENAMEX>, 'delete-batch' and
        <ENAMEX TYPE="FAC">'query_batch'</ENAMEX>.
        The flyshell application is an interactive command-line
        Perl <ENAMEX TYPE="PER_DESC">interpreter</ENAMEX> that presents the database and pipeline
        <ENAMEX TYPE="ORGANIZATION">APIs</ENAMEX> to the end <ENAMEX TYPE="PER_DESC">user</ENAMEX>, providing a more flexible interface
        to <ENAMEX TYPE="PER_DESC">users</ENAMEX> who are familiar with object-oriented <ENAMEX TYPE="ORGANIZATION">Perl</ENAMEX>.
        The web front end allows convenient, <ENAMEX TYPE="GPE">browser</ENAMEX>-based
        access for <ENAMEX TYPE="PER_DESC">end users</ENAMEX> to follow the status of analyses. An
        HTML form allows users to query the pipeline database by
        job, analysis, batch or sequence identifier. The <ENAMEX TYPE="PER_DESC">user</ENAMEX> can
        drill down through batches and analyses to get to
        individual jobs and get the status, raw job output and
        error files for each job. This window on the <ENAMEX TYPE="FAC_DESC">pipeline</ENAMEX> has
        proved a useful tool for quickly viewing results.
        Once a program has successfully completed an analysis of
        a sequence, the pipeline system sets its job status in the
        database to <ENAMEX TYPE="ORGANIZATION">FIN</ENAMEX> (Figure <NUMEX TYPE="CARDINAL">2</NUMEX>). The raw results are recorded in
        the database and may be retrieved through the web or Perl
        <ENAMEX TYPE="ORGANIZATION">interfaces</ENAMEX>. The raw results are then parsed, filtered and
        stored in the database and the job's status is set to
        <ENAMEX TYPE="ORGANIZATION">PROCD</ENAMEX>. At this point a <ENAMEX TYPE="GAME">GAME</ENAMEX> (<ENAMEX TYPE="ORGANIZATION">Genome Annotation Markup</ENAMEX>
        <ENAMEX TYPE="PER_DESC">Elements</ENAMEX>) <ENAMEX TYPE="ORGANIZATION">XML</ENAMEX> (extensible <ENAMEX TYPE="LANGUAGE">Markup Language</ENAMEX> [ <TIMEX TYPE="DATE">17</TIMEX>])
        representation of the processed data can be retrieved
        through either the <ENAMEX TYPE="ORGANIZATION">Perl</ENAMEX> or web interfaces.
      
      
        Analysis software
        
          Sim4wrap
          Sim4 [ <TIMEX TYPE="DATE">18</TIMEX>] is a highly useful and largely accurate way
          of aligning full-length cDNA and <TIMEX TYPE="TIME">EST</TIMEX> sequences against
          the <ENAMEX TYPE="ORG_DESC">genome</ENAMEX> [ <TIMEX TYPE="DATE">19</TIMEX>]. Sim4 is designed to align nearly
          identical <ENAMEX TYPE="SUBSTANCE">sequences</ENAMEX> and if dissimilar sequences are used,
          the results will contain many errors and the execution
          <ENAMEX TYPE="ORGANIZATION">time</ENAMEX> will be long. To circumvent this problem, we split
          the alignment of 
          Drosophila cDNA and <TIMEX TYPE="TIME">EST</TIMEX> sequences
          into <NUMEX TYPE="CARDINAL">two</NUMEX> serial tasks and wrote a utility program,
          sim4wrap, to manage these tasks. Sim4wrap executes a
          <NUMEX TYPE="ORDINAL">first</NUMEX> pass using <ENAMEX TYPE="ORGANIZATION">BLASTN</ENAMEX>, using the genome sequence as the
          query sequence and the cDNA sequences as the subject
          database. We run <ENAMEX TYPE="ORGANIZATION">BLASTN</ENAMEX> [ <TIMEX TYPE="DATE">20, 21</TIMEX>] with the '<ENAMEX TYPE="PRODUCT">-B 0</ENAMEX>' option,
          as we are only interested in the summary part of the
          BLAST report, not in the high-scoring pairs (HSPs)
          portion where the alignments are shown. From this BLAST
          <ENAMEX TYPE="ORGANIZATION">report</ENAMEX> summary <NUMEX TYPE="ORDINAL">sim4wrap</NUMEX> parses out the sequence
          <ENAMEX TYPE="ORGANIZATION">identifiers</ENAMEX> and filters the original database to produce
          a temporary <ENAMEX TYPE="ORGANIZATION">FASTA</ENAMEX> data file that contains only these
          <ENAMEX TYPE="ORGANIZATION">sequences</ENAMEX>. Finally we run <NUMEX TYPE="ORDINAL">sim4</NUMEX> again using the genomic
          sequence as the query and the minimal set of sequences
          that we have culled as the subject.
        
        
          Autopromote
          The 
          Drosophila genome was not a blank
          slate because there were previous annotations from the
          <ENAMEX TYPE="PRODUCT">Release 2</ENAMEX> genomic sequence [ <TIMEX TYPE="DATE">22</TIMEX>]. Therefore, before the
          curation of a chromosome <ENAMEX TYPE="ORG_DESC">arm</ENAMEX> began, we first
          <ENAMEX TYPE="ORGANIZATION">'autopromoted</ENAMEX>' the <ENAMEX TYPE="PRODUCT">Release 2</ENAMEX> <ENAMEX TYPE="PRODUCT_DESC">annotations</ENAMEX> and certain
          results from the computational analyses to the status of
          <ENAMEX TYPE="ORGANIZATION">annotations</ENAMEX>. This simplified the annotation process by
          providing an advanced starting point for the <ENAMEX TYPE="PER_DESC">curators</ENAMEX> to
          work from.
          <ENAMEX TYPE="ORGANIZATION">Autopromotion</ENAMEX> is not a straightforward process. First,
          there have been significant changes to the genome
          sequence between releases. <NUMEX TYPE="ORDINAL">Second</NUMEX>, all of the annotations
          present in <ENAMEX TYPE="PRODUCT">Release 2</ENAMEX> must be accounted for, even if
          ultimately they are deleted. <NUMEX TYPE="ORDINAL">Third</NUMEX>, the autopromotion
          software must synthesize different analysis results, some
          of which may be conflicting. Autopromote resolves
          conflicts using graph theory and voting networks.
          <ENAMEX TYPE="PRODUCT">Table 1lists</ENAMEX> the programs and parameters that were
          used for the analysis of the genomic sequence and peptide
          analysis.
        
      
      
        <ENAMEX TYPE="GPE">Berkeley Output Parser</ENAMEX> filtering
        
          Sim4 filtering
          Our primary objective in using <ENAMEX TYPE="SUBSTANCE">sim4</ENAMEX> was to align 
          <ENAMEX TYPE="ORGANIZATION">Drosophila ESTs</ENAMEX> and cDNA sequences
          only to the genes that encoded them, and not to
          <ENAMEX TYPE="PERSON">gene-</ENAMEX>family <ENAMEX TYPE="PER_DESC">members</ENAMEX>, and for this reason we applied
          stringent measures before accepting an alignment. For
          sim4 the filtering parameters are as follows.
          
          Score is the minimum percent
          identity that is required to retain an HSP or alignment;
          the default value is <NUMEX TYPE="PERCENT">95%</NUMEX>.
          
          Coverage is a percentage of the
          total length of the sequence that is aligned to the
          genome sequence. Any alignments that are less than this
          percentage length are eliminated; we required <NUMEX TYPE="PERCENT">80%</NUMEX> of the
          length of a cDNA to be aligned.
          
          <ENAMEX TYPE="PER_DESC">Discontinuity</ENAMEX> sets a maximum gap
          length in <TIMEX TYPE="TIME">the aligned EST</TIMEX> or cDNA sequence. The primary
          <ENAMEX TYPE="ORGANIZATION">aim</ENAMEX> of this parameter is to identify and eliminate
          unrelated sequences that were physically linked by a cDNA
          library construction artifact.
          
          Remove <ENAMEX TYPE="SUBSTANCE">poly</ENAMEX>(A) tail is a <ENAMEX TYPE="ORGANIZATION">Boolean</ENAMEX> to
          indicate that short terminal HSPs consisting primarily of
          runs of a single <ENAMEX TYPE="FAC_DESC">base</ENAMEX> (either <ENAMEX TYPE="PRODUCT">T</ENAMEX> or A because we could not
          be certain of the strand) are to be removed.
          
          <ENAMEX TYPE="CONTACT_INFO">Join 5</ENAMEX>' and <ENAMEX TYPE="PRODUCT">3</ENAMEX>' is a Boolean
          operation and is used for <TIMEX TYPE="TIME">EST</TIMEX> data. If it is true, BOP
          will do <NUMEX TYPE="CARDINAL">two</NUMEX> things. First, <ENAMEX TYPE="ORGANIZATION">BOP</ENAMEX> will reverse complement
          any hits where the name of the sequence contains the
          <ENAMEX TYPE="CONTACT_INFO">phrase '3prime'.</ENAMEX> <NUMEX TYPE="ORDINAL">Second</NUMEX>, it will merge all alignments
          where the prefixes of the name are the same. Originally
          this was used solely for the <ENAMEX TYPE="PRODUCT">5</ENAMEX>' and <ENAMEX TYPE="PRODUCT">3</ENAMEX>' <ENAMEX TYPE="SUBSTANCE">ESTs</ENAMEX> that were
          available. However, when we introduced the internal
          <ENAMEX TYPE="PER_DESC">sequencing</ENAMEX> reads from the 
          <ENAMEX TYPE="PERSON">Drosophila Gene Collection</ENAMEX> (DGC)
          cDNA sequencing project [ <TIMEX TYPE="DATE">23</TIMEX>] into the pipeline this
          portion of code became an alternative means of
          effectively assembling the cDNA sequence. Using the
          intersection of each individual sequence alignment with
          the genome sequence a single virtual cDNA sequence was
          constructed.
          Another tactic for condensing primary results, without
          removing any information, is to reconstruct all logically
          possible alternative transcripts from the raw <TIMEX TYPE="TIME">EST</TIMEX>
          alignments by building a graph from a complete set of
          <ENAMEX TYPE="ORGANIZATION">overlapping ESTs</ENAMEX>. Each <ENAMEX TYPE="PER_DESC">node</ENAMEX> comprises the set of spans
          that share common splice junctions. The root of the graph
          is the node with the most <ENAMEX TYPE="PER_DESC">5</ENAMEX>' <ENAMEX TYPE="PER_DESC">donor</ENAMEX> site. It is, of
          course, also possible to have <NUMEX TYPE="CARDINAL">more than one</NUMEX> starting
          point for the graph, if there are overlapping nodes with
          alternative donor sites. The set of possible transcripts
          is the number of <ENAMEX TYPE="FAC_DESC">paths</ENAMEX> through this tree(s). This
          analysis produced an additional set of alignments that
          augmented the original <TIMEX TYPE="TIME">EST</TIMEX> alignments.
        
        
          External pipelines
          Of the numerous gene-prediction programs available, we
          incorporated <NUMEX TYPE="CARDINAL">only two</NUMEX> into our <ENAMEX TYPE="FAC_DESC">pipeline</ENAMEX>. This was because
          some of these programs are difficult to integrate into a
          <ENAMEX TYPE="ORGANIZATION">pipeline</ENAMEX>, some are highly computationally expensive and
          others are only available under restricted licenses.
          Rather than devoting resources to running an
          exhaustive <ENAMEX TYPE="FAC_DESC">suite</ENAMEX> of analyses, we asked a number of
          <ENAMEX TYPE="ORGANIZATION">external</ENAMEX> <ENAMEX TYPE="ORG_DESC">groups</ENAMEX> to run their <ENAMEX TYPE="ORG_DESC">pipelines</ENAMEX> on our genomic
          <ENAMEX TYPE="ORGANIZATION">sequences</ENAMEX>. We received results for <NUMEX TYPE="CARDINAL">three</NUMEX> of the <NUMEX TYPE="CARDINAL">five</NUMEX>
          chromosome arms (<ENAMEX TYPE="CONTACT_INFO">2L, 2R, 3R</ENAMEX>) from <ENAMEX TYPE="ORGANIZATION">Celera Genomics</ENAMEX>,
          <ENAMEX TYPE="ORGANIZATION">Ensembl</ENAMEX> and <ENAMEX TYPE="ORGANIZATION">NCBI</ENAMEX> <ENAMEX TYPE="ORG_DESC">pipelines</ENAMEX>. These predictions were
          presented to <ENAMEX TYPE="PER_DESC">curators</ENAMEX> as extra analysis tiers in <ENAMEX TYPE="ORGANIZATION">Apollo</ENAMEX>
          and were helpful in suggesting where coding regions were
          located. However, in practice, human <ENAMEX TYPE="PER_DESC">curators</ENAMEX> required
          detailed alignment data to establish biologically
          accurate gene structures and this information was only
          available from our internal pipeline.
        
        
          Hardware
          As an inexpensive solution to satisfy the
          computational requirements of the genomic analyses we
          built a <ENAMEX TYPE="ORGANIZATION">Beowulf</ENAMEX> cluster [ <TIMEX TYPE="DATE">24</TIMEX>] and utilized the portable
          batch system (<ENAMEX TYPE="ORGANIZATION">PBS</ENAMEX>) software developed by <ENAMEX TYPE="ORGANIZATION">NASA</ENAMEX> [ <TIMEX TYPE="DATE">25</TIMEX>] for
          job control. A <ENAMEX TYPE="ORGANIZATION">Beowulf</ENAMEX> cluster is a collection of
          processor nodes that are interconnected in a network and
          the sole purpose of these nodes and the <ENAMEX TYPE="ORG_DESC">network</ENAMEX> is to
          provide processor compute cycles. The nodes themselves
          are inexpensive off-the-shelf processor chips, connected
          using standard networking technology, and running
          open-source software; when combined, these components
          generate a low-cost, high-performance compute system. Our
          nodes are all identical and use <ENAMEX TYPE="ORGANIZATION">Linux</ENAMEX> as their base
          operating system, as is usual for <ENAMEX TYPE="ORGANIZATION">Beowulf</ENAMEX> clusters.
          The <ENAMEX TYPE="ORGANIZATION">Beowulf</ENAMEX> cluster was built by <ENAMEX TYPE="ORGANIZATION">Linux NetworX</ENAMEX> [ <TIMEX TYPE="DATE">26</TIMEX>]
          which also provided additional hardware (<ENAMEX TYPE="ORGANIZATION">ICE box</ENAMEX>) and
          Clusterworx software to install the system software and
          <ENAMEX TYPE="ORGANIZATION">control</ENAMEX> and monitor the <ENAMEX TYPE="ORG_DESC">hardware</ENAMEX> of the nodes. The
          <ENAMEX TYPE="PRODUCT">cluster</ENAMEX> configuration used in this work consisted of <NUMEX TYPE="CARDINAL">32</NUMEX>
          standard IA32 architecture nodes, each with dual Pentium
          <ENAMEX TYPE="ORGANIZATION">III CPUs</ENAMEX> running at <NUMEX TYPE="CARDINAL">700</NUMEX> <ENAMEX TYPE="PRODUCT">MHz/1 GHz</ENAMEX> and <ENAMEX TYPE="PRODUCT">512 MB</ENAMEX> memory. In
          addition, a single <ENAMEX TYPE="PERSON">Pentium III</ENAMEX>-based <ENAMEX TYPE="PER_DESC">master</ENAMEX> node was used
          to control the cluster nodes and distribute the compute
          jobs. Nodes were interconnected with standard 100BT
          Ethernet on an isolated subnet with the master node as
          the only interface to the outside <ENAMEX TYPE="ORG_DESC">network</ENAMEX>. The private
          <ENAMEX TYPE="ORGANIZATION">cluster 100 BT</ENAMEX> <ENAMEX TYPE="ORG_DESC">network</ENAMEX> was connected to the <ENAMEX TYPE="ORGANIZATION">NAS</ENAMEX>-based
          storage volumes housing the data and <ENAMEX TYPE="PER_DESC">user</ENAMEX> home
          directories with <ENAMEX TYPE="ORGANIZATION">Gigabit</ENAMEX> ethernet. Each <ENAMEX TYPE="PER_DESC">node</ENAMEX> had a <NUMEX TYPE="CARDINAL">2</NUMEX> GB
          swap partition used to cache the sequence databases from
          the <ENAMEX TYPE="ORG_DESC">network</ENAMEX> storage volumes. To provide a consistent
          environment, the <ENAMEX TYPE="ORG_DESC">nodes</ENAMEX> had the same mounting points of
          the directories as all other <ENAMEX TYPE="ORGANIZATION">BDGP</ENAMEX> Unix computers. The
          <ENAMEX TYPE="ORGANIZATION">network-wide NIS</ENAMEX> maps were translated to the internal
          <ENAMEX TYPE="ORGANIZATION">cluster NIS</ENAMEX> maps with an automated script. Local hard
          disks on the nodes were used as temporary storage for the
          pipeline jobs.
          Job distribution to the cluster nodes was done with
          the queuing system <ENAMEX TYPE="PERSON">OpenPBS</ENAMEX>, version 2.3.12 [ <TIMEX TYPE="DATE">25</TIMEX>]. <ENAMEX TYPE="ORGANIZATION">PBS</ENAMEX> was
          configured with several queues and each queue having
          access to a dynamically resizable overlapping fraction of
          <ENAMEX TYPE="ORGANIZATION">nodes</ENAMEX>. Queues were configured to use <NUMEX TYPE="CARDINAL">one</NUMEX> node at a time,
          either running <NUMEX TYPE="CARDINAL">one</NUMEX> job using both <ENAMEX TYPE="ORGANIZATION">CPUs</ENAMEX> (such as the
          multithreaded BLAST or Interpro motif analysis) or two
          jobs using <NUMEX TYPE="CARDINAL">one</NUMEX> <ENAMEX TYPE="PRODUCT">CPU</ENAMEX> each for optimal utilization of the
          <ENAMEX TYPE="ORGANIZATION">resources</ENAMEX>. Because of the architecture of the pipeline,
          individual jobs were often small but <NUMEX TYPE="CARDINAL">tens of thousands</NUMEX> of
          them may be submitted at any given time. Because the
          default <ENAMEX TYPE="ORGANIZATION">PBS</ENAMEX> <ENAMEX TYPE="PER_DESC">first</ENAMEX>-in/<NUMEX TYPE="ORDINAL">first</NUMEX>-out (FIFO) scheduler, while
          providing a lot of flexibility, does not scale up beyond
          <NUMEX TYPE="CARDINAL">about 5,000</NUMEX>-<NUMEX TYPE="CARDINAL">10,000</NUMEX> jobs per queue, the scheduler was
          extended. With this extension the scheduler caches jobs
          in memory if a maximum queue limit is exceeded. Job
          resource allocation was managed on a per queue basis.
          Individual jobs could only request cluster resources
          based on the queue they were submitted to and each queue
          was run on a strict <ENAMEX TYPE="ORGANIZATION">FIFO</ENAMEX> basis. With those modifications
          <ENAMEX TYPE="ORGANIZATION">PBS</ENAMEX> was scaled to <NUMEX TYPE="CARDINAL">over 100,000</NUMEX> jobs while still
          permitting higher-priority jobs to be submitted to a
          separate high-priority queue.
        
      
      
        <ENAMEX TYPE="PERSON">Storing</ENAMEX> and querying the annotation results: the
        Gadfly database
        A pipeline database is useful for managing the execution
        and post-processing of computational analyses. The end
        result of the pipeline process is streams of prediction and
        alignment data localized to genomic, transcript or peptide
        <ENAMEX TYPE="ORGANIZATION">sequences</ENAMEX>. We store these data in a relational database,
        called <ENAMEX TYPE="WORK_OF_ART">Genome Annotation Database of the Fly</ENAMEX> (<ENAMEX TYPE="ORGANIZATION">Gadfly</ENAMEX>).
        <ENAMEX TYPE="ORGANIZATION">Gadfly</ENAMEX> is the <NUMEX TYPE="ORDINAL">second</NUMEX> of the two database schemata used by
        the annotation system and will be discussed elsewhere.
        We initially considered using <ENAMEX TYPE="ORGANIZATION">Ensembl</ENAMEX> as our sequence
        database. At the time we started building our system,
        Ensembl was also in an early stage of development. We
        decided to develop our own database and software, while
        trying to retain interoperability between the <NUMEX TYPE="CARDINAL">two</NUMEX>. This
        proved difficult, and the <NUMEX TYPE="CARDINAL">two</NUMEX> systems <ENAMEX TYPE="ORG_DESC">diverged</ENAMEX>. While this
        was wasteful in terms of redundant software development, it
        did allow us to hone our system to the particular needs of
        our project. <ENAMEX TYPE="ORGANIZATION">Gadfly</ENAMEX> remains similar in architecture and
        implementation details to <ENAMEX TYPE="ORGANIZATION">Ensembl</ENAMEX>. Both projects make use
        of the bioPerl bioinformatics programming <ENAMEX TYPE="ORG_DESC">components</ENAMEX> [ <TIMEX TYPE="DATE">27</TIMEX>,
        <TIMEX TYPE="DATE">28, 29</TIMEX>].
        The core data type in <ENAMEX TYPE="GPE">Gadfly</ENAMEX> is called a 'sequence
        feature'. This can be any piece of data of biological
        interest that can be localized to a sequence. These roughly
        correspond to the types of data found in the 'feature
        table' summary of a <ENAMEX TYPE="ORGANIZATION">GenBank</ENAMEX> report. Every sequence feature
        has a 'feature type' - examples of feature types are
        <ENAMEX TYPE="ORGANIZATION">'exon'</ENAMEX>, <ENAMEX TYPE="FAC">'transcript'</ENAMEX>, 'protein-coding gene', 'tRNA gene',
        and so on.
        In <ENAMEX TYPE="GPE">Gadfly</ENAMEX>, sequence features are linked together in
        <ENAMEX TYPE="ORGANIZATION">hierarchies</ENAMEX>. For instance, a gene model is linked to the
        different transcripts that are expressed by that gene, and
        these transcripts are linked to exons. <ENAMEX TYPE="ORGANIZATION">Gadfly</ENAMEX> does not
        store some sequence features, such as introns or
        untranslated <ENAMEX TYPE="GPE_DESC">regions</ENAMEX> (UTR), as this data can be inferred
        from other features. Instead <ENAMEX TYPE="PERSON">Gadfly</ENAMEX> contains software rules
        for producing these features on demand.
        Sequence features can have other pieces of data linked
        to them. Examples of the kind of data we attach are:
        functional data such as <ENAMEX TYPE="PERSON">Gene Ontology</ENAMEX> (<ENAMEX TYPE="ORGANIZATION">GO</ENAMEX>) [ <TIMEX TYPE="DATE">30</TIMEX>] term
        assignments; tracking data such as symbols, synonyms and
        <ENAMEX TYPE="CONTACT_INFO">accession</ENAMEX> numbers; data relevant to the annotation process,
        such as <ENAMEX TYPE="PER_DESC">curator</ENAMEX> comments [ <ENAMEX TYPE="LAW">8</ENAMEX>]; data relevant to the
        pipeline process, such as scores and expectation values in
        the case of computed features. Note that there is a wealth
        of information that we do not store, particularly genetic
        and phenotypic data, as this would be redundant with the
        FlyBase relational database.
        A core design principle in <ENAMEX TYPE="GPE">Gadfly</ENAMEX> is flexibility, using
        a design principle known as generic modeling. We do not
        constrain the kinds of sequence features that can be stored
        in <ENAMEX TYPE="GPE">Gadfly</ENAMEX>, or constrain the properties of these features,
        because our knowledge of biology is constantly changing,
        and because biology itself is often unconstrained by rules
        that can be coded into databases. As much as possible, we
        avoid built-in assumptions that, if proven wrong, would
        force us to revisit and explicitly modify the software that
        embodies them.
        The generic modeling principle has been criticized for
        being too loosely constrained and leading to databases that
        are difficult to maintain and query. This is a perceived
        weakness of the ACeDB database. We believe we have found a
        <ENAMEX TYPE="PERSON">way</ENAMEX> round this by building the desired constraints into the
        program components that work with the database; we are also
        investigating the use of ontologies or controlled
        <ENAMEX TYPE="ORGANIZATION">vocabularies</ENAMEX> to enforce these constraints. A detailed
        discussion of this effort is outside the scope of this
        paper and will be reported elsewhere.
        Figure <NUMEX TYPE="CARDINAL">3shows</NUMEX> the data flow in and out of <ENAMEX TYPE="ORGANIZATION">Gadfly</ENAMEX>.
        Computational analysis features come in through analysis
        <ENAMEX TYPE="ORGANIZATION">pipelines</ENAMEX> - either the <ENAMEX TYPE="ORGANIZATION">Pipeline</ENAMEX>, via <ENAMEX TYPE="ORGANIZATION">BOP</ENAMEX>, or through an
        <ENAMEX TYPE="ORGANIZATION">external</ENAMEX> <ENAMEX TYPE="FAC_DESC">pipeline</ENAMEX>, usually delivered as files conforming to
        some standardized bioinformatics format (for example, GAME
        <ENAMEX TYPE="ORGANIZATION">XML</ENAMEX>, <ENAMEX TYPE="ORGANIZATION">GFF</ENAMEX>).
        <ENAMEX TYPE="ORGANIZATION">Data</ENAMEX> within <ENAMEX TYPE="ORGANIZATION">Gadfly</ENAMEX> is sometimes transformed by other
        Gadfly software <ENAMEX TYPE="ORG_DESC">components</ENAMEX>. For instance, just before
        curation of a chromosome arm commences, different
        computational analyses are synthesized into 'best <ENAMEX TYPE="PER_DESC">guesses</ENAMEX>'
        of gene models, as part of the autopromote software we
        described <TIMEX TYPE="DATE">earlier</TIMEX>.
        During the creation of <ENAMEX TYPE="PRODUCT">Release 3</ENAMEX> <ENAMEX TYPE="PRODUCT_DESC">annotations</ENAMEX>, <ENAMEX TYPE="PER_DESC">curators</ENAMEX>
        requested data from <ENAMEX TYPE="GPE">Gadfly</ENAMEX> by specifying a genomic region.
        Although this region can be of any size, we generally
        allocated work by <ENAMEX TYPE="ORGANIZATION">GenBank</ENAMEX> accessions. Occasionally,
        <ENAMEX TYPE="PERSON">curators</ENAMEX> worked <NUMEX TYPE="CARDINAL">one</NUMEX> <ENAMEX TYPE="SUBSTANCE">gene</ENAMEX> at a time by requesting genomic
        regions immediately surrounding the gene of interest.
        Gadfly delivers a <ENAMEX TYPE="EVENT">GAME</ENAMEX> XML file containing all of the
        computed results and the current annotations within the
        requested genomic region. The <ENAMEX TYPE="PER_DESC">curator</ENAMEX> used the <ENAMEX TYPE="ORGANIZATION">Apollo</ENAMEX>
        editing tool to annotate the region, after which the data
        in the modified XML file was stored in <ENAMEX TYPE="GPE">Gadfly</ENAMEX>.
        The generation of a high-quality set of predicted
        <ENAMEX TYPE="ORGANIZATION">peptides</ENAMEX> is one of our primary goals. To achieve this goal,
        we needed a means of evaluating the peptides and presenting
        this assessment to the <ENAMEX TYPE="PER_DESC">curators</ENAMEX> for inspection, so that
        they might improve the quality of the predicted peptides
        <ENAMEX TYPE="ORGANIZATION">iteratively</ENAMEX>. Every peptide was sent through a peptide
        pipeline to assess the predicted peptide both
        <ENAMEX TYPE="ORGANIZATION">quantitatively</ENAMEX> and qualitatively. Where possible, we wanted
        to apply a quantifiable metric, requiring a standard
        against which we could rate the peptides. For this purpose
        we used <ENAMEX TYPE="SUBSTANCE">peptides</ENAMEX> found in <ENAMEX TYPE="ORGANIZATION">SPTRREAL</ENAMEX> (<ENAMEX TYPE="PERSON">E. Whitfield</ENAMEX>, personal
        <ENAMEX TYPE="ORGANIZATION">communication</ENAMEX>), a carefully reviewed database of published
        peptide sequences, for comparison to our predicted
        <ENAMEX TYPE="SUBSTANCE">proteins</ENAMEX>. <ENAMEX TYPE="ORGANIZATION">SPTRREAL</ENAMEX> is composed of <NUMEX TYPE="CARDINAL">3,687</NUMEX> 
        <ENAMEX TYPE="NATIONALITY">D.</ENAMEX> melanogaster sequences from the
        <ENAMEX TYPE="ORGANIZATION">SWISS-PROT</ENAMEX> and <ENAMEX TYPE="PRODUCT">TrEMBL</ENAMEX> <ENAMEX TYPE="SUBSTANCE">protein databases</ENAMEX> [ <TIMEX TYPE="DATE">31</TIMEX>] and provides
        a curated <ENAMEX TYPE="SUBSTANCE">protein</ENAMEX>-sequence database with a high level of
        annotation, a minimal level of redundancy and the absence
        of any hypothetical or computational gene models. Our
        program, <ENAMEX TYPE="ORGANIZATION">PEP-QC</ENAMEX>, performed this crucial aspect of the
        annotation process and is described below. In cases where a
        known peptide was unavailable, we used a qualitative
        measure to evaluate the peptide. The peptide pipeline
        provided a <ENAMEX TYPE="ORGANIZATION">BLASTP</ENAMEX> analysis with comparisons to peptides
        from other model <ENAMEX TYPE="SUBSTANCE">organism genome sequences</ENAMEX> and InterProScan
        [ <TIMEX TYPE="DATE">32</TIMEX>] analysis for <ENAMEX TYPE="SUBSTANCE">protein</ENAMEX>-family motifs to enable the
        <ENAMEX TYPE="PER_DESC">curators</ENAMEX> to judge whether the biological properties of the
        peptide were reasonable.
        Each annotation cycle on a sequence may affect the
        primary structure of the <ENAMEX TYPE="SUBSTANCE">proteins</ENAMEX> encoded by that sequence
        and these changes must therefore trigger a reanalysis of
        the edited peptides. Whereas the genomic pipeline is
        launched at distinct stages, on an arm-by-arm basis, the
        <ENAMEX TYPE="ORGANIZATION">peptide</ENAMEX> <ENAMEX TYPE="FAC_DESC">pipeline</ENAMEX> is run whenever a <ENAMEX TYPE="PER_DESC">curator</ENAMEX> changes a gene
        model and saves it to the <ENAMEX TYPE="ORGANIZATION">Gadfly</ENAMEX> database. To rapidly
        identify whether the peptide sequence generated by the
        altered gene model has also changed, the database uniquely
        identifies every peptide sequence by its name and its MD5
        <ENAMEX TYPE="ORGANIZATION">checksum</ENAMEX> [ <TIMEX TYPE="DATE">33</TIMEX>]. The <NUMEX TYPE="ORDINAL">MD5</NUMEX> checksum provides a fast and
        convenient way of determining whether <NUMEX TYPE="CARDINAL">two</NUMEX> <ENAMEX TYPE="PER_DESC">sequences</ENAMEX> are
        identical. To determine whether a peptide sequence has been
        altered is a simple comparison of the prior checksum to the
        new checksum, allowing us to avoid using compute cycles in
        reanalyzing sequences that have not changed.
        <ENAMEX TYPE="ORGANIZATION">PEP-QC</ENAMEX> generates both summary status codes and detailed
        <ENAMEX TYPE="ORGANIZATION">alignment</ENAMEX> information for each gene and each peptide.
        <ENAMEX TYPE="ORGANIZATION">ClustalW</ENAMEX> [ <TIMEX TYPE="DATE">34</TIMEX>] and showalign [ <TIMEX TYPE="DATE">35</TIMEX>] are used to generate a
        multiple alignment from the annotated peptides for the gene
        and the corresponding <ENAMEX TYPE="ORGANIZATION">SPTRREAL</ENAMEX> peptide or peptides. In
        addition, brief 'discrepancy' reports are generated that
        describe each <ENAMEX TYPE="ORGANIZATION">SPTRREAL</ENAMEX> mismatch clearly. For instance, an
        annotated peptide might contain any or all of the
        <ENAMEX TYPE="ORGANIZATION">mismatches</ENAMEX> in <ENAMEX TYPE="PRODUCT">Table 2</ENAMEX>(in this example, <ENAMEX TYPE="PRODUCT">CG2903-PB</ENAMEX> is the
        initial FlyBase annotation and <ENAMEX TYPE="PRODUCT">Q960X8</ENAMEX> is the SPTRREAL
        entry).
        The quality assessments produced by the peptide pipeline
        need to be available to the <ENAMEX TYPE="PER_DESC">curators</ENAMEX> for inspection during
        annotation sessions so that any corrections that are needed
        can be made. <ENAMEX TYPE="PERSON">Curators</ENAMEX> also need to access other relevant
        <ENAMEX TYPE="ORGANIZATION">FlyBase</ENAMEX> information associated with a gene in order to
        refine an annotation efficiently. We developed
        automatically generated 'mini-gene-reports' to consolidate
        this gene data into a single web page. <ENAMEX TYPE="PERSON">Mini</ENAMEX>-gene-reports
        include all the names and synonyms associated with a gene,
        its cytological location and accessions for the genomic
        <ENAMEX TYPE="PERSON">sequence</ENAMEX>, <ENAMEX TYPE="ORGANIZATION">ESTs</ENAMEX>, <ENAMEX TYPE="ORGANIZATION">PIR</ENAMEX> records and <ENAMEX TYPE="PERSON">Drosophila Gene Collection</ENAMEX>
        [ <TIMEX TYPE="DATE">23</TIMEX>] assignments, if any. All of these items are
        <ENAMEX TYPE="ORGANIZATION">hyperlinked</ENAMEX> to the appropriate databases for easy access to
        more extensive information. All literature references for
        the gene appear in the reports, with hyperlinks to the
        complete text or abstracts. The <ENAMEX TYPE="PRODUCT_DESC">mini-</ENAMEX>gene-reports also
        consolidate any comments about the gene, including
        amendments to the gene annotation submitted by FlyBase
        <ENAMEX TYPE="PER_DESC">curators</ENAMEX> or <ENAMEX TYPE="PER_DESC">members</ENAMEX> of the 
        Drosophila <ENAMEX TYPE="PER_DESC">community</ENAMEX>. The
        <ENAMEX TYPE="PERSON">mini</ENAMEX>-gene-reports can be accessed directly from <ENAMEX TYPE="ORGANIZATION">Apollo</ENAMEX>, or
        searched via a web form by gene name, <ENAMEX TYPE="PER_DESC">symbol</ENAMEX>, synonym
        (including the <ENAMEX TYPE="ORGANIZATION">FlyBase</ENAMEX> unique identifier, or FBgn) or
        <ENAMEX TYPE="GPE">genomic</ENAMEX> location. A web report, grouped by genomic segment
        and annotator, is updated <TIMEX TYPE="DATE">nightly</TIMEX> and contains lists of
        genes indexed by status code and linked to their individual
        <ENAMEX TYPE="PERSON">mini</ENAMEX>-gene-reports.
      
      
        Other integrity checks
        Before submission to <ENAMEX TYPE="ORGANIZATION">GenBank</ENAMEX> a number of additional
        checks are run to detect potential oversights in the
        <ENAMEX TYPE="ORGANIZATION">annotation</ENAMEX>. These checks include confirming the validity of
        any annotations with open reading frames (ORF) that are
        either unusually short (<NUMEX TYPE="CARDINAL">less than 50</NUMEX> amino <ENAMEX TYPE="SUBSTANCE">acids</ENAMEX>) or less
        than <NUMEX TYPE="PERCENT">25%</NUMEX> of the transcript length. In the special case of
        known small <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX>, such as the 
        Drosophila immune response genes
        (<ENAMEX TYPE="ORGANIZATION">DIRGs</ENAMEX>) [ <TIMEX TYPE="DATE">36</TIMEX>], the genome annotations are scanned to ensure
        that no well-documented <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> have been missed. Similarly,
        the genome is scanned for particular annotations to verify
        their presence, including those that have been submitted as
        corrections from the <ENAMEX TYPE="GPE_DESC">community</ENAMEX>, or are cited in the
        literature, such as tRNA, snRNA, snoRNA, microRNA or rRNA
        genes documented in <ENAMEX TYPE="GPE">FlyBase</ENAMEX>. If the translation start site
        is absent, an explanation must be provided in the comments.
        Annotations may also be eliminated if annotations with
        different identifiers are found at the same genome
        coordinates or if a <ENAMEX TYPE="SUBSTANCE">protein</ENAMEX>-coding gene overlaps a
        transposable element, or a tRNA overlaps a <ENAMEX TYPE="SUBSTANCE">protein</ENAMEX>-coding
        <ENAMEX TYPE="PERSON">gene</ENAMEX>. Conversely, duplicated gene identifiers that are
        found at different genome coordinates are either renamed or
        removed. A simple syntax check is also carried out on all
        the annotation symbols and identifiers. Known mutations in
        the sequenced strain are documented and the wild-type
        <ENAMEX TYPE="ORGANIZATION">peptide</ENAMEX> is submitted in place of the mutated version.
        The <ENAMEX TYPE="ORGANIZATION">BDGP</ENAMEX> also submits to GenBank the cDNA sequence from
        the <ENAMEX TYPE="ORGANIZATION">DGC</ENAMEX> project. Each of these cDNA clones represents an
        expressed transcript and it is important to the community
        that the records for these cDNA sequences correctly
        correspond to the records for the annotated transcripts in
        both <ENAMEX TYPE="ORGANIZATION">GenBank</ENAMEX> and <ENAMEX TYPE="ORGANIZATION">FlyBase</ENAMEX>. This correspondence is
        accomplished via the cDNA sequence alignments to the genome
        described previously. After annotation of the entire genome
        was completed these results were used to find the
        intersection of cDNA alignments and exons. A cDNA was
        assigned to a gene when the cDNA overlapped most of the
        <ENAMEX TYPE="SUBSTANCE">gene exons</ENAMEX> and the predicted peptides of each were verified
        using a method similar to <ENAMEX TYPE="ORGANIZATION">PEP-QC</ENAMEX>.
      
      
        <ENAMEX TYPE="ORGANIZATION">Public World Wide Web interface</ENAMEX>
        We provide a website for the <ENAMEX TYPE="GPE_DESC">community</ENAMEX> to query <ENAMEX TYPE="ORGANIZATION">Gadfly</ENAMEX>.
        This allows queries by gene, by genomic or map region, by
        <ENAMEX TYPE="PERSON">Gene Ontology</ENAMEX> (<ENAMEX TYPE="ORGANIZATION">GO</ENAMEX>) assignments or by <ENAMEX TYPE="ORGANIZATION">InterPro</ENAMEX> domains. As
        well as delivering human-readable web pages, we also allow
        downloading of data in a variety of computer-readable
        formats supported by common bioinformatics tools. We use
        the <ENAMEX TYPE="ORGANIZATION">GBrowse</ENAMEX> [ <TIMEX TYPE="DATE">37</TIMEX>] application, which is part of the <ENAMEX TYPE="ORGANIZATION">GMOD</ENAMEX> [
        <NUMEX TYPE="CARDINAL">38</NUMEX>] collection of software for visualization and
        exploration of genomic regions.
      
      
        Software engineering
        The main software engineering lesson we learned in the
        course of this project was the importance of flexibility.
        Nowhere was this more important than in the database
        <ENAMEX TYPE="ORGANIZATION">schema</ENAMEX>. In any genome, normal biology <ENAMEX TYPE="FAC_DESC">conspires</ENAMEX> to break
        carefully designed data models. Among the examples we
        encountered while annotating the 
        <ENAMEX TYPE="NATIONALITY">D.</ENAMEX> melanogaster genome were: the
        occurrence of distinct transcripts with overlapping UTRs
        but non-overlapping coding regions, leading us to modify
        our original definition of 'alternative transcript'; the
        existence of <ENAMEX TYPE="SUBSTANCE">dicistronic genes</ENAMEX>, <NUMEX TYPE="CARDINAL">two or more</NUMEX> distinct and
        non-overlapping coding regions contained on a single
        processed <ENAMEX TYPE="SUBSTANCE">mRNA</ENAMEX>, requiring support for <NUMEX TYPE="CARDINAL">one</NUMEX> to many
        relationships between transcript and peptides; and 
        <ENAMEX TYPE="ORGANIZATION">trans -splicing</ENAMEX>, exhibited by the 
        <ENAMEX TYPE="ORGANIZATION">mod</ENAMEX> ( 
        mdg4 ) gene [ <TIMEX TYPE="DATE">39</TIMEX>], requiring a new
        <ENAMEX TYPE="ORGANIZATION">data</ENAMEX> model. We also needed to adapt the pipeline to
        different types and qualities of input sequence. For
        example, to analyze the draft sequence of the repeat-rich
        <ENAMEX TYPE="ORGANIZATION">heterochromatin</ENAMEX> [ <TIMEX TYPE="DATE">40</TIMEX>], we needed to adjust the parameters
        and <ENAMEX TYPE="SUBSTANCE">datasets</ENAMEX> used, but also to develop an entirely new
        repeat-masking approach to facilitate gene finding in
        highly repetitive regions. We are now in the process of
        modifying the pipeline to exploit comparative genome
        sequences more efficiently. Our intention is to continue
        extending the system to accommodate new biological research
        situations.
        Improvements to tools and techniques are often as
        fundamental to scientific progress as new discoveries, and
        thus the sharing of research tools is as essential as
        sharing the discoveries themselves. We are active
        <ENAMEX TYPE="PER_DESC">participants</ENAMEX> in, and contributors to, the <ENAMEX TYPE="PRODUCT">Generic Model</ENAMEX>
        <ENAMEX TYPE="PERSON">Organism Database</ENAMEX> (GMOD) project, which seeks to bring
        together open-source applications and <ENAMEX TYPE="ORG_DESC">utilities</ENAMEX> that are
        useful to the <ENAMEX TYPE="ORG_DESC">developers</ENAMEX> of biological and genomic
        <ENAMEX TYPE="ORGANIZATION">databases</ENAMEX>. We are contributing the software we have
        developed during this project to <ENAMEX TYPE="ORGANIZATION">GMOD</ENAMEX>. Conversely, we reuse
        the <ENAMEX TYPE="GPE">Perl</ENAMEX>-based software, <ENAMEX TYPE="ORGANIZATION">GBrowse</ENAMEX>, from <ENAMEX TYPE="ORGANIZATION">GMOD</ENAMEX> for the visual
        display of our annotations.
        Automated <ENAMEX TYPE="ORG_DESC">pipelines</ENAMEX> and the <ENAMEX TYPE="PER_DESC">management</ENAMEX> of downstream
        <ENAMEX TYPE="ORGANIZATION">data</ENAMEX> require a significant investment in software
        <ENAMEX TYPE="ORGANIZATION">engineering</ENAMEX>. The pipeline software, the database, and the
        annotation tool, <ENAMEX TYPE="ORGANIZATION">Apollo</ENAMEX>, as a <ENAMEX TYPE="ORG_DESC">group</ENAMEX>, provide a core set of
        utilities to any genome effort that shares our annotation
        strategy. Exactly how portable they are remains to be seen,
        as there is a tradeoff between customization and ease of
        use. We will only know the extent to which we were
        successful when other <ENAMEX TYPE="ORG_DESC">groups</ENAMEX> try to reuse and extend these
        software tools. Nevertheless, the wealth of experience we
        gained, as well as the tools we developed in the process of
        reannotating the 
        Drosophila genome, will be a valuable
        <ENAMEX TYPE="ORGANIZATION">resource</ENAMEX> to any <ENAMEX TYPE="PER_DESC">group</ENAMEX> wishing to undertake a similar
        exercise.
      
    
  
