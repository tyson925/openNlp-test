
  
    
      
        Background
        Most health statistics are reported with an explicit
        quantification of uncertainty because they are based on a
        sample from a target population (possibly with random
        assignment of treatments), and quantifying the resulting
        stochastic error is done almost universally. Extrapolations
        from <ENAMEX TYPE="SUBSTANCE">samples</ENAMEX> are not, however, the only way to calculate
        rates, totals, or other quantitative measures of health.
        The availability of data may lead to an approach that does
        not involve sampling or any other random process. For
        example:
        <ENAMEX TYPE="ORGANIZATION">• Automobile</ENAMEX> fatality totals are typically computed with
        an attempt to completely enumerate, counting every
        case.
        • <NUMEX TYPE="CARDINAL">Two</NUMEX> <ENAMEX TYPE="GPE_DESC">states</ENAMEX> might create a "natural experiment" by
        having different traffic or safety regulations. Differences
        between or ratios of frequencies of accidents or injuries
        could then be computed by enumeration and arithmetic.
        <ENAMEX TYPE="PERSON">• Samples</ENAMEX> of convenience are extrapolated to the entire
        <ENAMEX TYPE="ORGANIZATION">population</ENAMEX>, such as trying to impute the <ENAMEX TYPE="GPE">U.S.</ENAMEX> incidence of 
        Escherichia <ENAMEX TYPE="SUBSTANCE">coli O157</ENAMEX>:<ENAMEX TYPE="CONTACT_INFO">H7 infections</ENAMEX>
        based on data from the few <ENAMEX TYPE="GPE_DESC">states</ENAMEX> that report good data.
        While this is a sample, the error comes not from random
        sampling error (which would be quite small) but the other
        <ENAMEX TYPE="PER_DESC">sources</ENAMEX> identified below.
        <ENAMEX TYPE="PERSON">• To</ENAMEX> estimate the rate of a disease in a <ENAMEX TYPE="PER_DESC">community</ENAMEX>, we
        frequently reverse this process and interpolate from a
        national average.
        <ENAMEX TYPE="PERSON">• Trends</ENAMEX> may be inferred from data that is believed to
        be related to the measure of interest, but in unknown ways,
        such as tracking the effect of economic changes on mental
        health by tracking the number of calls to hotlines.
        <ENAMEX TYPE="ORGANIZATION">Observational</ENAMEX> studies and randomized trials almost
        always quantify error due to random sampling and
        allocation. The realization, which was close to universal
        as long as <TIMEX TYPE="DATE">four decades ago</TIMEX>, that health research results
        need to include such quantification has helped reduce the
        frequency of conclusions based on inadequate sample sizes.
        However, the ease of quantifying that single source of
        error has distracted <ENAMEX TYPE="PER_DESC">researchers</ENAMEX> from the many other, often
        larger, errors in any estimated quantity. Quantification of
        the <NUMEX TYPE="CARDINAL">one</NUMEX> <ENAMEX TYPE="PER_DESC">source</ENAMEX> of error implies that this represents all
        uncertainty. This implication grossly overstates precision
        by ignoring uncorrected confounding, selection biases,
        measurement errors, and specification of functional
        relationships. [ <NUMEX TYPE="CARDINAL">1 2 3 4</NUMEX> ] This is especially clear when a
        calculation does not involve sampling and, lacking the one
        source of error that we commonly quantify, the numbers are
        reported as point estimates with no acknowledgment of
        uncertainty at all. A complete lack of error quantification
        implies the even more misleading claim that the result is
        perfect. While some <ENAMEX TYPE="PER_DESC">readers</ENAMEX> recognize the inevitable
        uncertainty and guess at its magnitude, most do not.
        Two highly publicized recent examples illustrate this.
        The death counts from the <TIMEX TYPE="DATE">September 11</TIMEX> attacks on <ENAMEX TYPE="GPE">New York</ENAMEX>
        were updated <TIMEX TYPE="TIME">hourly</TIMEX>, and reported to <NUMEX TYPE="CARDINAL">four</NUMEX> significant
        figures (the exact count). But the reports from the first
        few <TIMEX TYPE="DATE">weeks</TIMEX> turned out to be high by a factor of <NUMEX TYPE="CARDINAL">two</NUMEX>, making
        it quite clear that even the apparently precise counting of
        fatalities from a single event can only be estimated within
        a range of error. The vote count in <ENAMEX TYPE="GPE">Florida</ENAMEX> in the <TIMEX TYPE="DATE">2000</TIMEX>
        <ENAMEX TYPE="GPE">U.S.</ENAMEX> presidential election involved complete enumeration.
        <ENAMEX TYPE="PER_DESC">People</ENAMEX> were shocked that there was so much uncertainty -
        due to measurement and recording errors, among other things
        - in what they imagined to be a flawless mechanistic
        process. Few <ENAMEX TYPE="PER_DESC">people</ENAMEX> understood that the results from the
        various counts represented a statistical tie, and that
        choosing which vote count was the "right" one was a matter
        of legalistic detail rather than scientific truth. (Note
        that this considers only the votes as counted. The illegal
        disenfranchisement of <NUMEX TYPE="CARDINAL">tens of thousands</NUMEX> of eligible <ENAMEX TYPE="PER_DESC">voters</ENAMEX>
        - who would have almost certainly broken the tie - reminds
        us that uncorrected systematic bias can have much larger
        magnitude than the measured result. [ <ENAMEX TYPE="LAW">5</ENAMEX> ] )
      
      
        Analysis and discussion
        
          A simple method for quantifying errors in simple
          statistics
          A quick and easy way to avoid overstating precision is
          appropriate rounding, as taught to high school science
          <ENAMEX TYPE="PER_DESC">students</ENAMEX> (and largely ignored in health science reports,
          though a brief exposition of the point can be found in a
          recent epidemiology textbook [ [ <ENAMEX TYPE="LAW">9</ENAMEX> ] , <ENAMEX TYPE="PERSON">p.</ENAMEX><NUMEX TYPE="CARDINAL">51</NUMEX>]). This
          method is rough (and thus not perfectly well-defined),
          but it is a fairly effective shorthand: do not report
          significant digits (i.e., digits other than place-holder
          <ENAMEX TYPE="ORGANIZATION">zeros</ENAMEX>) beyond the level of precision of your estimate. If
          your point estimate for some value is <NUMEX TYPE="CARDINAL">2.3456</NUMEX>, but you
          think it is fairly likely that the true value is lower or
          higher by <NUMEX TYPE="PERCENT">as much as 5%</NUMEX>, only report <NUMEX TYPE="CARDINAL">2.3</NUMEX>. This can be
          interpreted as roughly, "we are pretty sure the result is
          between <NUMEX TYPE="CARDINAL">2.25</NUMEX> and <NUMEX TYPE="CARDINAL">2.35</NUMEX>, but cannot be much more precise."
          Similarly, if your estimate is <NUMEX TYPE="CARDINAL">87,654</NUMEX> but you know the
          measurement is only precise to plus-or-<NUMEX TYPE="QUANTITY">minus five</NUMEX>
          <ENAMEX TYPE="GPE">thousand</ENAMEX>, report <NUMEX TYPE="CARDINAL">90,000</NUMEX>.
          The limits of this method are clear when you consider
          what to report in the first example if you want to
          reflect confidence of plus-or-minus <NUMEX TYPE="PERCENT">15%</NUMEX>. Reporting <NUMEX TYPE="CARDINAL">2.3</NUMEX>
          implies a bit too much precision, but reporting 2 implies
          too little. It usually makes sense to imply a bit too
          much precision rather than too little (thus providing
          more information about the point estimate), but we should
          stop at the minimum level of <NUMEX TYPE="CARDINAL">over</NUMEX>-precision possible (<NUMEX TYPE="CARDINAL">2.3</NUMEX>
          in this case) and not imply more precision still (e.g.,
          by reporting <NUMEX TYPE="MONEY">2.35</NUMEX>).
          <TIMEX TYPE="DATE">Annual</TIMEX> U.S. automobile accident fatalities are
          reported to <NUMEX TYPE="CARDINAL">five</NUMEX> <ENAMEX TYPE="PER_DESC">figures</ENAMEX> (e.g., <NUMEX TYPE="CARDINAL">41,611</NUMEX> for <TIMEX TYPE="DATE">1999</TIMEX> [ <TIMEX TYPE="DATE">10</TIMEX> ] ),
          but when presenting this result for most purposes, it is
          better to report <NUMEX TYPE="CARDINAL">42,000</NUMEX>, roughly estimating the
          limitations of measurement (e.g., some deaths should be
          counted as suicides or were fatal cardiovascular events
          before the crash) and record keeping (e.g., cases
          inadvertently recorded and reported by <NUMEX TYPE="CARDINAL">two</NUMEX> different
          <ENAMEX TYPE="ORGANIZATION">jurisdictions</ENAMEX>).
          Notwithstanding the lack of a perfect rule, it should
          be clear when a result is presented with far too much
          <ENAMEX TYPE="ORGANIZATION">precision</ENAMEX>, as is often the case. <NUMEX TYPE="CARDINAL">One</NUMEX> of the most
          influential epidemiologic <ENAMEX TYPE="ORG_DESC">publications</ENAMEX> of <TIMEX TYPE="DATE">recent years</TIMEX>,
          <ENAMEX TYPE="ORGANIZATION">Kernan et al.</ENAMEX>'s [ <TIMEX TYPE="DATE">11</TIMEX> ] study of phenylpropanolamine and
          <ENAMEX TYPE="PERSON">stroke</ENAMEX> (that resulted in that popular decongestant and
          diet aid being removed from the <ENAMEX TYPE="GPE">U.S.</ENAMEX> market) reported one
          odds ratio of <NUMEX TYPE="MONEY">15.92</NUMEX>, even though one of the cell counts
          generating that ratio (exposed <ENAMEX TYPE="DISEASE">noncases</ENAMEX>) was exactly <NUMEX TYPE="CARDINAL">1</NUMEX>,
          and thus it could only be precise to <NUMEX TYPE="CARDINAL">about 1</NUMEX> part in <TIMEX TYPE="DATE">2</TIMEX>,
          not <NUMEX TYPE="CARDINAL">1</NUMEX> in <TIMEX TYPE="DATE">1000</TIMEX>. (Consider that if the <ENAMEX TYPE="FAC_DESC">cell</ENAMEX> count differed
          by the minimum possible, <NUMEX TYPE="CARDINAL">1</NUMEX>, the odds ratio would either
          be cut in <NUMEX TYPE="CARDINAL">half</NUMEX> or increased to infinity.) It is difficult
          to assess exactly what impact this misleading claim of
          <ENAMEX TYPE="ORGANIZATION">precision</ENAMEX> had on policy <ENAMEX TYPE="ORG_DESC">makers</ENAMEX> and other <ENAMEX TYPE="PER_DESC">readers</ENAMEX>, but we
          might suspect that it contributed to the unwarranted
          confidence in the study's findings.
          If a more formal quantification of uncertainty is
          reported - such as reporting "<NUMEX TYPE="MONEY">2.35</NUMEX> plus-or-minus <NUMEX TYPE="MONEY">0.12</NUMEX>"
          for the above example - then the significant digits are
          no longer the only reporting of uncertainty, and are not
          so important. Nevertheless, if <NUMEX TYPE="CARDINAL">2.3456</NUMEX> appears in a paper,
          there is a risk that it will be repeated out of context
          in all its implicit precision, without the +/-<NUMEX TYPE="CARDINAL">.12</NUMEX>
          clarification.
          It should be noted that rounding to an appropriate
          number of significant digits (or any other method in this
          paper) does not 
          create any imprecision; the
          imprecision exists even if we do not accurately report
          it.
        
        
          Improved quantification of errors in simple
          statistics
          
            Example
            A report presented to the public and policy makers
            <ENAMEX TYPE="GPE_DESC">states</ENAMEX> that <NUMEX TYPE="PERCENT">2.76 percent</NUMEX> of the <ENAMEX TYPE="PER_DESC">people</ENAMEX> in a community
            have been diagnosed with a certain <ENAMEX TYPE="DISEASE">disease</ENAMEX> during a
            <TIMEX TYPE="DATE">one-year</TIMEX> period, based on active monitoring that
            identified <NUMEX TYPE="CARDINAL">8650</NUMEX> <ENAMEX TYPE="PER_DESC">people</ENAMEX> diagnosed out of a <ENAMEX TYPE="PER_DESC">community</ENAMEX> of
            <NUMEX TYPE="CARDINAL">312,962</NUMEX>. Because the study method was a complete
            enumeration, there is no random process, and thus no
            frequentist error statistics. The resulting lack of a
            confidence interval means that no statement of error
            accompanied the result. It is certain, however, that
            there is still error. In particular, the <ENAMEX TYPE="PER_DESC">researchers</ENAMEX>
            believe that <TIMEX TYPE="DATE">8650</TIMEX> is an undercount of <NUMEX TYPE="PERCENT">as much as 20%</NUMEX>,
            due to the likely inability of the monitoring system to
            detect all cases.
            The total <ENAMEX TYPE="PER_DESC">population</ENAMEX> of the <ENAMEX TYPE="GPE_DESC">community</ENAMEX> is also
            uncertain, but this is inconsequential (by the above
            definition). If reporting the figure in a final report,
            it would probably be appropriate to report <NUMEX TYPE="CARDINAL">313,000</NUMEX>
            rather than the <NUMEX TYPE="CARDINAL">six</NUMEX> figures, but this uncertainty is
            dwarfed by that of the numerator (on the order of <NUMEX TYPE="CARDINAL">1</NUMEX>
            part in <NUMEX TYPE="CARDINAL">100</NUMEX> or even <NUMEX TYPE="CARDINAL">1</NUMEX> in <TIMEX TYPE="DATE">1000</TIMEX>, compared to <NUMEX TYPE="CARDINAL">1</NUMEX> in <TIMEX TYPE="DATE">10</TIMEX> for
            the numerator), and so can be ignored in the
            calculation. Even setting aside the downward bias, the
            <ENAMEX TYPE="ORGANIZATION">precision</ENAMEX> implied by <NUMEX TYPE="PERCENT">2.76 percent</NUMEX> - that we are fairly
            confident that we know the true value to <NUMEX TYPE="CARDINAL">about 1</NUMEX> part
            in <NUMEX TYPE="CARDINAL">100</NUMEX> - is unwarranted.
            After further contemplation and examination of
            validation data, the <ENAMEX TYPE="PER_DESC">researchers</ENAMEX> decide that their best
            estimate is that the raw estimate is low by <NUMEX TYPE="CARDINAL">between 0</NUMEX>
            and <NUMEX TYPE="PERCENT">20 percent</NUMEX> of the estimated value, uniformly
            distributed. The process by which they came to this
            conclusion - possibly involving a series of
            calculations based on the quality of monitoring, test
            <ENAMEX TYPE="PERSON">sensitivity</ENAMEX>, etc. - is beyond the present scope. One
            might dispute the implicit claim that there is no
            chance of false positives, but it should be remembered
            that uncertainty distributions are never going to be
            perfect or beyond criticism. The <ENAMEX TYPE="PER_DESC">researchers</ENAMEX> are simply
            of the opinion that the number of false positives will,
            with extremely high probability, be exceeded by the
            <ENAMEX TYPE="ORGANIZATION">undercount</ENAMEX>.
            (Rather than bundling these <NUMEX TYPE="CARDINAL">two</NUMEX> <ENAMEX TYPE="PER_DESC">sources</ENAMEX> of error
            into a single distribution, using intuition that may or
            may not be right, the <ENAMEX TYPE="PER_DESC">researchers</ENAMEX> might have been
            better off reading ahead and using one of the methods
            for combining multiple <ENAMEX TYPE="PER_DESC">sources</ENAMEX> of uncertainty. Had they
            done so, they might well have concluded that the
            misclassification error was indeed dwarfed by the
            under-reporting, and returned to a single
            <ENAMEX TYPE="PERSON">quantification</ENAMEX>.)
            The result of the <ENAMEX TYPE="PER_DESC">researchers</ENAMEX>' uncertainty
            distribution is a uniform distribution for the annual
            disease incidence over the range [<NUMEX TYPE="PERCENT">2.76%,3.32%</NUMEX>]. How
            should this be reported? <NUMEX TYPE="CARDINAL">One</NUMEX> option is to report <NUMEX TYPE="PERCENT">3%</NUMEX>,
            which, conveniently, is the rounded result for the
            entire range. It accurately implies that our precision
            (with a known error of <NUMEX TYPE="PERCENT">up to 10%</NUMEX> on either side of the
            mean) warrants <NUMEX TYPE="CARDINAL">only about one</NUMEX> significant figure. To
            provide more precision, the certainty interval
            containing <NUMEX TYPE="PERCENT">50% or 90%</NUMEX> of the probability mass could be
            reported. (<ENAMEX TYPE="WORK_OF_ART">Again</ENAMEX>, without implying too much precision
            for the boundaries. An uncertainty distribution should
            itself not be stated in an overly-precise manner.) It
            is usually not a good idea to report the extremes and
            imply that the corrected value certainly falls between
            them. Extreme values can be misleading to the <ENAMEX TYPE="PER_DESC">reader</ENAMEX>.
            They are also very sensitive to the exact input
            distributions used, such as in the current example,
            where the input distributions with <NUMEX TYPE="CARDINAL">zero</NUMEX> probability
            beyond some range are good estimates for most of the
            probability mass, but they exclude extreme values that
            the <ENAMEX TYPE="PER_DESC">researchers</ENAMEX> do not actually believe have zero
            probability.
          
        
        
          The choice and nature of subjective uncertainty
          distributions
          A detailed assessment of what needs to be considered
          in developing uncertainty distributions for inputs in
          this kind of analysis is beyond the present scope, but it
          is worth making a few comments to provide perspective and
          help <ENAMEX TYPE="PER_DESC">researchers</ENAMEX> get started. The uniform distribution in
          the preceding example provides the easiest teaching
          example, but is probably not realistic. Even interpreting
          it as an approximation, it is unlikely that someone
          believes some range of values are approximately equally
          likely, but a value slightly outside that range is
          (<NUMEX TYPE="MONEY">approximately</NUMEX>) impossible.
          Typically, we have a point estimate and think the true
          value of the parameter is likely near it and the
          probability drops off as we deviate in either direction.
          This describes various distributions, including the
          normal, logistic, triangular (where the probability
          <ENAMEX TYPE="PERSON">density</ENAMEX> is unimodal, dropping off <NUMEX TYPE="CARDINAL">linearly to zero</NUMEX> in
          each direction, forming a triangular density function),
          and others. The choice among distribution shapes can be
          made largely based on whether the <ENAMEX TYPE="PER_DESC">researcher</ENAMEX> wants to
          allow values to trail off to infinity or not and whether
          the distribution is symmetrical. A triangular
          distribution, while seldom appearing in nature, might
          effectively approximate someone's beliefs, and has the
          advantage for pedagogic purposes of allowing calculations
          using polynomial algebra. Normal and logistic
          distributions are easy to work with using numerical
          methods.
          It turns out that the choice of the exact shape of the
          distribution, after the rough magnitude of uncertainty
          has been determined, is relatively unimportant. Estimates
          like those presented here are fairly stable across
          unimodal distribution shapes, as long as the probability
          <ENAMEX TYPE="GPE">mass</ENAMEX> is substantially overlapping. (<ENAMEX TYPE="ORGANIZATION">I.e.</ENAMEX>, if two
          distributions have a very similar range for the middle
          <NUMEX TYPE="PERCENT">50%</NUMEX> of their probability mass and also for the middle
          <NUMEX TYPE="PERCENT">90%</NUMEX>, they will have very similar implications in these
          uncertainty calculations.) It should be remembered that
          the purpose of these methods is to better represent
          uncertainty, and that goal is not well served by claiming
          too much precision about the details of the inputs and
          calculations.
          The question of whether an input distribution
          <ENAMEX TYPE="ORGANIZATION">corresponds</ENAMEX> to something found in nature brings up a
          complicated philosophy-of-statistics question: What
          exactly are these probabilities? To give an abbreviated
          answer, they are subjective probabilities that take into
          consideration all of the <ENAMEX TYPE="PER_DESC">researcher</ENAMEX>'s knowledge, except
          the point estimate for the parameter of interest they
          have calculated (in the above example that would be the
          <TIMEX TYPE="DATE">annual</TIMEX> disease incidence) and any prior beliefs about
          what the true value of that parameter is. The
          subjectivity of this probability should not be seen as
          surprising or regarded as a limitation. All of scientific
          inquiry, from hypothesis <ENAMEX TYPE="PER_DESC">generation</ENAMEX> to study design to
          drawing conclusions, is a highly subjective and
          sociologic process. Furthermore, the alternative to
          specifying such a distribution is to produce calculations
          based on the assumption that there is zero uncertainty,
          which is either a subjective belief itself or (more
          likely) is strongly believed to be wrong.
          The restriction that prior beliefs about the true
          value should be excluded from the <ENAMEX TYPE="PER_DESC">researchers</ENAMEX>' generation
          of the input probabilities is a subtlety that relates to
          how we can interpret the results and how the resulting
          uncertainty would relate to random error if it were
          included in the calculations. While it is not necessary
          to delve deeply into <ENAMEX TYPE="NATIONALITY">Bayesian</ENAMEX> analysis to do the simple
          calculations proposed in this <ENAMEX TYPE="ORG_DESC">paper</ENAMEX> (or to generally
          revise our thinking about how certain most results are),
          a formal interpretation of the quantified uncertainty of
          a result is that it is a <ENAMEX TYPE="ORGANIZATION">Bayesian</ENAMEX> posterior distribution
          based on a prior distribution (i.e., belief about the
          distribution before the research in question) that
          assigns equal likelihood to all values in the relevant
          range. To understand the importance of the prior
          distribution, consider the possibility that one of the
          <ENAMEX TYPE="PER_DESC">researchers</ENAMEX> in the previous example was very confident
          that the actual incidence of the disease was above <NUMEX TYPE="PERCENT">3%</NUMEX>. In
          that case, upon seeing the results of the study, she
          would not believe that the whole range was equally
          likely, but instead would think the upper end of it was
          more likely because her new beliefs would be based on a
          combination of the study result and what she knew
          before.
          The implicit assumption that all possible values were
          equally likely (called a "flat prior") is problematic,
          because a flat distribution across all possible values is
          never realistic. The next step in improving these
          calculations should be to relax that assumption. However,
          the problems inherent in the implicit assumption of a
          flat prior (which makes the calculations much easier to
          perform and to understand) are reduced by a few factors.
          First, the "relevant range" condition says that the prior
          only needs to be flat across the range that contains most
          of the probability mass resulting from the calculation
          (e.g., in the above example, it would only have to be
          flat across [<NUMEX TYPE="PERCENT">2.76%,3.32%</NUMEX>]). This means that the worst
          problem of a totally flat prior, that unrealistically
          extreme values have to be considered to be as likely as
          realistic values, is absent. Furthermore, the intuition
          we as <ENAMEX TYPE="PER_DESC">readers</ENAMEX> have learned from <TIMEX TYPE="DATE">years</TIMEX> of interpreting
          point estimates and frequentist confidence intervals is
          to, roughly, treat them as calculations based on flat
          <ENAMEX TYPE="ORGANIZATION">priors</ENAMEX> and then roughly incorporate them into whatever
          actual prior belief we have. That is, if a study reports
          an estimate of <NUMEX TYPE="CARDINAL">3</NUMEX> or some interval around it, and we were
          previously quite sure that the true value was <NUMEX TYPE="CARDINAL">5</NUMEX> or more,
          the new evidence might push our beliefs downward, but it
          is not going to replace them. This is just as true if the
          <ENAMEX TYPE="PERSON">interval</ENAMEX> is a standard confidence interval (based only on
          <ENAMEX TYPE="ORGANIZATION">random</ENAMEX> sampling error) or an uncertainty quantification,
          and our practiced intuition will be useful until these
          methods are advanced.
        
        
          Increasing complexity
          
            Example, continued
            The <ENAMEX TYPE="PER_DESC">researchers</ENAMEX> wish to extrapolate the frequency of
            disease from the study <ENAMEX TYPE="PER_DESC">community</ENAMEX> to estimate the total
            cases for the entire <ENAMEX TYPE="GPE_DESC">state</ENAMEX>, with a <ENAMEX TYPE="PER_DESC">population</ENAMEX> of
            <NUMEX TYPE="CARDINAL">10,456,000</NUMEX>. A naive way to introduce quantified
            uncertainty into the calculation would be to treat the
            original study of <NUMEX TYPE="CARDINAL">312,962</NUMEX> <ENAMEX TYPE="PER_DESC">people</ENAMEX> as a random sample
            from a <ENAMEX TYPE="PER_DESC">population</ENAMEX> of <NUMEX TYPE="CARDINAL">10,456,000</NUMEX>. The result could be
            quantified using the usual frequentist statistical
            methods, with the result misleadingly suggesting high
            <ENAMEX TYPE="ORGANIZATION">precision</ENAMEX> (a <NUMEX TYPE="PERCENT">95%</NUMEX> confidence interval of (<NUMEX TYPE="MONEY">2.71,2.82</NUMEX>)).
            But greater uncertainty is introduced by the
            extrapolation of the results, which introduces unknown
            levels of non-stochastic error. Perhaps the sample
            <ENAMEX TYPE="ORGANIZATION">community</ENAMEX> was studied because it was particularly
            convenient, because it was more urban or had a better
            local health <ENAMEX TYPE="ORG_DESC">department</ENAMEX>, and so is different from the
            state average.
            The <ENAMEX TYPE="PER_DESC">researchers</ENAMEX> do not know the specific amount of
            <ENAMEX TYPE="PERSON">bias</ENAMEX> for their estimate, but they recognize that there
            is likely some bias. Their best estimate is that the
            actual rate for the <ENAMEX TYPE="GPE_DESC">state</ENAMEX> is most likely within +/-<NUMEX TYPE="PERCENT">10%</NUMEX>
            of the sample <ENAMEX TYPE="PER_DESC">community</ENAMEX>, but it is plausible that the
            <ENAMEX TYPE="ORGANIZATION">extrapolation</ENAMEX> is off by <NUMEX TYPE="PERCENT">as much as 25%</NUMEX>. To fit these
            <ENAMEX TYPE="PERSON">probabilities</ENAMEX>, the <ENAMEX TYPE="PER_DESC">researchers</ENAMEX> use a symmetrical
            triangular distribution from <NUMEX TYPE="CARDINAL">.75</NUMEX> to <NUMEX TYPE="CARDINAL">1.25</NUMEX> of the point
            estimate (i.e., <NUMEX TYPE="CARDINAL">zero</NUMEX> probability density at <NUMEX TYPE="CARDINAL">.75</NUMEX>,
            <ENAMEX TYPE="ORGANIZATION">linearly</ENAMEX> increasing to the midpoint, <NUMEX TYPE="CARDINAL">1.0</NUMEX>, and linearly
            decreasing to <NUMEX TYPE="CARDINAL">zero</NUMEX> at <NUMEX TYPE="MONEY">1.25</NUMEX>). They could have chosen a
            normal distribution or various other similarly-shaped
            distributions to represent these beliefs with similar
            <ENAMEX TYPE="PERSON">outcomes</ENAMEX>.
            This new source of error now combines with the
            original underestimate to produce a probability density
            for the total number of cases in the <ENAMEX TYPE="GPE_DESC">state</ENAMEX>. The
            additional uncertainty from random sampling error is
            small and can be ignored as inconsequential.
            Alternatively, the random sampling error could be
            incorporated into the <ENAMEX TYPE="PER_DESC">researchers</ENAMEX>' subjective
            probability. (<ENAMEX TYPE="ORGANIZATION">Objectively</ENAMEX> determinable stochastic
            processes can be brought into the uncertainty
            calculation differently from subjective uncertainty,
            but this introduces complexity that is left for future
            analyses. The purpose of the present analysis is to
            consider cases where sampling error is absent or is
            insignificant compared to other sources of error.)
            The density for a given final value, x, which
            results from a calculation involving <NUMEX TYPE="CARDINAL">two</NUMEX> uncertain
            values is the integral across values of the two
            functions that produce x. In the present case, this is
            relatively simple to calculate. The probability
            distribution for the total number of cases in the state
            is described by the continuous approximation:
            
            where the definition of <ENAMEX TYPE="ORGANIZATION">g</ENAMEX>(<ENAMEX TYPE="ORGANIZATION">t</ENAMEX>) describes the
            triangular distribution and h(s) the uniform
            distribution, and k is the scale factor to make f(x) a
            probability density function. The continuous
            <ENAMEX TYPE="ORGANIZATION">approximation</ENAMEX> is necessary not just for computational
            <ENAMEX TYPE="ORGANIZATION">convenience</ENAMEX>, but because the form of the error
            distributions was continuous. Since our practical
            interest is for ranges of values, and not the exact
            probability for a given value, nothing is lost by this
            <ENAMEX TYPE="ORGANIZATION">approximation</ENAMEX>.
            For this distribution, the middle <NUMEX TYPE="PERCENT">90%</NUMEX> of the
            probability mass falls in the range <NUMEX TYPE="CARDINAL">260,000 to 380,000</NUMEX>.
            A normal distribution with a mean of <NUMEX TYPE="CARDINAL">1.0</NUMEX> and a standard
            deviation of <NUMEX TYPE="MONEY">.11</NUMEX> would also have represented the
            <ENAMEX TYPE="PER_DESC">researchers</ENAMEX>' beliefs about the bias from extrapolation,
            and would have yielded the same interval (after
            <ENAMEX TYPE="ORGANIZATION">rounding</ENAMEX>) for the middle <NUMEX TYPE="PERCENT">90%</NUMEX> of the probability
            <ENAMEX TYPE="GPE">mass.</ENAMEX>
            Solving this equation (and thus figuring out
            uncertainty intervals) is easy. But adding much more
            <ENAMEX TYPE="PERSON">complication</ENAMEX> makes it unwieldy. <NUMEX TYPE="CARDINAL">A third</NUMEX> layer of
            multiplicative uncertainty would require a double
            <ENAMEX TYPE="PERSON">integral</ENAMEX> over a more complicated product, and so on. An
            uncertain input that entered the equation other than by
            <ENAMEX TYPE="PERSON">multiplying</ENAMEX> would be more complicated still. Indeed,
            simply using the normal distribution for the
            uncertainty from the extrapolation would make this
            calculation considerably more complicated. The
            implication is clear: with <NUMEX TYPE="CARDINAL">more than a few</NUMEX> simple
            <ENAMEX TYPE="PER_DESC">sources</ENAMEX> of uncertainty, closed-form (analytic)
            calculation is not a practical method for quantifying
            it.
          
        
        
          Estimating complex combinations of
          uncertainty
          Any calculation with a large number of inputs is
          likely to resist closed-form calculation of uncertainty
          and intuitive statements about total uncertainty are
          likely to be worthless. ("<ENAMEX TYPE="PERSON">Large</ENAMEX>," in this case, can mean
          as few <NUMEX TYPE="CARDINAL">as three or four</NUMEX> inputs if they all introduce
          uncertainty.) However, there are tools developed in
          <ENAMEX TYPE="ORGANIZATION">finance</ENAMEX> and engineering that can be used to calculate
          uncertainty in such health research.
          To estimate the probability density for parameters of
          interest given multiple uncertain input values, we
          propose using <ENAMEX TYPE="ORGANIZATION">Monte Carlo</ENAMEX> (random number-based) numerical
          methods as follows:
          <NUMEX TYPE="CARDINAL">1</NUMEX>. Probability distributions are specified for the
          <ENAMEX TYPE="PERSON">inputs</ENAMEX>, as presented above.
          <NUMEX TYPE="CARDINAL">2</NUMEX>. A random draw is made from each of those
          <ENAMEX TYPE="PERSON">distributions</ENAMEX>, producing <NUMEX TYPE="CARDINAL">one</NUMEX> set of possible true values.
          The calculation (the same <NUMEX TYPE="CARDINAL">one</NUMEX> used to generate the point
          estimate) is carried out for those values to produce a
          possible final value of the estimate.
          <NUMEX TYPE="CARDINAL">3</NUMEX>. Step <NUMEX TYPE="CARDINAL">2</NUMEX> is iterated a large number of times,
          producing a new <ENAMEX TYPE="PER_DESC">candidate</ENAMEX> value for each new set of
          <ENAMEX TYPE="ORGANIZATION">random</ENAMEX> draws.
          <NUMEX TYPE="CARDINAL">4</NUMEX>. These values are recorded, and can then be used to
          calculate the probability of the true value being in a
          particular interval or grouped into fixed-width intervals
          and represented by a histogram that approximates the
          probability density function for the total
          uncertainty.
          This approach takes a difficult problem and
          approximates the answer by carrying out simple
          calculations a large number of times. Since <TIMEX TYPE="DATE">these</TIMEX> simple
          calculations are the ones that had to be constructed to
          get the point estimate in the <NUMEX TYPE="ORDINAL">first</NUMEX> place, a relatively
          small amount of extra effort is required. <ENAMEX TYPE="ORGANIZATION">Monte Carlo</ENAMEX>
          simulations of this sort are used extensively for similar
          calculations of uncertainty in business and engineering
          applications (often under the rubric "risk analysis"),
          and so there is user-friendly off-the-shelf software that
          does these calculations. (Further background in these
          applications is available, at the time of writing, from
          the <ENAMEX TYPE="ORG_DESC">manufacturer</ENAMEX> of the software we used at
          <ENAMEX TYPE="CONTACT_INFO">http://www.</ENAMEX><ENAMEX TYPE="ORGANIZATION">crystalball</ENAMEX>.<ENAMEX TYPE="CONTACT_INFO">com/risk-analysis-</ENAMEX>start.<ENAMEX TYPE="ORGANIZATION">html</ENAMEX>.)
          Extremely complicated <ENAMEX TYPE="GPE">Monte Carlo</ENAMEX> simulations are used to
          model everything from nuclear explosions to biological
          evolution, but the tools needed for present purposes are
          usable by any competent quantitative <ENAMEX TYPE="PER_DESC">researcher</ENAMEX>.
          <ENAMEX TYPE="ORGANIZATION">Monte Carlo</ENAMEX> uncertainty calculations have been
          proposed for the errors in a typical epidemiologic study,
          [ <NUMEX TYPE="CARDINAL">1 2 3 4 6 7</NUMEX> ] which are much more complicated than the
          errors considered in the examples presented here. Such
          applications involve complicated interactions of random
          error, selection bias, measurement error, and other
          <ENAMEX TYPE="PER_DESC">sources</ENAMEX> of uncertainty that <ENAMEX TYPE="FAC_DESC">compound</ENAMEX> in mathematically
          complicated ways. For the straightforward adding and
          <ENAMEX TYPE="PERSON">multiplying</ENAMEX> used in the examples presented here, these
          calculations are simple to program and do not require
          much computer time.
          Indeed, the biggest challenge for quantifying
          uncertainty in these calculations - quantifying the
          various input uncertainties - is partially ameliorated by
          the ease with which different values can be run to
          produce sensitivity analyses. While sensitivity analyses
          cannot determine which values are better to use, they can
          point out which ones matter enough to warrant more
          attention.
        
        
          A <ENAMEX TYPE="GPE">Monte Carlo</ENAMEX>-based uncertainty calculation
          As an example of a calculation combining many sources
          of uncertainty, we use a simplified version of <ENAMEX TYPE="ORGANIZATION">Mead et</ENAMEX>
          <ENAMEX TYPE="PERSON">al.</ENAMEX>'s frequently-quoted calculation of the incidence of
          <ENAMEX TYPE="DISEASE">foodborne disease</ENAMEX> in the <ENAMEX TYPE="GPE">U.S.</ENAMEX> [ <TIMEX TYPE="DATE">12</TIMEX> ] (The present
          example, a highly simplified version of the second
          <ENAMEX TYPE="PER_DESC">author</ENAMEX>'s master thesis, [ <TIMEX TYPE="DATE">13</TIMEX> ] is intended primarily to
          illustrate the method rather than explore the details of
          food safety data. <ENAMEX TYPE="PERSON">Powell</ENAMEX>, <ENAMEX TYPE="GPE">Ebel</ENAMEX>, and <ENAMEX TYPE="ORGANIZATION">Schlosser</ENAMEX> have also
          conducted a <ENAMEX TYPE="GPE">Monte-Carlo</ENAMEX>-based analysis of the uncertainty
          in some of <ENAMEX TYPE="ORGANIZATION">Mead et al.</ENAMEX>'s numbers. [ <TIMEX TYPE="DATE">14</TIMEX> ] An in-depth
          analysis of the uncertainty for a particular foodborne
          disease risk can be found in <ENAMEX TYPE="ORGANIZATION">Vose et al.</ENAMEX> [ <TIMEX TYPE="DATE">15</TIMEX> ] )
          <ENAMEX TYPE="ORGANIZATION">Mead et al.</ENAMEX>'s calculation was based on literally
          <NUMEX TYPE="CARDINAL">hundreds</NUMEX> of input numbers, including:
          • the <ENAMEX TYPE="GPE">U.S.</ENAMEX> rate of total <ENAMEX TYPE="DISEASE">gastrointestinal illness</ENAMEX>,
          <ENAMEX TYPE="DISEASE">• foodborne illness</ENAMEX> case counts from passive
          <ENAMEX TYPE="PERSON">surveillance</ENAMEX> (data reported voluntarily from <ENAMEX TYPE="ORG_DESC">clinics</ENAMEX>) and
          outbreak reports (health <ENAMEX TYPE="ORG_DESC">department</ENAMEX> investigations),
          <ENAMEX TYPE="PERSON">•</ENAMEX> case counts from active surveillance (attempts to
          enumerate every case) at <NUMEX TYPE="CARDINAL">five</NUMEX> <ENAMEX TYPE="GPE">county</ENAMEX>- or state-level
          sites,
          • several estimates of how many unobserved cases are
          represented by <NUMEX TYPE="CARDINAL">one</NUMEX> observed case (in order to extrapolate
          from data to true values),
          <ENAMEX TYPE="PERSON">•</ENAMEX> estimates of the fraction of cases of each disease
          that should be attributed to <ENAMEX TYPE="SUBSTANCE">food</ENAMEX>,
          • several other extrapolations.
          The need to understand and quantify uncertainty is
          obvious when we observe that <ENAMEX TYPE="ORGANIZATION">Mead et al.</ENAMEX>, despite the
          clearly uncertain data and calculations, emphasized (in
          the abstract and press releases) a result to <NUMEX TYPE="CARDINAL">two</NUMEX>
          significant figures, <NUMEX TYPE="MONEY">76 million U.S.</NUMEX> cases per year, with
          no statement of uncertainty or a plausible range of
          values. Widely quoted in this form, this number implies
          that the <ENAMEX TYPE="PER_DESC">experts</ENAMEX> are confident that the result is not <NUMEX TYPE="CARDINAL">75</NUMEX>
          <NUMEX TYPE="CARDINAL">million</NUMEX> or <NUMEX TYPE="MONEY">77 million</NUMEX>, but is <NUMEX TYPE="CARDINAL">somewhere between 75.5</NUMEX> and
          <NUMEX TYPE="MONEY">76.5 million</NUMEX>. After all, they did not report <NUMEX TYPE="MONEY">80 million</NUMEX>,
          which would tend to imply less precision. (Note a
          weakness of relying on significant figures alone: If they
          had reported <NUMEX TYPE="MONEY">75 million</NUMEX>, it would not have been clear
          whether they were rounding to the nearest <TIMEX TYPE="DATE">1, 5</TIMEX>, or <NUMEX TYPE="CARDINAL">25</NUMEX>
          <ENAMEX TYPE="PERSON">million</ENAMEX>. This would only become clear if multiple
          numbers, all with the same rounding, were reported.) The
          <ENAMEX TYPE="PER_DESC">body</ENAMEX> of the <ENAMEX TYPE="ORGANIZATION">Mead et</ENAMEX> al. paper actually contains an
          estimate to <NUMEX TYPE="CARDINAL">ten</NUMEX> significant figures. (Such overly precise
          reporting could be justified in a paper if intended to
          help the <ENAMEX TYPE="PER_DESC">reader</ENAMEX> duplicate the calculations, rather than
          as a conclusory statement. This is unlikely to be the
          explanation in the present case, since their calculation
          is difficult to duplicate from the information given and
          this kind of replication is not common in health
          research.)
          <ENAMEX TYPE="ORGANIZATION">Monte Carlo</ENAMEX>-based numerical methods allow us to
          estimate the uncertainty for calculations as complicated
          as <ENAMEX TYPE="ORGANIZATION">Mead et al.'s</ENAMEX>. The complexity of their effort to use
          existing, highly incomplete data to estimate <ENAMEX TYPE="GPE">U.S.</ENAMEX>
          foodborne disease incidence is illustrated by the
          spreadsheet we developed to duplicate their calculations,
          which includes <NUMEX TYPE="CARDINAL">over 200</NUMEX> numerical cells, <NUMEX TYPE="CARDINAL">more than 50</NUMEX> of
          which are inputs from outside information. Even if we
          believe that the analysis reflected science's best
          knowledge about these values, we can be sure that such an
          estimate is not accurate to better than <NUMEX TYPE="PERCENT">2%</NUMEX>. But what more
          can we say about the range of possible values?
          To answer this, we examined the various inputs and the
          certainty of their <ENAMEX TYPE="PER_DESC">sources</ENAMEX>, and developed a model that
          <ENAMEX TYPE="ORGANIZATION">incorporated</ENAMEX> estimates for each source of uncertainty.
          For the current example, we use a simplified version of
          the calculation, reducing the list of <NUMEX TYPE="CARDINAL">28</NUMEX> different
          diseases to the <NUMEX TYPE="CARDINAL">3</NUMEX> that contributed the most to the total
          plus an "other" category, and simplifying some of the
          multiplicative correction factors used in the original.
          The first example presented here uses conservative
          uncertainty distributions that are relatively small and
          mean-preserving (i.e., they use the original <ENAMEX TYPE="ORGANIZATION">Mead et</ENAMEX> al.
          point estimates as the distribution mean). Even with this
          optimistic level of uncertainty, it is easy to see the
          need to avoid implying too much precision.
          The calculation is summarized in <ENAMEX TYPE="PRODUCT">Table 1</ENAMEX>. It starts
          with incidence rates of several diseases that are
          partially attributable to foodborne transmission. <NUMEX TYPE="CARDINAL">Two</NUMEX> of
          the <NUMEX TYPE="CARDINAL">three</NUMEX> major contributors are based on incomplete
          <ENAMEX TYPE="SUBSTANCE">samples</ENAMEX> from monitoring efforts. They are multiplied by a
          factor of <NUMEX TYPE="CARDINAL">38</NUMEX> to estimate the total incidence. The total
          incidence of each <ENAMEX TYPE="DISEASE">disease</ENAMEX> is then multiplied by an
          estimate of the portion of cases that are foodborne.
          These figures are summed across the <ENAMEX TYPE="DISEASE">diseases</ENAMEX> to get the
          incidence of foodborne illnesses attributable to known
          <ENAMEX TYPE="ORGANIZATION">pathogens</ENAMEX>. The total cases from unknown <ENAMEX TYPE="PER_DESC">sources</ENAMEX> is then
          calculated by estimating the total cases of
          <ENAMEX TYPE="ORGANIZATION">gastroenteritis</ENAMEX> in the <ENAMEX TYPE="GPE">U.S.</ENAMEX> and subtracting the cases of
          known origins. To get the portion of these that are
          attributable to foodborne pathogens, <ENAMEX TYPE="ORGANIZATION">Mead et al.</ENAMEX> assumed
          that the foodborne portion is the same as that for known
          <ENAMEX TYPE="ORGANIZATION">pathogens</ENAMEX>. This result is added to the incidence from
          known pathogens to get a total.
          Every <NUMEX TYPE="CARDINAL">one</NUMEX> of these inputs introduces uncertainty. To
          reflect this, we introduced the following distributions.
          (Most of these distributions have some external basis.
          However, this calculation should be seen primarily as a
          demonstration of the methods for doing the analysis and a
          rough estimate of the minimal uncertainty in <ENAMEX TYPE="ORGANIZATION">Mead et</ENAMEX>
          <ENAMEX TYPE="PERSON">al.</ENAMEX>'s number, rather than a claim that these are the
          right distributions.) For the total cases of each of the
          <NUMEX TYPE="CARDINAL">three</NUMEX> identified diseases, the point estimate is replaced
          by a normal distribution, with a mean of the point
          estimate and a standard deviation of <NUMEX TYPE="PERCENT">10 percent</NUMEX> of the
          point estimate. For the <NUMEX TYPE="CARDINAL">25</NUMEX> other pathogens, which each
          contributed a relatively small portion of the total, we
          simply used the point estimates because the uncertainty
          for each is inconsequential. (Assuming their errors are
          uncorrelated, they tend to average out, leaving a
          relatively tight distribution of total uncertainty. If
          <NUMEX TYPE="CARDINAL">one</NUMEX> believes that inaccuracies in the estimates of
          incidence rates are correlated across diseases, the
          overall uncertainty would be greater.) For the
          multiplicative <ENAMEX TYPE="SUBSTANCE">factor</ENAMEX> used for <NUMEX TYPE="CARDINAL">two</NUMEX> of those, we used a
          symmetrical triangular distribution centered on the
          original <NUMEX TYPE="MONEY">38</NUMEX>, with a range of (<NUMEX TYPE="MONEY">24,52</NUMEX>).
          For the percent of each disease attributable to <ENAMEX TYPE="SUBSTANCE">food</ENAMEX>,
          we used the point estimates for <NUMEX TYPE="CARDINAL">two</NUMEX> of the pathogens
          because they were fairly close to <NUMEX TYPE="PERCENT">100%</NUMEX> (leaving little
          room for error) and appeared to be reasonably solid
          estimates. The remaining pathogen, <ENAMEX TYPE="GPE">Norwalk</ENAMEX>-like viruses,
          accounts for most of the total cases and the fraction
          that are foodborne is highly uncertain and far from
          either <NUMEX TYPE="CARDINAL">0</NUMEX> or <NUMEX TYPE="PERCENT">100%</NUMEX>, leaving room for substantial variation.
          Not only does this percentage affect the estimated number
          of foodborne cases of that <ENAMEX TYPE="DISEASE">disease</ENAMEX>, but it dominates the
          overall estimated percentage of gastroenteritis cases
          attributed to <ENAMEX TYPE="SUBSTANCE">food</ENAMEX>, and thus the estimate for cases of
          unknown etiology. Given the large impact of uncertainty
          in this input parameter, we conduct a sensitivity
          analysis for its impact below. For the initial example,
          we modeled the uncertainty as a uniform distribution on
          [<NUMEX TYPE="PERCENT">20%,60%</NUMEX>], centered on the <ENAMEX TYPE="ORGANIZATION">Mead et</ENAMEX> al. point estimate of
          <NUMEX TYPE="PERCENT">40 percent</NUMEX>.
          For total cases of gastroenteritis, we used a normal
          distribution with a mean of the original point estimate,
          and a standard deviation of <NUMEX TYPE="PERCENT">20 percent</NUMEX> of the original
          estimate. To represent the uncertainty of the assumption
          that the portion of gastroenteritis cases of unknown
          origin attributable to foodborne pathogens is the same as
          for cases of known origin, we draw the portion of unknown
          cases from a normal distribution around the portion of
          known cases (after it is calculated, since it varies with
          each iteration) with a standard deviation of <NUMEX TYPE="CARDINAL">8</NUMEX> percentage
          points.
          This example was constructed and calculated using a
          <ENAMEX TYPE="ORGANIZATION">Microsoft</ENAMEX> <ENAMEX TYPE="PRODUCT">Excel</ENAMEX> spreadsheet and the off-the-shelf <ENAMEX TYPE="ORGANIZATION">Monte</ENAMEX>
          <ENAMEX TYPE="PERSON">Carlo</ENAMEX> simulation package, <ENAMEX TYPE="ORGANIZATION">Crystal Ball</ENAMEX> (Decisioneering
          <ENAMEX TYPE="ORGANIZATION">Inc.</ENAMEX>, <ENAMEX TYPE="GPE">Denver</ENAMEX>, <ENAMEX TYPE="GPE">Colorado</ENAMEX>). We ran <NUMEX TYPE="CARDINAL">half a million</NUMEX> iterations
          of the <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX>, producing the histogram in Figure 1that
          approximates the probability density for the total number
          of cases that results from these input uncertainties. It
          would overstate the quality of our estimates to interpret
          this as providing precise probability estimates. But the
          rough cut at estimating the total uncertainty is very
          informative. Even with these relatively conservative
          estimates of uncertainty, chances are <NUMEX TYPE="CARDINAL">about half</NUMEX> that the
          real total is outside the range of <NUMEX TYPE="MONEY">50 million</NUMEX> to <NUMEX TYPE="CARDINAL">100</NUMEX>
          <ENAMEX TYPE="PERSON">million</ENAMEX>.
          (The <ENAMEX TYPE="ORGANIZATION">Microsoft</ENAMEX> <ENAMEX TYPE="PRODUCT">Excel 2000</ENAMEX> spreadsheet used to run the
          <ENAMEX TYPE="PERSON">Crystal Ball simulation</ENAMEX> is available as a additional
          fileto this <ENAMEX TYPE="ORG_DESC">paper</ENAMEX>. Running the simulations directly from
          the spreadsheet requires a copy of <ENAMEX TYPE="ORGANIZATION">Crystal Ball</ENAMEX> <TIMEX TYPE="DATE">2000</TIMEX> or a
          later compatible version.)
          Additional file 1
          
          .
          Click here for file
          What should we make of such a result? That depends on
          our goal for the estimate in the <NUMEX TYPE="ORDINAL">first</NUMEX> place. If the goal
          is to get an estimate into the scientific literature for
          others to use, it is probably a good idea to report the
          entire distribution, along with sensitivity analyses, and
          let others use them as they will. <ENAMEX TYPE="PER_DESC">Researchers</ENAMEX> interested
          in combining this with other estimates of the value in
          question might want to look at how likely the other
          estimates are according to this calculation and how
          likely this estimate is according to other calculations.
          A sophisticated policy <ENAMEX TYPE="ORG_DESC">maker</ENAMEX> trying to figure out how
          much attention to devote to this problem might want to
          look at the probability that the total was greater than
          some critical level, say <NUMEX TYPE="CARDINAL">ten million or one hundred</NUMEX>
          <ENAMEX TYPE="PERSON">million</ENAMEX>. Indeed, even a rough cut might be sufficient to
          answer important policy questions (e.g., "we are pretty
          sure that there are <NUMEX TYPE="CARDINAL">more than fifty million</NUMEX> cases per
          <TIMEX TYPE="DATE">year</TIMEX>, which means this is a bigger problem than most
          <ENAMEX TYPE="PER_DESC">people</ENAMEX> think").
        
        
          Sensitivity analysis
          This method for quantifying uncertainty lends itself
          easily to sensitivity analysis of the inputs. The
          quantification of uncertainty is itself is sometimes
          called a sensitivity analysis, but despite involving some
          of the same math, uncertainty quantification and
          sensitivity analysis are fundamentally different. A
          sensitivity analysis asks questions like, "if we are
          wrong about a certain input, how much does our best
          estimate change?" An uncertainty distribution does not
          answer that question. Rather, the uncertainty
          distribution 
          is that best estimate, the best
          estimate of the probabilities of possible true values
          given our knowledge. An uncertainty distribution does not
          report deviations from the result of an analysis; it 
          is the result of an analysis . A
          sensitivity analysis can be done to see how much that
          distribution changes if we change one of our inputs.
          For example, compare a <NUMEX TYPE="ORDINAL">second</NUMEX> estimate, identical
          except that the distribution of foodborne attribution for
          <ENAMEX TYPE="GPE">Norwalk</ENAMEX>-like viruses is [<NUMEX TYPE="PERCENT">40%,60%</NUMEX>] instead of the previous
          values of [<NUMEX TYPE="PERCENT">20%,60%</NUMEX>]. (This is chosen primarily to be
          <ENAMEX TYPE="ORGANIZATION">illustrative</ENAMEX> and emphasize the effect of changing the
          mean and variance of the range. It turns out, though,
          that <ENAMEX TYPE="ORGANIZATION">Mead et al.</ENAMEX> based their input of <NUMEX TYPE="PERCENT">40%</NUMEX> on a single
          rough estimate in the literature which was <NUMEX TYPE="PERCENT">47%</NUMEX>, so the
          new mean of <NUMEX TYPE="PERCENT">50%</NUMEX> is actually closer.) The result,
          represented in Figure <NUMEX TYPE="CARDINAL">2</NUMEX>, shows that the new mean value is
          <NUMEX TYPE="CARDINAL">about 85 million</NUMEX> and that half the probability mass is
          now in the narrower range of <NUMEX TYPE="MONEY">70 to 100 million</NUMEX>. The
          substantial difference between this and our previous
          distribution makes clear that one of the most useful
          things that could be done to improve our estimate of the
          total cases is to reduce our uncertainty about this key
          input. Furthermore, it calls to mind the question of
          whether even <ENAMEX TYPE="PER_DESC">experts</ENAMEX> who see the estimate of <NUMEX TYPE="CARDINAL">76 million</NUMEX>
          (which appears, usually presented as fact, in almost
          everything written about foodborne illness in the <ENAMEX TYPE="GPE">U.S.</ENAMEX>)
          have any idea that it hinges so significantly on this
          <NUMEX TYPE="CARDINAL">one</NUMEX>, rather arcane, guesstimate about how much of the
          illness caused by <NUMEX TYPE="CARDINAL">one</NUMEX> pathogen is attributable to
          foodborne transmission.
        
      
      
        Summary
        It is possible to quantify uncertainty in complex
        calculations in health research, as is commonly done for
        non-sample-based calculations in business or engineering.
        In addition to simply being a more accurate presentation of
        scientific knowledge, such quantification could
        dramatically increase the value of the underlying estimates
        in several ways. It would clarify whether the estimates are
        certain enough for the purposes for which they are used.
        Furthermore, it would suggest how likely further research
        is to produce a substantially different answer and would
        direct such research toward improving the particular inputs
        that create more of the uncertainty. The notion of
        reporting uncertainty sometimes provokes opposition, as if
        the revelation of uncertainty were responsible for creating
        the uncertainty. But quantification does not introduce
        uncertainty that did not previously exist, but rather,
        replaces ignorance about the magnitude of that uncertainty
        with the best available knowledge.
      
      
        Competing interests
        <ENAMEX TYPE="PERSON">Phillips</ENAMEX> and a few of his <ENAMEX TYPE="PER_DESC">students</ENAMEX> have received free
        short-term licences for the <ENAMEX TYPE="ORGANIZATION">Crystal Ball</ENAMEX> software (though
        this analysis was carried out using previously purchased
        copies). After this <ENAMEX TYPE="ORG_DESC">paper</ENAMEX> was written and reviewed, but
        before resubmission, <ENAMEX TYPE="PERSON">Phillips</ENAMEX> was briefly retained as an
        <ENAMEX TYPE="PER_DESC">expert witness</ENAMEX> by <ENAMEX TYPE="ORGANIZATION">The Delaco Company</ENAMEX> in litigation related
        to the phenylpropanolamine study that is briefly
        mentioned.
      
      
        <ENAMEX TYPE="PER_DESC">Authors</ENAMEX>' contributions
        <ENAMEX TYPE="PERSON">CVP</ENAMEX> conceived of the goal, developed the method of
        analysis, created the simple examples in the manuscript,
        and simplified the final example for the manuscript. LMP
        carried out the research and original analysis of the
        in-depth example (<ENAMEX TYPE="DISEASE">foodborne illness</ENAMEX>) and created the
        programming for the calculation. <ENAMEX TYPE="ORGANIZATION">CVP</ENAMEX> composed the
        <ENAMEX TYPE="ORGANIZATION">manuscript; LMP</ENAMEX> read and approved the final manuscript.
      
    
  
