
  
    
      
        Background
        Health care providers compete for managed care contracts
        based on cost-effectiveness and quality of care [ <NUMEX TYPE="CARDINAL">1 2 3 4</NUMEX> ]
        . Information technology (IT) provides a cost-effective way
        to document productivity, performance measures, cost, and
        quality of care. Since IT has dropped in cost over time,
        physician practices are now turning to it to meet these
        needs. Information technology for this study is defined as
        computer <ENAMEX TYPE="ORG_DESC">software</ENAMEX> used to store, transport, or communicate
        information [ <NUMEX TYPE="CARDINAL">2 5 6 7</NUMEX> ] .
        The health care <ENAMEX TYPE="ORG_DESC">organizations</ENAMEX> that succeed in the <NUMEX TYPE="CARDINAL">21</NUMEX>
        <ENAMEX TYPE="ORGANIZATION">stcentury</ENAMEX> will be those that improve quality and reduce
        cost. These juxtaposed objectives most likely will be
        reached through improved handling of information [ <NUMEX TYPE="CARDINAL">2 8 9</NUMEX> ]
        . The <ENAMEX TYPE="ORGANIZATION">Committee on Quality of Health Care</ENAMEX> in <ENAMEX TYPE="GPE">America</ENAMEX>
        reported that most clinical information remains in paper
        form [ <ENAMEX TYPE="LAW">9</ENAMEX> ] . This <ENAMEX TYPE="ORG_DESC">committee</ENAMEX> made several recommendations
        for improving quality, including moving clinical
        information to an electronic format by the end of the
        <TIMEX TYPE="DATE">decade</TIMEX>.
        Information technology selection in health care has
        often been performed in a rather informal way, resulting in
        the purchase of "white <ENAMEX TYPE="ANIMAL">elephants</ENAMEX>" [ <TIMEX TYPE="DATE">10</TIMEX> ] . The <ENAMEX TYPE="ORG_DESC">systems</ENAMEX> may
        not perform as planned and may cause additional work for
        medical <ENAMEX TYPE="PER_DESC">staff</ENAMEX>. The <ENAMEX TYPE="ORG_DESC">systems</ENAMEX> are often purchased or developed
        in <ENAMEX TYPE="PRODUCT_DESC">pieces</ENAMEX> without consideration to the overall business
        strategy [ <ENAMEX TYPE="LAW">1</ENAMEX> ] .
        To date, few <ENAMEX TYPE="ORG_DESC">publications</ENAMEX> have documented the selection
        process and the resulting impact of the IT on the health
        care <ENAMEX TYPE="ORG_DESC">organization</ENAMEX>. Most <ENAMEX TYPE="ORG_DESC">papers</ENAMEX> give anecdotal descriptions,
        often by <ENAMEX TYPE="ORG_DESC">vendors</ENAMEX>, but lack client perceptions of the
        information <ENAMEX TYPE="ORG_DESC">system</ENAMEX>'s value [ <NUMEX TYPE="CARDINAL">1 2 7 11 12 13 14</NUMEX> ] . Even at
        the <ENAMEX TYPE="ORG_DESC">hospital</ENAMEX> level, only a few <ENAMEX TYPE="PER_DESC">client</ENAMEX> perceptions of IT
        adoption have been reported [ <NUMEX TYPE="CARDINAL">15 16 17 18 19</NUMEX> ] . The number
        of available papers that examine IT selections within
        physician practices is even smaller than those papers
        addressing <ENAMEX TYPE="ORG_DESC">hospital</ENAMEX> selections [ <ENAMEX TYPE="LAW">3 20</ENAMEX> ] . However, many
        <ENAMEX TYPE="PER_DESC">physicians</ENAMEX> are transitioning from <ENAMEX TYPE="ORG_DESC">paper</ENAMEX> to electronic
        formats for billing records, medical charts, etc. This
        study aims to understand the process for selecting IT for
        <ENAMEX TYPE="PER_DESC">physicians</ENAMEX>' practices and the perceptions of the IT after
        it is implemented. 
        The primary objective of this research
        was to identify the relationship (if any) between the IT
        selection process and the office <ENAMEX TYPE="PER_DESC">staff</ENAMEX>'s perceptions of the
        it's impact on practice activities. 
      
      
        Methods
        
          Sample
          <ENAMEX TYPE="ORGANIZATION">Providence Health System</ENAMEX> in <ENAMEX TYPE="GPE">Portland</ENAMEX>, <ENAMEX TYPE="GPE">Oregon</ENAMEX> provided
          a database of practices (<ENAMEX TYPE="ORGANIZATION">n</ENAMEX> = <NUMEX TYPE="CARDINAL">933</NUMEX>) for this study. These
          practices all served <ENAMEX TYPE="GPE">Providence</ENAMEX> <ENAMEX TYPE="ORGANIZATION">Health System</ENAMEX> in some
          capacity - e.g., as primary care <ENAMEX TYPE="PER_DESC">physicians</ENAMEX> or
          <ENAMEX TYPE="PER_DESC">specialists</ENAMEX>. Eligible practices had acquired software
          within <TIMEX TYPE="DATE">the past five years</TIMEX> but not within <TIMEX TYPE="DATE">the past six</TIMEX>
          <TIMEX TYPE="DATE">months</TIMEX>. Practices with software older than <TIMEX TYPE="DATE">five years</TIMEX>
          were disqualified because it was unlikely that the
          decision <ENAMEX TYPE="PER_DESC">makers</ENAMEX> (if present) would recall the details of
          the selection process. Practices with software selected
          within <TIMEX TYPE="DATE">the last six months</TIMEX> were dropped because new
          software often requires a learning time period. The
          original sample of <NUMEX TYPE="CARDINAL">933</NUMEX> contained <NUMEX TYPE="CARDINAL">70</NUMEX> practices that had no
          computers and <NUMEX TYPE="CARDINAL">35</NUMEX> that had software purchased only in past
          <TIMEX TYPE="DATE">six months or more than five years ago</TIMEX>. In total, <NUMEX TYPE="PERCENT">11.1%</NUMEX>
          of the original sample were excluded.
          Of the remaining eligible practices (<ENAMEX TYPE="ORGANIZATION">n</ENAMEX> = <NUMEX TYPE="CARDINAL">828</NUMEX>), <NUMEX TYPE="CARDINAL">407</NUMEX>
          completed the telephone survey, representing a response
          rate of <NUMEX TYPE="PERCENT">49.2%</NUMEX>. If a qualified respondent at a practice
          was not reached after <NUMEX TYPE="CARDINAL">at least three</NUMEX> attempts (<ENAMEX TYPE="ORGANIZATION">n</ENAMEX> = <NUMEX TYPE="CARDINAL">269</NUMEX>)
          or the <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> declined the interview (<ENAMEX TYPE="ORGANIZATION">n</ENAMEX> = <NUMEX TYPE="CARDINAL">152</NUMEX>), the
          practice was counted as a nonrespondent. Qualified
          <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> were involved with software selection or
          software customization for the practice. <NUMEX TYPE="CARDINAL">Seven</NUMEX> practices
          gave partial interviews and were also counted as
          <ENAMEX TYPE="ORGANIZATION">nonrespondents</ENAMEX>. These <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> had to leave in the
          middle of the interview to address urgent <ENAMEX TYPE="ORG_DESC">clinic</ENAMEX> needs.
          Although these <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> were rescheduled, they were
          not reached to complete the interviews. Additionally, one
          <ENAMEX TYPE="PERSON">respondent</ENAMEX> gave many "don't know" responses. The
          <ENAMEX TYPE="PERSON">interviewer</ENAMEX> wrote in the comment section for this office
          that the respondent was not qualified for the study and
          should be dropped. Thus, in total, <NUMEX TYPE="CARDINAL">seven</NUMEX> partial
          interviews, and one unqualified interview were dropped
          from the sample, reducing the total number of <ENAMEX TYPE="ORG_DESC">offices</ENAMEX> in
          the study to <NUMEX TYPE="CARDINAL">399</NUMEX>. The <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> and participating
          practices are summarized in <ENAMEX TYPE="PRODUCT">Table 1</ENAMEX>.
          <NUMEX TYPE="ORDINAL">Second</NUMEX> interviews were gathered for <NUMEX TYPE="CARDINAL">189</NUMEX> of the <NUMEX TYPE="CARDINAL">407</NUMEX>
          responding practices. Since <NUMEX TYPE="CARDINAL">almost half</NUMEX> of the responding
          <ENAMEX TYPE="ORG_DESC">offices</ENAMEX> represented single <ENAMEX TYPE="PER_DESC">practitioners</ENAMEX>, many of these
          smaller <ENAMEX TYPE="ORG_DESC">offices</ENAMEX> had <NUMEX TYPE="CARDINAL">only one</NUMEX> eligible <ENAMEX TYPE="PER_DESC">participant</ENAMEX>.
        
        
          Telephone survey
          The survey questions were developed based on the
          literature review and discussions with an expert <ENAMEX TYPE="ORG_DESC">panel</ENAMEX>.
          Since many of the <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> were not familiar with
          technical IT terms, care was taken to present the survey
          in a "respondent friendly" format.
          Thirteen college <ENAMEX TYPE="PER_DESC">student</ENAMEX> interviewers and <NUMEX TYPE="CARDINAL">two</NUMEX>
          <ENAMEX TYPE="PER_DESC">supervisors</ENAMEX> conducted the interviews using a telephone
          interviewing software package, <ENAMEX TYPE="ORGANIZATION">Computer Assisted Survey</ENAMEX>
          <ENAMEX TYPE="ORGANIZATION">Execution System</ENAMEX>. A program was written to provide the
          interviewers with precise dialogue, questions, and
          <ENAMEX TYPE="ORGANIZATION">precoded</ENAMEX> responses. As the interview progressed, the
          <ENAMEX TYPE="PERSON">interviewer</ENAMEX> entered the responses into a personal
          computer.
          Since the study objective included capturing the
          perceived impacts of IT, we attempted to record
          perceptions from <NUMEX TYPE="CARDINAL">two</NUMEX> <ENAMEX TYPE="PER_DESC">representatives</ENAMEX> from each <ENAMEX TYPE="ORG_DESC">practice</ENAMEX>:
          the decision <ENAMEX TYPE="ORG_DESC">maker</ENAMEX> and a primary <ENAMEX TYPE="PER_DESC">user</ENAMEX> (see Additional
          File <NUMEX TYPE="CARDINAL">1</NUMEX>: "<ENAMEX TYPE="ORGANIZATION">Physician Practice Software Telephone Survey</ENAMEX>,
          <ENAMEX TYPE="ORGANIZATION">Dialog</ENAMEX> and Questions"). The initial interview that
          included questions related to the selection process and
          perceived impacts of the IT lasted <TIMEX TYPE="TIME">approximately 15-25</TIMEX>
          <TIMEX TYPE="TIME">minutes</TIMEX>. The respondent was asked to describe a recent IT
          purchase (<TIMEX TYPE="DATE">at least six months old</TIMEX>). For each practice,
          the respondent indicated whether a <ENAMEX TYPE="PER_DESC">person</ENAMEX> in a specific
          role - e.g., an <ENAMEX TYPE="PER_DESC">administrator</ENAMEX> - was involved or not
          involved in selection, and involved or not involved in
          software customization. <ENAMEX TYPE="ORGANIZATION">Customization</ENAMEX> in this study
          referred to providing input to the software <ENAMEX TYPE="ORG_DESC">vendor</ENAMEX> for
          writing software specific to the practice.
          During the interview we read the <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> a list of
          selection steps. For each step, the respondent answered
          "yes" or "no" as to whether it was performed. During the
          interview the <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> were read several potential
          factors that might have influenced the purchase. For each
          one they rated the statement on a <NUMEX TYPE="QUANTITY">1-to-6</NUMEX> scale of
          importance, (ranging from "no importance" to "very high
          importance"). Finally, we asked the <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> to react
          to <NUMEX TYPE="CARDINAL">12</NUMEX> statements describing potential impacts of the IT
          on selected practice activities. The statements were
          intentionally not grouped by any particular theme. The
          <ENAMEX TYPE="ORGANIZATION">respondents</ENAMEX> rated each impact statement on a <NUMEX TYPE="QUANTITY">1-to-5</NUMEX> scale
          of agreement ("strongly disagree", "slightly disagree",
          "neither agree or disagree", "slightly agree", "strongly
          agree") or selected "not applicable."
          The <NUMEX TYPE="ORDINAL">second</NUMEX> interview with a primary <ENAMEX TYPE="PER_DESC">user</ENAMEX> of the
          software included mainly the perceived impact questions,
          and lasted <TIMEX TYPE="TIME">7-10 minutes</TIMEX>. At the completion of the initial
          interview, each respondent was offered a summary of the
          results.
          Additional file 1
          <ENAMEX TYPE="CONTACT_INFO">Scripted</ENAMEX> telephone survey, "<ENAMEX TYPE="WORK_OF_ART">Physician Practice</ENAMEX>
          <ENAMEX TYPE="ORGANIZATION">Software Telephone Survey</ENAMEX>, <ENAMEX TYPE="WORK_OF_ART">Dialog and Questions"</ENAMEX>, by K.B.
          <ENAMEX TYPE="CONTACT_INFO">Eden,</ENAMEX>
          The file contains the script, questions and pre-coded
          responses, variables names (in left margins, that appear
          as: <ENAMEX TYPE="CONTACT_INFO">>xxxx<</ENAMEX>), and several logical statements (e.g.,
          goto, if, etc.) to lead the <ENAMEX TYPE="PER_DESC">interviewer</ENAMEX> through the
          interview.
          Click here for file
        
        
          Statistical evaluation
          The data from all interviews were first descriptively
          <ENAMEX TYPE="PERSON">evaluated</ENAMEX>, primarily by computing frequencies of
          responses for each question. Factor analysis (principal
          <ENAMEX TYPE="ORGANIZATION">components</ENAMEX>) revealed <NUMEX TYPE="CARDINAL">four</NUMEX> latent factors related to the
          <ENAMEX TYPE="PERSON">respondent</ENAMEX>'s perceived impacts of the IT on <NUMEX TYPE="CARDINAL">four</NUMEX> practice
          activities: scheduling, financial analysis,
          <ENAMEX TYPE="ORGANIZATION">communication</ENAMEX>, and medical documentation [ <ENAMEX TYPE="LAW">2</ENAMEX> ] .
          Therefore, <NUMEX TYPE="CARDINAL">four</NUMEX> <ENAMEX TYPE="PER_DESC">subscales</ENAMEX> were created. The scheduling,
          financial analysis, communication subscales each included
          <NUMEX TYPE="CARDINAL">two</NUMEX> items, and the medical documentation subscale
          included <NUMEX TYPE="CARDINAL">three</NUMEX> items. Responses of "not applicable" were
          <ENAMEX TYPE="PERSON">coded</ENAMEX> as missing. For each subscale the mean of the items
          was computed.
          Diagnostic plots of the <NUMEX TYPE="CARDINAL">four</NUMEX> practice activity
          subscales suggested that an explanatory model might be
          best approached using logistic regression, which relaxes
          the assumption of normality. The <NUMEX TYPE="CARDINAL">four</NUMEX> <ENAMEX TYPE="PER_DESC">subscales</ENAMEX> were
          <ENAMEX TYPE="ORGANIZATION">recoded</ENAMEX> to dichotomous variables corresponding to 
          agree or 
          not agree. If the mean score (of
          <NUMEX TYPE="CARDINAL">2</NUMEX>-3 impact statements) for a practice activity was
          greater than <NUMEX TYPE="CARDINAL">3.0</NUMEX>, the respondent was scored as "<ENAMEX TYPE="WORK_OF_ART">1</ENAMEX>" for 
          agree. If the mean score for a
          practice activity was <NUMEX TYPE="MONEY">3.0</NUMEX> ("neither agree or disagree")
          or less, the respondent was scored as "<ENAMEX TYPE="WORK_OF_ART">0</ENAMEX>" for 
          not agree. Each of the <NUMEX TYPE="CARDINAL">four</NUMEX>
          practice activity subscales became the dependent variable
          in a predictive model. The independent variables entered
          into the <ENAMEX TYPE="PER_DESC">models</ENAMEX> included the demographic and selection
          variables.
        
        
          Multiple logistic regression
          We attempted <NUMEX TYPE="CARDINAL">four</NUMEX> predictive <ENAMEX TYPE="PRODUCT_DESC">models</ENAMEX>, one for each of
          the newly created dichotomous subscales. Only <ENAMEX TYPE="PER_DESC">respondents</ENAMEX>
          who found the impact statements relevant were included in
          the predictive models. Multiple logistic regression
          revealed relationships between the selection process and
          the perceptions related to the scheduling, financial
          analysis, and communication processes. Variables that
          achieved a significance level of <ENAMEX TYPE="PRODUCT_DESC">p</ENAMEX> < <NUMEX TYPE="CARDINAL">.05</NUMEX> were retained
          in the <ENAMEX TYPE="PRODUCT_DESC">models</ENAMEX>. For the perceptions related to medical
          <ENAMEX TYPE="PERSON">documentation</ENAMEX>, no significant selection variables
          survived the analysis. This was most likely due to the
          small number of practices with electronic medical records
          (n = <NUMEX TYPE="CARDINAL">89</NUMEX>) and aggregating all types of electronic medical
          record (EMRs) regardless of type and number of functions.
          It is also possible that the decision to purchase an EMR
          is often made outside the practice - e.g., a large health
          system offers <ENAMEX TYPE="ORGANIZATION">EMRs</ENAMEX> to the practices. For <NUMEX TYPE="CARDINAL">11</NUMEX> of the <NUMEX TYPE="CARDINAL">89</NUMEX>
          practices that had <ENAMEX TYPE="ORGANIZATION">EMRs</ENAMEX>, the decision was made by a large
          health system. <ENAMEX TYPE="ORGANIZATION">Data</ENAMEX> from these practices were not
          included in the predictive <ENAMEX TYPE="PRODUCT_DESC">models</ENAMEX>, thus reducing the
          number of available practices with <ENAMEX TYPE="ORGANIZATION">EMRs</ENAMEX> to <NUMEX TYPE="CARDINAL">78</NUMEX>.
          A summary of the <ENAMEX TYPE="PRODUCT_DESC">models</ENAMEX> is presented in this <ENAMEX TYPE="ORG_DESC">paper</ENAMEX>.
          The complete analysis and <ENAMEX TYPE="PRODUCT_DESC">models</ENAMEX> are available elsewhere
          [ <ENAMEX TYPE="LAW">2</ENAMEX> ] . The predictive <ENAMEX TYPE="PRODUCT_DESC">models</ENAMEX> were built using a model
          building data set (<NUMEX TYPE="CARDINAL">299</NUMEX> randomly selected interviews). The
          models were then tested with a testing data set (the
          remaining <NUMEX TYPE="CARDINAL">100</NUMEX> interviews). <NUMEX TYPE="CARDINAL">One</NUMEX>-<NUMEX TYPE="CARDINAL">hundred</NUMEX> interviews were
          needed to insure adequate statistical power. As a check
          for cross-validation, the accuracy with which the models
          predicted the perceived impact subscale values using the
          model building data set was compared to the accuracy
          achieved with the testing data set. Using the parameters
          established with the model building data set, agreement
          (or not agreement) to a perceived impact subscale was
          predicted for the testing data set.
          For cross-validation, the accuracy levels were
          compared using a z-test for proportions. As seen in Table
          <NUMEX TYPE="CARDINAL">2</NUMEX>, the scheduling and financial analysis <ENAMEX TYPE="PER_DESC">models</ENAMEX> had
          non-significant (p <NUMEX TYPE="MONEY">> .05</NUMEX>) drops in accuracy. This
          suggests that the <ENAMEX TYPE="PER_DESC">models</ENAMEX> may be generalized to other
          physician <ENAMEX TYPE="ORG_DESC">offices</ENAMEX> with similar demographics. Since the
          accuracy level dropped dramatically for the communication
          model, this model did not "cross-validate." The
          observations made in this study accurately describe the
          idiosyncrasies of this sample used to build the
          <ENAMEX TYPE="ORGANIZATION">communication</ENAMEX> model, but may not accurately describe
          other samples of physician <ENAMEX TYPE="ORG_DESC">offices</ENAMEX>.
          Once the results were completed, the expert <ENAMEX TYPE="ORG_DESC">panel</ENAMEX> was
          <ENAMEX TYPE="ORGANIZATION">reconvened</ENAMEX> to provide insight in interpreting the
          results. In the <ENAMEX TYPE="FAC_DESC">sections</ENAMEX> that follow, the descriptive
          results, comparison of the decision <ENAMEX TYPE="ORG_DESC">maker</ENAMEX> vs <ENAMEX TYPE="PER_DESC">user</ENAMEX>, and
          each cross-validated <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX> are summarized and
          discussed.
        
      
      
        Results and Discussion
        
          Comparison of decision-maker vs user
          The primary <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> agreed with <ENAMEX TYPE="PER_DESC">users</ENAMEX> on their
          perceptions of the software's impact on scheduling and
          financial analysis activities (p <NUMEX TYPE="MONEY">< .001</NUMEX>). For the
          scheduling model, <ENAMEX TYPE="ORGANIZATION">Phi</ENAMEX> was <NUMEX TYPE="MONEY">.359</NUMEX>, with a maximal <ENAMEX TYPE="ORGANIZATION">Phi of</ENAMEX>
          <NUMEX TYPE="CARDINAL">.778</NUMEX>. For the financial analysis model, <ENAMEX TYPE="ORGANIZATION">Phi</ENAMEX> was <NUMEX TYPE="CARDINAL">.418</NUMEX> with
          a maximal <ENAMEX TYPE="ORGANIZATION">Phi</ENAMEX> of <NUMEX TYPE="CARDINAL">.920</NUMEX>. Since the primary respondent was
          reasonably knowledgeable about the perceived impacts of
          the software, we did not include the <ENAMEX TYPE="PER_DESC">user</ENAMEX> data in the
          remainder of the cross-validated <ENAMEX TYPE="PRODUCT_DESC">models</ENAMEX>. The user
          provided only a few demographics and the perceived impact
          <ENAMEX TYPE="ORGANIZATION">data</ENAMEX>, while the primary respondent provided the selection
          <ENAMEX TYPE="ORGANIZATION">data</ENAMEX> as well as the perceived impact data.
        
        
          Predicting the impact of the software on scheduling
          activities
          For the scheduling model, <NUMEX TYPE="CARDINAL">five</NUMEX> selection variables as
          a <ENAMEX TYPE="PER_DESC">group</ENAMEX> predicted with <NUMEX TYPE="PERCENT">73%</NUMEX> accuracy the subscale of
          whether the <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> on average would agree with the
          following two impact statements:
          "The <ENAMEX TYPE="ORG_DESC">software</ENAMEX> has improved the scheduling of patients
          for routine, preventive and urgent appointments."
          "The <ENAMEX TYPE="ORG_DESC">software</ENAMEX> has improved the referral process in
          sending and receiving referrals quickly."
          The statistically significant (p <NUMEX TYPE="MONEY">< .05</NUMEX>) predictors
          are presented in <ENAMEX TYPE="PRODUCT">Table 6along</ENAMEX> with the expected response
          by the respondent and the results of the multiple
          logistic regression analysis. The <NUMEX TYPE="ORDINAL">second</NUMEX> column of the
          table contains the coefficient (or weighting value of B).
          The <ENAMEX TYPE="PERSON">Wald</ENAMEX> statistic (Bj/standard error) gives a measure of
          significance of B for the predictor variable.
          Looking at the odds ratios in <ENAMEX TYPE="PRODUCT">Table 6</ENAMEX>, the likelihood
          of agreement with the scheduling subscale is almost <NUMEX TYPE="CARDINAL">four</NUMEX>
          times (odds ratio, OR = <NUMEX TYPE="CARDINAL">3.89</NUMEX>) as great when practices
          selected EMR packages than if they did not select EMR
          packages. At first this finding was surprising. Many
          <ENAMEX TYPE="ORGANIZATION">EMRs</ENAMEX>, however, have automatic recall features when the
          <ENAMEX TYPE="PER_DESC">patient</ENAMEX> should be called or sent a reminder for a health
          check. Similarly, the likelihood of agreement was almost
          <NUMEX TYPE="CARDINAL">four</NUMEX> times (OR = <NUMEX TYPE="CARDINAL">3.88</NUMEX>) as great when the practice
          compared the software options with the best in the field
          than if it did not perform this step.
          The practices that selected practice management
          software were <NUMEX TYPE="CARDINAL">1.70</NUMEX> times more likely to agree that the
          software had improved the scheduling and referring of
          <ENAMEX TYPE="PER_DESC">patients</ENAMEX> than practices that selected other types of
          software. This finding was expected since these packages
          typically include a scheduling module. Additionally,
          practices that considered "prior <ENAMEX TYPE="PER_DESC">user</ENAMEX> testimony"
          important in the selection process were <NUMEX TYPE="CARDINAL">1.39</NUMEX> times more
          likely to agree with the scheduling subscale than those
          practices that did not consider prior <ENAMEX TYPE="PER_DESC">user</ENAMEX> testimony as
          an important influence.
          Finally, a respondent who had personally selected the
          software was less likely to agree with the impact
          statements (OR = <NUMEX TYPE="CARDINAL">0.20</NUMEX>). The <ENAMEX TYPE="PER_DESC">members</ENAMEX> of the expert panel
          felt this was a symptom of "unmet expectations." The
          <ENAMEX TYPE="PER_DESC">members</ENAMEX> of the selection <ENAMEX TYPE="PER_DESC">team</ENAMEX> knew how the software was
          supposed to perform and were likely disappointed when it
          didn't live up to the <ENAMEX TYPE="PER_DESC">vendor</ENAMEX> promises. These <ENAMEX TYPE="PER_DESC">respondents</ENAMEX>
          had also probably seen the "<ENAMEX TYPE="ORGANIZATION">Cadillac</ENAMEX>" <ENAMEX TYPE="PER_DESC">performers</ENAMEX> and
          realized that their software had only achieved
          "<ENAMEX TYPE="ORGANIZATION">Chevrolet</ENAMEX>" status. Another explanation is that these
          practices failed to fully implement the software or to
          adapt <ENAMEX TYPE="ORG_DESC">clinic</ENAMEX> workflows to fully utilize the software.
          In summary, practices that selected <ENAMEX TYPE="ORGANIZATION">EMR</ENAMEX> or practice
          management software, that made software comparisons, or
          that considered prior <ENAMEX TYPE="PER_DESC">user</ENAMEX> testimony as important were
          more likely to have perceived improvements in the
          scheduling process than were other practices.
        
        
          Predicting the impact of the software on financial
          analysis activities
          For the financial analysis model, <NUMEX TYPE="CARDINAL">five</NUMEX> selection
          variables as a <ENAMEX TYPE="PER_DESC">group</ENAMEX> predicted with <NUMEX TYPE="PERCENT">86%</NUMEX> accuracy the
          subscale of whether the <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> on average would
          agree with the following two impact statements:
          "The <ENAMEX TYPE="ORG_DESC">software</ENAMEX> has created a more accurate and timely
          billing process."
          "The <ENAMEX TYPE="ORG_DESC">software</ENAMEX> has improved the <ENAMEX TYPE="ORG_DESC">practice</ENAMEX>'s ability to
          track and analyze costs and revenues associated with
          managed care contracts."
          The most dramatic increase in odds of agreement (OR =
          <NUMEX TYPE="MONEY">8.2</NUMEX>) occurred when the practice reduced the workload to
          allow time to learn the software, <ENAMEX TYPE="PRODUCT">Table 7</ENAMEX>. However, only
          <NUMEX TYPE="PERCENT">36%</NUMEX> of the <NUMEX TYPE="CARDINAL">399</NUMEX> practices reported that reduced workloads
          were provided during the implementation phase. According
          to the survey conducted by <ENAMEX TYPE="ORGANIZATION">Ambosa et al.</ENAMEX> [ <TIMEX TYPE="DATE">21</TIMEX> ] ,
          expecting medical <ENAMEX TYPE="PER_DESC">staff</ENAMEX> to learn new software while
          caring for a full load of <ENAMEX TYPE="PER_DESC">patients</ENAMEX> is a common reason for
          failure.
          The odds of agreement were increased by more than a
          factor of <NUMEX TYPE="CARDINAL">four</NUMEX> (OR = <NUMEX TYPE="CARDINAL">4.59</NUMEX>) for each increase in managed
          care activities the software contained. Since most
          managed care software packages are marketed to assist the
          practice in documenting costs associated with managed
          care contracts, this finding was expected.
          Practices that considered value an important
          consideration were twice (OR = <NUMEX TYPE="CARDINAL">2.0</NUMEX>) as likely to agree
          with the financial analysis subscale. By contrast,
          practices that considered compatibility an important
          influence were less likely (OR = <NUMEX TYPE="CARDINAL">0.66</NUMEX>) to agree with
          financial analysis subscale. At first the compatibility
          result was surprising. However, <NUMEX TYPE="PERCENT">51%</NUMEX> of these practices
          were first-time <ENAMEX TYPE="PER_DESC">buyers</ENAMEX>, and usually buying billing
          software, so compatibility was not a critical
          consideration. <ENAMEX TYPE="PERSON">Ninety</ENAMEX>-<NUMEX TYPE="CARDINAL">one</NUMEX> percent of first-time buyers
          who rated compatibility as low-to-no importance agreed
          with the financial analysis subscale. It is also possible
          that practices with existing good financial analysis
          processes (and little room to improve) rated
          compatibility as important but disagreed that the new
          software had improved the existing good process.
          The finding that less expensive packages related to
          more satisfied <ENAMEX TYPE="PER_DESC">buyers</ENAMEX> was interesting (OR = <NUMEX TYPE="CARDINAL">0.25</NUMEX>). There
          were many good financial packages available for less than
          <NUMEX TYPE="MONEY">$10,000</NUMEX> in <TIMEX TYPE="DATE">1996</TIMEX>. Practices that spent <NUMEX TYPE="MONEY">less than $10,000</NUMEX>
          bought software packages with few, but very functional,
          features. Those practices that spent <NUMEX TYPE="MONEY">more than $10,000</NUMEX>
          were purchasing complex systems, perhaps for multiple
          sites. Financial analysis may just have been a small
          module of these multi-purpose packages.
          In summary, practices that considered value important,
          that did not consider compatibility important, that
          selected managed care software, that spent <TIMEX TYPE="TIME">less than</TIMEX>
          <NUMEX TYPE="MONEY">$10,000</NUMEX>, or that provided learning time during
          implementation were more likely to perceive that the
          software had improved the financial analysis process than
          were other practices.
        
        
          Observations from both models
          In looking over the predictors for the two
          <ENAMEX TYPE="ORGANIZATION">cross-validated</ENAMEX> <ENAMEX TYPE="PRODUCT_DESC">models</ENAMEX> (scheduling and financial
          analysis), some predictors naturally belong in <NUMEX TYPE="CARDINAL">one</NUMEX> model
          or the other - e.g., practice management software in the
          <ENAMEX TYPE="PRODUCT">scheduling</ENAMEX> <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX> and managed care software in the
          financial analysis model. The themes in the scheduling
          model center around software features (emr and practice
          management software, comparison of software options) and
          <ENAMEX TYPE="PERSON">usability</ENAMEX> (prior <ENAMEX TYPE="PER_DESC">user</ENAMEX> testimony and personal selection by
          <ENAMEX TYPE="ORGANIZATION">respondent</ENAMEX>). The themes in the financial analysis model
          include cost (software cost, value), software features
          (managed care software and compatibility), and learning
          <ENAMEX TYPE="ORGANIZATION">time</ENAMEX>. This might suggest that the <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> for the
          financial analysis model had differing roles in the
          practice than the <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> for the scheduling model.
          In both of these <ENAMEX TYPE="PRODUCT_DESC">models</ENAMEX>, <NUMEX TYPE="PERCENT">79%</NUMEX> of the <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> were
          <ENAMEX TYPE="ORGANIZATION">administrators</ENAMEX>.
          Since all types of <ENAMEX TYPE="PER_DESC">administrators</ENAMEX> (e.g., office
          <ENAMEX TYPE="PER_DESC">managers</ENAMEX>, finance <ENAMEX TYPE="PER_DESC">managers</ENAMEX>) were grouped together, it was
          impossible to identify the primary role of <ENAMEX TYPE="PER_DESC">administrator</ENAMEX>
          who responded. The differences in the models also suggest
          that the predictors of success differ by the types of
          activities the software is intended to perform.
          It might appear odd that some predictors (e.g.,
          learning time) did not carry through to both <ENAMEX TYPE="PRODUCT_DESC">models</ENAMEX>. It
          is likely that the type and complexity of software
          package contributed to the learning demands on the
          <ENAMEX TYPE="ORG_DESC">office</ENAMEX>. Many of the <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> who agreed with the
          financial analysis <ENAMEX TYPE="ORG_DESC">subscale</ENAMEX> chose managed care software
          that bundled together many activities (tracking incoming
          and outgoing referrals, <ENAMEX TYPE="PER_DESC">patient</ENAMEX> enrollment, capitation
          accounting, and/or utilization reporting). For practices
          learning this type of software, protected learning time
          was an important predictor of success. For practices
          implementing practice management software (scheduling,
          billing, and/or accounting spreadsheets), the learning
          demand was less. This naturally suggests that the
          decision to reduce the workload while learning a software
          package should consider the number and complexity of the
          tasks to be learned.
        
        
          Limitations and research opportunities
          The <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> for this study primarily represented
          practices that serve <ENAMEX TYPE="GPE">Providence</ENAMEX> <ENAMEX TYPE="ORGANIZATION">Health System</ENAMEX> in <ENAMEX TYPE="GPE">Oregon</ENAMEX>.
          These practices served either as managed care providers
          or as fee-for-service <ENAMEX TYPE="ORG_DESC">providers</ENAMEX>. The only practices
          excluded were pure <ENAMEX TYPE="SUBSTANCE">HMO</ENAMEX> <ENAMEX TYPE="ORG_DESC">providers</ENAMEX> - e.g., <ENAMEX TYPE="ORGANIZATION">Kaiser</ENAMEX>
          <ENAMEX TYPE="ORGANIZATION">Permanente</ENAMEX>. The pure <ENAMEX TYPE="ORGANIZATION">HMO</ENAMEX> practices were excluded because
          it was unclear whom to interview regarding software
          selections. Often these practices are given software
          directly from the <ENAMEX TYPE="ORG_DESC">organization</ENAMEX>. <NUMEX TYPE="CARDINAL">Eighty</NUMEX>-<NUMEX TYPE="CARDINAL">seven</NUMEX> percent of
          these practices in this study had <NUMEX TYPE="CARDINAL">10</NUMEX> <ENAMEX TYPE="PER_DESC">practitioners</ENAMEX> or
          less. <NUMEX TYPE="PERCENT">Only 17%</NUMEX> of these practices had in-house computer
          <ENAMEX TYPE="PER_DESC">specialists</ENAMEX> assisting with software selection. The
          results of this study may not generalize to large
          practices that often have in-house computer specialists
          assisting with selection. A future study could include a
          nationwide survey of all types of <ENAMEX TYPE="PER_DESC">physician</ENAMEX> practices,
          regardless of managed care status, ownership, specialty,
          or size.
          This study is retrospective in nature, requiring the
          <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> to recall a software purchase that occurred
          <TIMEX TYPE="DATE">several months</TIMEX>, perhaps <TIMEX TYPE="DATE">more than a year</TIMEX>, <TIMEX TYPE="DATE">earlier</TIMEX>. In an
          "ideal study design," a questionnaire should be
          distributed to practices that have recently made
          selections. Another questionnaire addressing the impact
          on the practice could be sent at a pre-defined follow-up
          period - e.g., <TIMEX TYPE="DATE">six months</TIMEX> after implementation. This
          "ideal study design" would be difficult to conduct
          without a sufficient list of practices that have recently
          purchased software. Perhaps software <ENAMEX TYPE="ORG_DESC">manufacturers</ENAMEX> and
          <ENAMEX TYPE="PERSON">vendors</ENAMEX> could provide lists of recent <ENAMEX TYPE="PER_DESC">clients</ENAMEX> (with
          permission) to interested <ENAMEX TYPE="PER_DESC">researchers</ENAMEX>.
          The cross-sectional survey design of this study
          captured the technical aspects of the selection process
          (e.g., who was involved, what steps that were taken).
          Although the <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> were given a few "open-ended"
          questions, most provided little additional information.
          There could have been additional selection steps,
          <ENAMEX TYPE="PERSON">influences</ENAMEX>, and impacts. It is also possible that the
          observed changes in impact were related to variables we
          didn't attempt to measure - e.g., ability and desire of
          <ENAMEX TYPE="PER_DESC">management</ENAMEX> to implement new technologies and to change
          existing practice activities. Focus <ENAMEX TYPE="ORG_DESC">groups</ENAMEX> might be more
          effective at capturing underlying management expertise.
          Another very time-invasive approach would be to conduct a
          series of case studies, documenting the decision-making
          process over time. This research would need support from
          practices for <ENAMEX TYPE="PER_DESC">observers</ENAMEX> to remain on-site during the
          selection process. This format would also promote a more
          well-rounded, multiple perspectives evaluation. The
          current study relies on perceptive responses (primarily
          from office <ENAMEX TYPE="PER_DESC">managers</ENAMEX>) to measure many variables,
          including impact variables. Their perceptions were
          related to business-related practice activities. Only
          <NUMEX TYPE="PERCENT">5.3%</NUMEX> of the <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> were clinicians. It is likely
          that expanding this study to include more clinician
          responses would reveal perceptions related to other
          processes - e.g., medical documentation or treatment
          processes.
          The subscales (related to practice activities) were
          formed from responses to <NUMEX TYPE="CARDINAL">only two to three</NUMEX> original
          impact questions. A stronger design of these practice
          activities impacts would include several questions
          related to each activity. Given the exploratory nature of
          this current research, this limitation could not have
          been foreseen. However, the results of this study open
          doors for more confirmatory types of studies to design
          survey instruments that measure software impact with
          underlying practice activity constructs. This study does
          not attempt to demonstrate cause and effect. It would be
          important to have <ENAMEX TYPE="PER_DESC">respondents</ENAMEX> rate existing practice
          activities (before purchasing software) to control for a
          "ceiling effect" - practices with existing good processes
          have little room to improve. If such a trial were
          designed, it would also need to control for the type of
          IT and the needs of the <ENAMEX TYPE="ORG_DESC">buyer</ENAMEX>.
          To move toward a more direct measure of impact would
          require the practices to closely measure performance and
          behavior. For example, in this study, the respondent is
          asked if the <ENAMEX TYPE="PER_DESC">practitioners</ENAMEX> have an improved ability to
          consult professional literature online. A direct
          measurement method would determine the number of online
          literature consultations before and after the software
          installation.
        
      
      
        Conclusions
        The results of this research describe the software
        selection process as it occurs in physician practices.
        Using a telephone interview survey gave the <ENAMEX TYPE="PER_DESC">researcher</ENAMEX> (and
        other interviewers) direct contact with the decision makers
        in each practice. The results of this study also describe
        how software is perceived to affect several practice
        activities.
        The objective of this study was to identify
        relationships (if any) between the IT selection process and
        the office <ENAMEX TYPE="PER_DESC">staff</ENAMEX>'s perceptions of the IT's impact on
        practice activities. The results of the multiple logistic
        regression <ENAMEX TYPE="PER_DESC">models</ENAMEX> confirmed relationships between the
        selection process and the perceived impacts related to the
        scheduling and financial analysis activities. The results
        of this study demonstrated a relationship (not cause and
        effect) between the selection process and the user
        perception of software usefulness.
        Although many of the relationships were expected (e.g.,
        performing software comparisons, interviewing prior <ENAMEX TYPE="PER_DESC">users</ENAMEX>,
        and selecting certain software features improved
        perceptions about practice activities), perhaps one of the
        most important predictors of improvement was reducing the
        <ENAMEX TYPE="ORGANIZATION">workload</ENAMEX> during implementation. Despite the importance of
        this predictor, <NUMEX TYPE="PERCENT">only 36%</NUMEX> of the practices performed this
        step in this study. If more practices had performed this
        step, it might have carried even more weight in the
        analysis. From a practical standpoint, many of the <ENAMEX TYPE="ORG_DESC">offices</ENAMEX>
        selected and implemented IT but expected the <ENAMEX TYPE="PER_DESC">staff</ENAMEX> to learn
        the software while caring for a full load of <ENAMEX TYPE="PER_DESC">patients</ENAMEX>.
        <ENAMEX TYPE="PER_DESC">Investigators</ENAMEX> from a previous study by <ENAMEX TYPE="ORGANIZATION">Ambroso et al.</ENAMEX> [ <NUMEX TYPE="CARDINAL">21</NUMEX>
        ] cite this expectation as a common reason for IT
        failure.
        <NUMEX TYPE="CARDINAL">One</NUMEX> of the secondary findings of this research is that
        the <ENAMEX TYPE="PER_DESC">purchasers</ENAMEX> of the software (often office <ENAMEX TYPE="PER_DESC">managers</ENAMEX>) had
        perceptions about the software's use similar to those of
        <ENAMEX TYPE="PER_DESC">users</ENAMEX> (who were not involved in the selection process).
        This finding supports the use of a single-survey-response
        study design for understanding perceived impacts related to
        <ENAMEX TYPE="ORGANIZATION">software</ENAMEX>'s impacts on business-related practice
        activities.
      
      
        List of Abbreviations
        EMR: <ENAMEX TYPE="ORGANIZATION">Electronic Medical Record</ENAMEX>
        IT: <ENAMEX TYPE="ORGANIZATION">Information Technology</ENAMEX>
        <ENAMEX TYPE="WORK_OF_ART">OR: Odds Ratio</ENAMEX>
      
      
        Competing Interests
        None Declared.
      
      
        Author comments on prior presentation of
        results
        The results of this study were presented at the <ENAMEX TYPE="GPE">Portland</ENAMEX>
        <ENAMEX TYPE="ORGANIZATION">International Conference on Management of Engineering</ENAMEX> and
        Technology, <ENAMEX TYPE="GPE">Portland Oregon</ENAMEX>, <TIMEX TYPE="DATE">1997 and 1999</TIMEX>. The results
        were also presented at the <ENAMEX TYPE="ORGANIZATION">Institute for Operations</ENAMEX>
        Research and <ENAMEX TYPE="ORGANIZATION">Management Science</ENAMEX>, <ENAMEX TYPE="GPE">Philadelphia</ENAMEX>,
        <ENAMEX TYPE="ORGANIZATION">Pennsylvania</ENAMEX>, <TIMEX TYPE="DATE">1999</TIMEX>. The references for the conference
        proceedings are listed below.
        
        <ENAMEX TYPE="ORGANIZATION">Eden K, Kocoaglu, D. Information</ENAMEX>
        Technology <ENAMEX TYPE="WORK_OF_ART">Selection Process and Perceived Impacts in</ENAMEX>
        <ENAMEX TYPE="ORGANIZATION">Physician Practices</ENAMEX>. In 
        Technology and Innovation
        <ENAMEX TYPE="PER_DESC">Management</ENAMEX>. <ENAMEX TYPE="ORGANIZATION">Portland State University</ENAMEX>, PICMET
        conference proceedings, <TIMEX TYPE="DATE">1999</TIMEX>, pp. <ENAMEX TYPE="CONTACT_INFO">562-568</ENAMEX>. Executive
        <ENAMEX TYPE="PERSON">summary</ENAMEX> presented in proceedings, <ENAMEX TYPE="GPE">Portland</ENAMEX> International
        Conference on <ENAMEX TYPE="ORGANIZATION">Management of Engineering and Technology</ENAMEX>,
        <ENAMEX TYPE="GPE">Portland</ENAMEX>, <ENAMEX TYPE="GPE">Oregon</ENAMEX>, <TIMEX TYPE="DATE">1999</TIMEX>, pp. <ENAMEX TYPE="CONTACT_INFO">392-394</ENAMEX>.
        
        <ENAMEX TYPE="ORGANIZATION">Eden K, Kocaoglu</ENAMEX>, <ENAMEX TYPE="PERSON">D. Selection</ENAMEX> of
        <ENAMEX TYPE="ORGANIZATION">Information Technology</ENAMEX> in the <ENAMEX TYPE="ORGANIZATION">Health Care Industry</ENAMEX>.
        Presented at <ENAMEX TYPE="ORGANIZATION">Institute for Operations Research</ENAMEX> and the
        <ENAMEX TYPE="ORGANIZATION">Management Sciences</ENAMEX> conference. <ENAMEX TYPE="GPE">Philadelphia</ENAMEX>, <ENAMEX TYPE="GPE">Pennsylvania</ENAMEX>,
        <TIMEX TYPE="DATE">November, 1999</TIMEX>.
        
        <ENAMEX TYPE="ORGANIZATION">Eden K</ENAMEX>, <ENAMEX TYPE="PERSON">Kocaoglu D. Selection</ENAMEX> and
        <ENAMEX TYPE="ORGANIZATION">Implementation of Information Technology</ENAMEX> in the <ENAMEX TYPE="ORGANIZATION">Health Care</ENAMEX>
        Industry. Preliminary results presented at the <ENAMEX TYPE="GPE">Portland</ENAMEX>
        <ENAMEX TYPE="ORGANIZATION">International Conference on Management of Engineering</ENAMEX> and
        Technology, published in proceedings, <ENAMEX TYPE="GPE">Portland</ENAMEX>, <ENAMEX TYPE="GPE">Oregon</ENAMEX>,
        <TIMEX TYPE="DATE">1997</TIMEX>, pp. <ENAMEX TYPE="CONTACT_INFO">199-202</ENAMEX>.
      
    
  
