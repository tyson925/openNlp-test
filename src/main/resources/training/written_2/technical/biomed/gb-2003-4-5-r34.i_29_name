
  
    
      
        Background
        
          Introduction to cluster analysis
          A dataset containing objects to be clustered is
          usually represented in one of <NUMEX TYPE="CARDINAL">two</NUMEX> formats: the data
          <ENAMEX TYPE="ORGANIZATION">matrix</ENAMEX> and the similarity (or distance) matrix. In a data
          <ENAMEX TYPE="ORGANIZATION">matrix</ENAMEX>, rows usually represent objects to be clustered
          (typically genes), and columns usually represent features
          or attributes of the objects (typically experiments). An
          entry in the data matrix usually represents the
          expression level or expression ratio of a gene under a
          given experiment. The similarity (or distance) matrix
          contains the pairwise similarities (or dissimilarities)
          between each pair of <ENAMEX TYPE="PER_DESC">objects</ENAMEX> (<ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> or experiments).
          There are many similarity measures that can be used to
          compute the similarity or dissimilarity between a pair of
          <ENAMEX TYPE="PERSON">objects</ENAMEX>, among which the <NUMEX TYPE="CARDINAL">two</NUMEX> most popular ones for gene
          expression data are correlation coefficient and Euclidean
          distance. <ENAMEX TYPE="PERSON">Correlation</ENAMEX> is a similarity measure, that is, a
          high correlation coefficient implies high similarity, and
          it captures the directions of change of <NUMEX TYPE="CARDINAL">two</NUMEX> expression
          profiles. Euclidean distance is a dissimilarity measure,
          that is, a high distance implies low similarity, and it
          measures both the magnitudes and directions of change
          between <NUMEX TYPE="CARDINAL">two</NUMEX> expression profiles.
          Most clustering algorithms take the similarity matrix
          as input and create as output an <ENAMEX TYPE="ORG_DESC">organization</ENAMEX> of the
          objects grouped by similarity to each other. The most
          common algorithms are hierarchical in nature.
          Hierarchical algorithms define a dendrogram (<ENAMEX TYPE="PLANT">tree</ENAMEX>)
          relating similar objects in the same subtrees. In
          agglomerative hierarchical algorithms (such as average
          <ENAMEX TYPE="PERSON">linkage</ENAMEX> and complete linkage), each object is initially
          assigned to its own subtree (cluster). In each step,
          similar subtrees (clusters) are merged to form the
          <ENAMEX TYPE="ORGANIZATION">dendrogram</ENAMEX>. Cluster similarity can be computed from the
          similarity matrix or the data matrix (see <ENAMEX TYPE="ORGANIZATION">Sherlock</ENAMEX> [ <TIMEX TYPE="DATE">15</TIMEX> ]
          or Sharan 
          <ENAMEX TYPE="ORGANIZATION">et al</ENAMEX> . [ <TIMEX TYPE="DATE">16</TIMEX> ] for reviews of
          popular clustering algorithms for gene-expression
          <ENAMEX TYPE="ORGANIZATION">data</ENAMEX>).
          Once a clustering algorithm has grouped similar
          <ENAMEX TYPE="PERSON">objects</ENAMEX> (<ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> and <ENAMEX TYPE="SUBSTANCE">samples</ENAMEX>) together, the <ENAMEX TYPE="PER_DESC">biologist</ENAMEX> is
          then faced with the task of interpreting these groupings
          (or clusters). For example, if a gene of unknown function
          is clustered together with many genes of similar, known
          function, one might hypothesize that the unknown gene
          also has a related function. Or, if biological sample 'A'
          is grouped with other <ENAMEX TYPE="SUBSTANCE">samples</ENAMEX> that have similar <ENAMEX TYPE="GPE_DESC">states</ENAMEX> or
          diagnoses, one might infer the state or diagnosis of
          sample 'A'. However, before one does subsequent
          laboratory work to confirm a hypothesis or, more
          important, makes a diagnosis based on the results of
          cluster analysis, a few questions need to be asked. The
          <NUMEX TYPE="ORDINAL">first</NUMEX> is how reproducible are the clustering results with
          respect to re-measurement of the data. Then, what is the
          likelihood that the grouping of the unknown sample or
          <ENAMEX TYPE="PERSON">gene</ENAMEX> of interest with other known samples or genes is
          false (due to noise in the data, inherent limitations of
          the data or limitations in the algorithm)? And finally,
          is there a better algorithm that will reduce errors in
          clustering results?
        
        
          Related work
          <ENAMEX TYPE="PERSON">Kerr</ENAMEX> and <ENAMEX TYPE="PERSON">Churchill</ENAMEX> [ <TIMEX TYPE="DATE">17</TIMEX> ] applied an analysis of
          <ENAMEX TYPE="ORGANIZATION">variance</ENAMEX> <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX> and bootstrapping to array data to assess
          stability of clusters (for example, 'if one re-measured
          the data and did the same analysis again, would the same
          genes/samples <ENAMEX TYPE="ORG_DESC">group</ENAMEX> together?'). In their approach, the
          original data was re-sampled using variability estimates
          and cluster analysis was performed using the re-sampled
          <ENAMEX TYPE="ORGANIZATION">data</ENAMEX>. This post-hoc analysis uses variability estimates
          to provide a good indication of cluster stability.
          However, this method does not improve the overall
          clustering results, it only provides an indication of the
          reproducibility of the clusters with a given dataset and
          <ENAMEX TYPE="ORGANIZATION">algorithm</ENAMEX>.
          <ENAMEX TYPE="ORGANIZATION">Hughes</ENAMEX> 
          <ENAMEX TYPE="ORGANIZATION">et al</ENAMEX> . [ <ENAMEX TYPE="LAW">2</ENAMEX> ] analyzed their yeast
          <ENAMEX TYPE="ORGANIZATION">datasets</ENAMEX> using the commercial software package Resolver
          (<ENAMEX TYPE="ORGANIZATION">Rosetta Inpharmatics</ENAMEX>, <ENAMEX TYPE="GPE">Kirkland</ENAMEX>, <ENAMEX TYPE="ORGANIZATION">WA</ENAMEX>). Resolver was
          developed with a built-in error model that is derived
          from repeated data obtained on the array platform of
          interest. <ENAMEX TYPE="ORGANIZATION">Resolver</ENAMEX> uses this error <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX> and available
          repeated data to estimate the error in expression ratios
          for each gene sampled. In addition, as described below
          and in [ <ENAMEX TYPE="LAW">2</ENAMEX> ] , <ENAMEX TYPE="ORGANIZATION">Resolver</ENAMEX>'s clustering algorithms use the
          error estimates to weigh the similarity measures. This
          results in lower weights for data points with lower
          confidence in the cluster analysis. The net result of
          this treatment (as we show below) is an improvement in
          both cluster accuracy and cluster stability.
          Medvedovic 
          <ENAMEX TYPE="ORGANIZATION">et al</ENAMEX> . [ <TIMEX TYPE="DATE">18</TIMEX> ] have taken a
          different approach by adopting the <ENAMEX TYPE="ORGANIZATION">Bayesian</ENAMEX> infinite
          mixture model (IMM) to incorporate repeated measurements
          in cluster analysis. They postulated a probability model
          for gene-expression data that incorporates repeated data,
          and estimated the posterior pairwise probabilities of
          coexpression with a <ENAMEX TYPE="ORGANIZATION">Gibbs</ENAMEX> sampler. They showed that the
          estimated posterior pairwise distance allowed for easy
          identification of unrelated objects. These posterior
          pairwise distances can be clustered using average linkage
          or complete linkage hierarchical algorithms.
        
        
          Our contributions
          We have implemented several approaches to take
          advantage of repeated measurements in cluster analysis
          and performed an empirical study evaluating clustering
          results using both real and synthetic gene-expression
          <ENAMEX TYPE="ORGANIZATION">datasets</ENAMEX>. We tested several different clustering
          <ENAMEX TYPE="ORGANIZATION">algorithms</ENAMEX> and similarity measure combinations on the
          same datasets and evaluated the quality of each approach
          using the same criteria. We also assessed <NUMEX TYPE="CARDINAL">four</NUMEX> different
          approaches to clustering repeated array data: clustering
          the averaged expression levels over the repeated
          measurements; using variability estimates in similarity
          measures (assigning low weights to noisy data points);
          clustering the repeated measurements as individual data
          points and assigning them to the same subtrees in
          agglomerative hierarchical algorithms; and an <ENAMEX TYPE="GPE">IMM</ENAMEX>-based
          approach with built-in error models for repeated data. We
          use <NUMEX TYPE="CARDINAL">two</NUMEX> assessment criteria to evaluate clustering
          results: cluster accuracy (comparing clustering results
          to known external knowledge of the data); and cluster
          stability (the consistency of <ENAMEX TYPE="PER_DESC">objects</ENAMEX> clustered together
          on synthetic remeasured data). In addition, we extended
          the <ENAMEX TYPE="GPE">IMM</ENAMEX>-based approach and the variability-weighted
          approach. We also created synthetic array datasets with
          error distributions taken from real data. These synthetic
          <ENAMEX TYPE="ORGANIZATION">data</ENAMEX> in which the clusters are known are crucial for the
          <ENAMEX TYPE="ORGANIZATION">development</ENAMEX> and testing of novel clustering
          <ENAMEX TYPE="ORGANIZATION">algorithms</ENAMEX>.
          Over a variety of clustering algorithms, we showed
          that array data with repeated measurements yield more
          accurate and more stable clusters. When repeated
          measurements are available, both the variability-weighted
          similarity approach and the <ENAMEX TYPE="GPE">IMM</ENAMEX>-based approach improve
          cluster accuracy and cluster stability to a greater
          extent than the simple approach of averaging over the
          repeated measurements. The <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX>-based approaches
          (hierarchical model-based <ENAMEX TYPE="ORG_DESC">algorithm</ENAMEX> [ <ENAMEX TYPE="LAW">8</ENAMEX> ] and the IMM
          approach [ <TIMEX TYPE="DATE">18</TIMEX> ] ) consistently produce more accurate and
          more stable clusters.
        
      
      
        Results
        
          Overview of our empirical study
          In our empirical study, we compare the quality of
          clustering results from a variety of algorithms on array
          <ENAMEX TYPE="ORGANIZATION">data</ENAMEX> with repeated measurements. We use <NUMEX TYPE="CARDINAL">two</NUMEX> methods to
          assess cluster quality: cluster accuracy and cluster
          stability. External validation compares clustering
          results to known independent external knowledge of which
          <ENAMEX TYPE="PERSON">objects</ENAMEX> (<ENAMEX TYPE="SUBSTANCE">genes</ENAMEX>, experiments or both) should cluster
          together [ <TIMEX TYPE="DATE">19</TIMEX> ] . A clustering result that agrees with
          the external knowledge is assumed to be accurate.
          However, for most biological data, there is little or no 
          a priori knowledge of this type. We
          also evaluate the stability of clusters with respect to
          synthetic remeasured array data. That is, if one
          remeasures the array data, how often are objects
          clustered together in the original data assigned to the
          same clusters in the remeasured data?
          In this section, we discuss the clustering algorithms
          implemented, approaches to clustering repeated
          measurements, and the real and synthetic <ENAMEX TYPE="SUBSTANCE">datasets</ENAMEX> used in
          our empirical study. We will also discuss assessment of
          cluster quality in greater detail. Finally, we present
          and discuss results of our study.
        
        
          Test algorithms and similarity measures
          We studied the performance of a wide variety of
          clustering algorithms, including several agglomerative
          hierarchical algorithms (average linkage, centroid
          <ENAMEX TYPE="PERSON">linkage</ENAMEX>, complete linkage and single linkage), a divisive
          hierarchical algorithm called <ENAMEX TYPE="PRODUCT">DIANA</ENAMEX> [ <TIMEX TYPE="DATE">20</TIMEX> ] , k-means [ <NUMEX TYPE="CARDINAL">7</NUMEX>
          ] , a graph-theoretic algorithm called <ENAMEX TYPE="PRODUCT">CAST</ENAMEX> [ <TIMEX TYPE="DATE">21</TIMEX> ] , a
          <ENAMEX TYPE="PERSON">finite Gaussian</ENAMEX> mixture model-based hierarchical
          clustering algorithm from <ENAMEX TYPE="ORGANIZATION">MCLUST</ENAMEX> [ <ENAMEX TYPE="LAW">8</ENAMEX> ] , and an <ENAMEX TYPE="GPE">IMM</ENAMEX>-based
          approach [ <TIMEX TYPE="DATE">18</TIMEX> ] . Agglomerative hierarchical clustering
          algorithms successively merge similar objects (or
          <ENAMEX TYPE="ORGANIZATION">subtrees</ENAMEX>) to form a dendrogram. To evaluate cluster
          quality, we obtain clusters from the dendrogram by
          stopping the merging process when the desired number of
          <ENAMEX TYPE="PERSON">clusters</ENAMEX> (subtrees) is produced. The <ENAMEX TYPE="PER_DESC">objects</ENAMEX> in these
          subtrees form the resulting clusters. Except for the two
          model-based approaches, all other clustering algorithms
          require a pairwise similarity measure. We used both
          <ENAMEX TYPE="PERSON">correlation</ENAMEX> and Euclidean distance in our empirical
          study.
        
        
          How to cluster array data with repeated
          measurements
          
            Average over repeated measurements
            The simplest approach is to compute the average
            expression levels over all repeated measurements for
            each gene and each experiment, and store these average
            expression levels in the raw data matrix. The pairwise
            <ENAMEX TYPE="PERSON">similarities</ENAMEX> (correlation or distance) can be computed
            using these average expression values. This is the
            approach taken in the vast majority of published
            reports for which repeated measurements are
            available.
          
          
            Variability-weighted similarity measures
            The averaging approach does not take into account
            the variability in repeated measurements. <ENAMEX TYPE="ORGANIZATION">Hughes</ENAMEX> 
            <ENAMEX TYPE="ORGANIZATION">et al</ENAMEX> . [ <ENAMEX TYPE="LAW">2</ENAMEX> ] proposed an
            error-weighted clustering approach that uses error
            estimates to weigh expression values in pairwise
            similarities such that expression values with high
            error estimates are down-weighted. These error-weighted
            pairwise similarities are then used as inputs to
            clustering algorithms. <ENAMEX TYPE="ORGANIZATION">Hughes</ENAMEX> 
            <ENAMEX TYPE="ORGANIZATION">et al</ENAMEX> . [ <ENAMEX TYPE="LAW">2</ENAMEX> ] developed an error
            model that assigns relatively high error estimates to
            genes that show greater variation in their repeated
            expression levels than other <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> at similar abundance
            in their control experiments. In our empirical study,
            we use variability estimates instead of error estimates
            in the weighted similarity measures. Intuitively, gene
            expression levels that show larger variations over the
            repeated measurements should be assigned lower
            confidence (weights). We use either the standard
            <ENAMEX TYPE="PERSON">deviation</ENAMEX> (SD) or coefficient of variation (CV) as
            <ENAMEX TYPE="ORGANIZATION">variability</ENAMEX> estimates. Let us illustrate this approach
            with an example: suppose our goal is to compute the
            variability-weighted correlation of <NUMEX TYPE="CARDINAL">two</NUMEX> <ENAMEX TYPE="SUBSTANCE">genes G1</ENAMEX> and
            G2. For each experiment, we compute the <ENAMEX TYPE="ORGANIZATION">SD</ENAMEX> or CV over
            the repeated measurements for these <NUMEX TYPE="CARDINAL">two</NUMEX> <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX>.
            Experiments with relatively high variability estimates
            (<ENAMEX TYPE="ORGANIZATION">SD</ENAMEX> or CV) are down-weighted in the
            variability-weighted correlation of <TIMEX TYPE="DATE">G1 and G2</TIMEX> (see
            Materials and methods for mathematical definitions of
            these weighted similarities).
          
          
            Hierarchical clustering of repeated
            measurements
            An alternative idea is to cluster the repeated
            measurements as individual <ENAMEX TYPE="PER_DESC">objects</ENAMEX> in hierarchical
            clustering algorithms. The idea is to initialize the
            agglomerative algorithm by assigning repeated
            measurements of each object to the same subtrees in the
            <ENAMEX TYPE="ORGANIZATION">dendrogram</ENAMEX>. In each successive step, <NUMEX TYPE="CARDINAL">two</NUMEX> subtrees
            containing repeated measurements are merged. This
            approach of forcing repeated measurements into the same
            <ENAMEX TYPE="ORGANIZATION">subtrees</ENAMEX> is abbreviated as <ENAMEX TYPE="ORGANIZATION">FITSS</ENAMEX> (forcing into the same
            <ENAMEX TYPE="ORGANIZATION">subtrees</ENAMEX>). In addition to heuristically based
            hierarchical algorithms (such as average linkage,
            complete linkage, centroid linkage and single linkage),
            we also investigate the performance of clustering
            repeated data with <ENAMEX TYPE="ORGANIZATION">MCLUST-HC</ENAMEX>, which is a model-based
            hierarchical clustering algorithm from <ENAMEX TYPE="ORGANIZATION">MCLUST</ENAMEX> [ <ENAMEX TYPE="LAW">8</ENAMEX> ]
            .
          
          
            <ENAMEX TYPE="GPE">IMM</ENAMEX>-based approach
            Medvedovic 
            <ENAMEX TYPE="ORGANIZATION">et al</ENAMEX> . [ <TIMEX TYPE="DATE">18</TIMEX> ] postulated a
            probability model (an infinite Gaussian mixture model)
            for gene-expression data which incorporates repeated
            <ENAMEX TYPE="ORGANIZATION">data</ENAMEX>. Each cluster is assumed to follow a multivariate
            normal distribution, and the measured repeated
            expression levels follow another multivariate normal
            distribution. They used a <ENAMEX TYPE="ORGANIZATION">Gibbs</ENAMEX> sampler to estimate the
            posterior pairwise probabilities of coexpression. These
            posterior pairwise probabilities are treated as
            pairwise similarities, which are used as inputs to
            clustering algorithms such as average linkage or
            complete linkage hierarchical algorithms. They showed
            that these posterior pairwise probabilities led to easy
            identification of unrelated objects, and hence are
            superior to other pairwise similarity measures such as
            Euclidean distance.
            The <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX> published in Medvedovic 
            <ENAMEX TYPE="ORGANIZATION">et al</ENAMEX> . [ <TIMEX TYPE="DATE">18</TIMEX> ] assumes that the
            variance between repeated measurements of the same
            genes is homogeneous across all experiments. We call
            this model the spherical model. We extended the IMM
            approach to include an elliptical model, in which
            repeated measurements may have different variance
            across the experiments. In other words, <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> may have
            different noise levels in the spherical model, while
            both genes and experiments may have different noise
            levels in the elliptical model.
            <ENAMEX TYPE="PRODUCT">Table 1summarizes</ENAMEX> the clustering algorithms and
            similarity measures implemented in our empirical study,
            and the corresponding methods to cluster repeated
            <ENAMEX TYPE="ORGANIZATION">data</ENAMEX>.
          
        
        
          Datasets
          
            Completely synthetic data
            Because independent external knowledge is often
            unavailable on real data, we created synthetic data
            that have error distributions derived from real array
            <ENAMEX TYPE="ORGANIZATION">data</ENAMEX>. We use a two-step process to generate synthetic
            <ENAMEX TYPE="ORGANIZATION">data</ENAMEX>. In the <NUMEX TYPE="ORDINAL">first</NUMEX> step, data are generated according
            to artificial patterns such that the true class of each
            object is known. We created <NUMEX TYPE="CARDINAL">six</NUMEX> equal-sized classes, of
            which <NUMEX TYPE="CARDINAL">four</NUMEX> are sine waves shifted in phase relative to
            each other (a periodic pattern) and the remaining <NUMEX TYPE="CARDINAL">two</NUMEX>
            classes are represented by linear functions
            (<NUMEX TYPE="MONEY">non-periodic</NUMEX>). In the <NUMEX TYPE="ORDINAL">second</NUMEX> step, error is added to
            the synthetic patterns using an experimentally derived
            error distribution. The error for each data point is
            randomly sampled (with <ENAMEX TYPE="PER_DESC">replacement</ENAMEX>) from the
            distribution of standard deviations of log ratios over
            the repeated measurements on the yeast galactose data
            (described below). The error-added data are generated
            from a random normal distribution with mean equal to
            the value of the synthetic pattern (from the first
            step), and <ENAMEX TYPE="ORGANIZATION">SD</ENAMEX> equal to the sampled error. The
            signal-to-noise of the synthetic data is adjusted by
            linearly scaling the error before adding it to the
            pattern. We generate multiple synthetic <ENAMEX TYPE="SUBSTANCE">datasets</ENAMEX> with
            <ENAMEX TYPE="PRODUCT">400</ENAMEX> data points, <NUMEX TYPE="CARDINAL">20</NUMEX> attributes, <ENAMEX TYPE="CONTACT_INFO">1, 4</ENAMEX> or <NUMEX TYPE="CARDINAL">20</NUMEX> repeated
            measurements and <NUMEX TYPE="CARDINAL">2</NUMEX> different levels of signal-to-noise
            (low and high noise levels). In our synthetic data, all
            genes in each <ENAMEX TYPE="PER_DESC">class</ENAMEX> have identical patterns (before
            error is added). The cluster structure of real data
            <ENAMEX TYPE="PERSON">will</ENAMEX>, in general, be less distinguishable than that of
            these synthetic data. Hence, it is of interest to study
            the performance of various clustering approaches as a
            function of noise level in the synthetic data. Figure
            1a,1bshows the expression profiles of the <ENAMEX TYPE="PER_DESC">classes</ENAMEX> in
            typical datasets with <NUMEX TYPE="CARDINAL">four</NUMEX> repeated measurements at low
            and high noise levels respectively.
          
          
            Real data: <ENAMEX TYPE="SUBSTANCE">yeast</ENAMEX> galactose data
            In the yeast galactose data of Ideker 
            <ENAMEX TYPE="ORGANIZATION">et al</ENAMEX> . [ <TIMEX TYPE="DATE">22</TIMEX> ] , <NUMEX TYPE="CARDINAL">four</NUMEX> replicate
            hybridizations were performed for each cDNA array
            experiment. We used a subset of <NUMEX TYPE="CARDINAL">205</NUMEX> <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> that are
            <ENAMEX TYPE="PRODUCT">reproducibly</ENAMEX> measured, whose expression patterns
            reflect <NUMEX TYPE="CARDINAL">four</NUMEX> functional categories in the <ENAMEX TYPE="PERSON">Gene Ontology</ENAMEX>
            (<ENAMEX TYPE="ORGANIZATION">GO</ENAMEX>) listings [ <TIMEX TYPE="DATE">23</TIMEX> ] and that we expect to cluster
            together. On this data, our goal is to cluster the
            genes, and the <NUMEX TYPE="CARDINAL">four</NUMEX> functional categories are used as
            our external knowledge. That is, we evaluate algorithm
            performance by how closely the clusters reproduce these
            <NUMEX TYPE="CARDINAL">four</NUMEX> functional categories.
          
          
            Synthetic remeasured data
            To generate synthetic remeasured array data to
            evaluate cluster stability, we need an error model that
            describes repeated measurements. Ideker 
            <ENAMEX TYPE="ORGANIZATION">et al</ENAMEX> . [ <TIMEX TYPE="DATE">24</TIMEX> ] proposed an error
            model for repeated cDNA array data in which the
            measured fluorescent intensity levels in each of the
            <NUMEX TYPE="CARDINAL">two</NUMEX> channels are related to their true intensities by
            <ENAMEX TYPE="PERSON">additive</ENAMEX>, multiplicative and random error parameters.
            The multiplicative error parameters represent errors
            that are proportional to the true intensity, while the
            additive error parameters represent errors that are
            constant with respect to the true intensity. The
            measured intensity levels in the <NUMEX TYPE="CARDINAL">two</NUMEX> channels are
            correlated such that <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> at higher intensities have
            higher correlation. Ideker 
            <ENAMEX TYPE="ORGANIZATION">et al</ENAMEX> . [ <TIMEX TYPE="DATE">24</TIMEX> ] estimated these
            <ENAMEX TYPE="PERSON">parameters</ENAMEX> (<ENAMEX TYPE="SUBSTANCE">additive</ENAMEX>, multiplicative and correlation
            parameters) from repeated cDNA array data using maximum
            likelihood, and showed that this model gives reasonable
            estimates of the true expression intensities with <NUMEX TYPE="CARDINAL">four</NUMEX>
            repeated measurements. We used this error model to
            estimate the true intensity for each gene, and the
            <ENAMEX TYPE="PERSON">correlation</ENAMEX>, <ENAMEX TYPE="SUBSTANCE">additive</ENAMEX> and multiplicative error
            parameters on the yeast galactose data. We generate
            synthetic remeasured data by generating the random
            error components in the <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX> from the specified random
            distributions.
          
        
        
          Assessment of cluster quality
          
            Cluster accuracy
            To assess algorithm performance, we need a statistic
            that indicates the agreement between the external
            knowledge and the clustering result. A clustering
            result can be considered as a partition of <ENAMEX TYPE="PER_DESC">objects</ENAMEX> into
            <ENAMEX TYPE="ORG_DESC">groups</ENAMEX>. In all subsequent discussion, the term 'class'
            is used to refer to the external knowledge, while the
            term 'cluster' refers to the partitions created by the
            <ENAMEX TYPE="ORGANIZATION">algorithm</ENAMEX>. Assuming known categories (<ENAMEX TYPE="PER_DESC">classes</ENAMEX>) of
            objects are available, we can compare clustering
            results by assessing the agreement of the clusters with
            the classes. Unfortunately, the results of a given
            cluster analysis may merge partitions that the external
            knowledge indicates should be separate or may create
            additional partitions that should not exist. Hence,
            comparison of clusters with <ENAMEX TYPE="PER_DESC">classes</ENAMEX> is not as simple as
            counting which <ENAMEX TYPE="PER_DESC">objects</ENAMEX> are placed in the 'correct'
            <ENAMEX TYPE="ORGANIZATION">partitions</ENAMEX>. In fact, with some datasets and algorithms,
            there is no obvious relationship between the classes
            and the clusters.
            The adjusted <ENAMEX TYPE="ORGANIZATION">Rand</ENAMEX> index [ <TIMEX TYPE="DATE">25</TIMEX> ] is a statistic
            designed to assess the degree of agreement between <NUMEX TYPE="CARDINAL">two</NUMEX>
            <ENAMEX TYPE="ORGANIZATION">partitions</ENAMEX>. On the basis of an extensive empirical
            study, <ENAMEX TYPE="ORGANIZATION">Milligan</ENAMEX> and <ENAMEX TYPE="ORGANIZATION">Cooper</ENAMEX> [ <TIMEX TYPE="DATE">26</TIMEX> ] recommended the
            adjusted Rand index as the measure of agreement even
            when comparing partitions with different numbers of
            <ENAMEX TYPE="PERSON">clusters</ENAMEX>. The <ENAMEX TYPE="ORGANIZATION">Rand</ENAMEX> index [ <TIMEX TYPE="DATE">27</TIMEX> ] is defined as the
            fraction of agreement, that is, the number of pairs of
            objects that are either in the same <ENAMEX TYPE="ORG_DESC">groups</ENAMEX> in both
            <ENAMEX TYPE="ORGANIZATION">partitions</ENAMEX> or in different <ENAMEX TYPE="ORG_DESC">groups</ENAMEX> in both partitions,
            divided by the total number of pairs of <ENAMEX TYPE="PER_DESC">objects</ENAMEX>. The
            <ENAMEX TYPE="ORGANIZATION">Rand</ENAMEX> index lies <TIMEX TYPE="DATE">between 0 and 1</TIMEX>. When the two
            <ENAMEX TYPE="PER_DESC">partitions</ENAMEX> agree perfectly, the <ENAMEX TYPE="ORGANIZATION">Rand</ENAMEX> index is <NUMEX TYPE="CARDINAL">1</NUMEX>. The
            adjusted Rand index [ <TIMEX TYPE="DATE">25</TIMEX> ] adjusts the score so that
            its expected value in the case of random partitions is
            <NUMEX TYPE="CARDINAL">0</NUMEX>. A high adjusted <ENAMEX TYPE="ORGANIZATION">Rand</ENAMEX> index indicates a high level of
            agreement between the <ENAMEX TYPE="PER_DESC">classes</ENAMEX> and clusters.
          
          
            Cluster stability
            A few recent papers suggested that the quality of
            <ENAMEX TYPE="PERSON">clusters</ENAMEX> could be evaluated via cluster stability, that
            is, how consistently objects are clustered together
            with respect to synthetic remeasured data. The
            synthetic remeasured data is created by randomly
            perturbing the original data using error parameters
            derived from repeated measurements. For example, <ENAMEX TYPE="ORGANIZATION">Kerr</ENAMEX>
            and <ENAMEX TYPE="PERSON">Churchill</ENAMEX> [ <TIMEX TYPE="DATE">17</TIMEX> ] and <ENAMEX TYPE="PERSON">Li</ENAMEX> and <ENAMEX TYPE="PERSON">Wong</ENAMEX> [ <TIMEX TYPE="DATE">28</TIMEX> ] generated
            randomly perturbed data from cDNA and oligonucleotide
            arrays respectively to identify objects that are
            consistently clustered.
            In our empirical study, we assess the level of
            agreement of clusters from the original data with
            clusters from the synthetic remeasured data by
            computing the average adjusted <ENAMEX TYPE="ORGANIZATION">Rand</ENAMEX> index over all the
            synthetic <ENAMEX TYPE="SUBSTANCE">datasets</ENAMEX>. We also compute the average
            adjusted Rand index <NUMEX TYPE="CARDINAL">between all</NUMEX> pairs of clustering
            results from the randomly remeasured data. A high
            average adjusted <ENAMEX TYPE="ORGANIZATION">Rand</ENAMEX> index implies that the clusters
            are stable with respect to data perturbations and
            <ENAMEX TYPE="ORGANIZATION">remeasurements</ENAMEX>. The external knowledge is not used in
            computing cluster stability.
          
        
        
          Completely synthetic data at low noise level
          Table 2a,2bshows selected results on cluster accuracy
          and cluster stability on the completely synthetic
          datasets with <NUMEX TYPE="CARDINAL">four</NUMEX> simulated repeated measurements. Table
          2a,2bshow results from average linkage, complete linkage
          and centroid linkage hierarchical algorithms, k-means,
          <ENAMEX TYPE="ORGANIZATION">MCLUST-HC</ENAMEX> (a hierarchical model-based clustering
          algorithm from <ENAMEX TYPE="ORGANIZATION">MCLUST</ENAMEX>) and <ENAMEX TYPE="ORGANIZATION">IMM</ENAMEX>. Both single linkage and
          <ENAMEX TYPE="PERSON">DIANA</ENAMEX> produce very low-quality and unstable clusters and
          their adjusted Rand indices are not shown. For each
          <ENAMEX TYPE="ORGANIZATION">clustering</ENAMEX> approach, we produced <NUMEX TYPE="CARDINAL">six</NUMEX> clusters (which is
          the number of <ENAMEX TYPE="PER_DESC">classes</ENAMEX>). The results from <ENAMEX TYPE="PER_DESC">CAST</ENAMEX> are not
          shown because the input <ENAMEX TYPE="PER_DESC">parameter</ENAMEX> cannot be tuned to
          produce exactly <NUMEX TYPE="CARDINAL">six</NUMEX> clusters in many cases. The FITSS
          column refers to the method of forcing repeated
          measurements into the same subtrees. Because k-means is
          not hierarchical, its results are not available (<ENAMEX TYPE="ORGANIZATION">NA</ENAMEX>)
          under the <ENAMEX TYPE="ORGANIZATION">FITSS</ENAMEX> column. Both centroid linkage
          hierarchical algorithm and k-means algorithm require the
          raw data matrix as input, so we cannot apply these two
          <ENAMEX TYPE="ORGANIZATION">algorithms</ENAMEX> to cluster the posterior pairwise
          probabilities from the <ENAMEX TYPE="ORGANIZATION">IMM</ENAMEX> approach.
          In terms of cluster accuracy, the elliptical model of
          <ENAMEX TYPE="ORGANIZATION">IMM</ENAMEX> produced the highest level of agreement (adjusted
          <ENAMEX TYPE="ORGANIZATION">Rand</ENAMEX> index = <NUMEX TYPE="CARDINAL">0.957</NUMEX>) with the <NUMEX TYPE="CARDINAL">six</NUMEX> classes, and the
          <ENAMEX TYPE="PRODUCT">hierarchical</ENAMEX> <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX>-based clustering algorithm (MCLUST-HC)
          also produced clusters with high agreement (adjusted Rand
          index = <NUMEX TYPE="CARDINAL">0.930</NUMEX>) with the <NUMEX TYPE="CARDINAL">six</NUMEX> classes. Within the same
          clustering algorithm, different similarity measures and
          different methods to deal with repeated measurements
          yield different cluster accuracy. For example, average
          linkage hierarchical algorithm produced more accurate
          clusters with Euclidean distance (variability-weighted or
          average over-repeated measurements) than correlation. The
          variability-weighted similarity approach produced more
          accurate clusters using <ENAMEX TYPE="ORGANIZATION">SDs</ENAMEX> as the variability estimates
          than using the <ENAMEX TYPE="ORGANIZATION">CVs</ENAMEX>. It is also interesting to note that
          SD-weighted correlation produced relatively low-quality
          <ENAMEX TYPE="PERSON">clusters</ENAMEX>, whereas <ENAMEX TYPE="ORGANIZATION">SD</ENAMEX>-weighted distance produced
          relatively accurate clusters. The <ENAMEX TYPE="ORGANIZATION">FITSS</ENAMEX> approach of
          forcing repeated measurements into the same subtrees in
          hierarchical clustering algorithms does not yield high
          cluster accuracy.
          In terms of cluster stability, most clustering
          approaches yield stable clusters (with average adjusted
          <ENAMEX TYPE="ORGANIZATION">Rand</ENAMEX> indices <NUMEX TYPE="MONEY">above 0.900</NUMEX>) except the spherical model of
          the <ENAMEX TYPE="ORGANIZATION">IMM</ENAMEX> approach. This is because the spherical model
          assumes homogeneous variability for each gene across the
          experiments (which is not true on this synthetic
          <ENAMEX TYPE="ORGANIZATION">data</ENAMEX>).
        
        
          Completely synthetic data at high noise
          level
          Tables 3a,3bshow the results on cluster accuracy and
          cluster stability on the completely synthetic data with
          <NUMEX TYPE="CARDINAL">four</NUMEX> repeated measurements at high noise level. Even at a
          higher noise level, the elliptical model of <ENAMEX TYPE="SUBSTANCE">IMM</ENAMEX> produced
          much more accurate clusters (average adjusted <ENAMEX TYPE="ORGANIZATION">Rand</ENAMEX> index
          = <NUMEX TYPE="CARDINAL">0.911</NUMEX> and <NUMEX TYPE="CARDINAL">0.910</NUMEX> using average linkage or complete
          <ENAMEX TYPE="ORGANIZATION">linkage</ENAMEX>) than all other approaches (SD-weighted distance
          and k-means produced an average adjusted <ENAMEX TYPE="ORGANIZATION">Rand</ENAMEX> index of
          <NUMEX TYPE="MONEY">0.801</NUMEX>). In general, the relative rankings of various
          <ENAMEX TYPE="PER_DESC">clustering</ENAMEX> approaches at high noise level are similar to
          those at low noise level, except that the <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX>-based
          hierarchical approach (MCLUST-HC) produced less accurate
          clusters than the SD-weighted distance approach using the
          <ENAMEX TYPE="ORGANIZATION">heuristically</ENAMEX> based algorithms.
          At high noise level, the approach of averaging over
          the repeated measurements produced relatively low-quality
          <ENAMEX TYPE="PERSON">clusters</ENAMEX>, especially when Euclidean distance is used (for
          example, both average linkage and centroid linkage
          produced an average adjusted <ENAMEX TYPE="ORGANIZATION">Rand</ENAMEX> index of <NUMEX TYPE="CARDINAL">0</NUMEX>). In
          addition, the quality of clusters produced using
          Euclidean distance deteriorates more rapidly than
          correlation at high noise level. The SD-weighted distance
          approach produced substantial improvement in cluster
          quality over the approach of averaging over repeated
          measurements using the same algorithms at high noise
          level.
          In terms of cluster stability (see <ENAMEX TYPE="PRODUCT">Table 3b</ENAMEX>), the
          following <NUMEX TYPE="CARDINAL">three</NUMEX> approaches yield average adjusted Rand
          index above <NUMEX TYPE="CARDINAL">0.900</NUMEX>: the elliptical model of the IMM
          approach; the SD-weighted distance using average linkage
          and centroid linkage. It is interesting that the
          <ENAMEX TYPE="ORGANIZATION">spherical</ENAMEX> model of the <ENAMEX TYPE="ORGANIZATION">IMM</ENAMEX> approach produces unstable
          clusters at both high and low noise levels.
        
        
          Yeast galactose data
          Table 4a,4bshow selected results on cluster accuracy
          and cluster stability on real yeast galactose data. The
          true mean column in <ENAMEX TYPE="PRODUCT">Table 4arefers</ENAMEX> to clustering the true
          mean data (estimated with the error model suggested by
          Ideker 
          <ENAMEX TYPE="ORGANIZATION">et al</ENAMEX> . [ <TIMEX TYPE="DATE">24</TIMEX> ] ) instead of
          clustering the repeated measurements. For each clustering
          approach, we produced <NUMEX TYPE="CARDINAL">four</NUMEX> clusters (which is the number
          of functional categories). The highest level of cluster
          <ENAMEX TYPE="PERSON">accuracy</ENAMEX> (adjusted <ENAMEX TYPE="ORGANIZATION">Rand</ENAMEX> index = <NUMEX TYPE="CARDINAL">0.968</NUMEX> in <ENAMEX TYPE="PRODUCT">Table 4a</ENAMEX>) was
          obtained with several algorithms: centroid linkage
          hierarchical algorithm with Euclidean distance and
          averaging over the repeated measurements; hierarchical
          model-based algorithm (MCLUST-HC); complete linkage
          hierarchical algorithm with SD-weighted distance; and IMM
          with complete linkage. Clustering with repeated
          measurements produced more accurate clusters than
          clustering with the estimated true mean data in most
          cases.
          <ENAMEX TYPE="PRODUCT">Table 4bshows</ENAMEX> that different clustering approaches
          lead to different cluster stability with respect to
          remeasured data. Similar to the results from the
          completely synthetic data, Euclidean distance tends to
          produce more stable clusters than correlation (both
          variability-weighted and average over repeated
          measurements). Clustering results using <ENAMEX TYPE="ORGANIZATION">FITSS</ENAMEX> were less
          stable than the variability-weighted approach and the
          averaging over repeated measurements approach.
          <ENAMEX TYPE="ORGANIZATION">SD</ENAMEX> produced more accurate and more stable clusters
          than <ENAMEX TYPE="ORGANIZATION">CV</ENAMEX> in the variability-weighted similarity approach,
          especially when Euclidean distance is used. In addition,
          the <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX>-based approaches (MCLUST-<ENAMEX TYPE="ORGANIZATION">HC</ENAMEX> and <ENAMEX TYPE="ORGANIZATION">IMM</ENAMEX>) produced
          relatively accurate and stable clusters on this data.
        
        
          Effect of different numbers of repeated
          measurements
          To study the effect of different numbers of repeated
          measurements on the performance of various clustering
          approaches, we generated completely synthetic data with
          different numbers of simulated repeated measurements for
          each data point. Specifically, we generated <NUMEX TYPE="MONEY">1</NUMEX>, <NUMEX TYPE="CARDINAL">4</NUMEX>, or <NUMEX TYPE="CARDINAL">20</NUMEX>
          repeated measurements at both the low and high noise
          levels. The quality of clustering results on datasets
          with higher numbers of repeated measurements is usually
          higher (<ENAMEX TYPE="PRODUCT">Table 5</ENAMEX>). For example, using the same algorithms
          and same similarity measures cluster accuracy is
          considerably improved with synthetic datasets of <NUMEX TYPE="CARDINAL">four</NUMEX>
          repeated measurements relative to datasets with no
          repeated measurement. With <NUMEX TYPE="CARDINAL">20</NUMEX> repeated measurements,
          Euclidean distance is less sensitive to noise, and the
          SD-weighted distance approach produces comparable cluster
          accuracy to <ENAMEX TYPE="ORGANIZATION">IMM</ENAMEX>. This is probably because the variability
          estimates computed over <TIMEX TYPE="DATE">20</TIMEX> repeated measurements are much
          more robust than those with <NUMEX TYPE="CARDINAL">four</NUMEX> repeated measurements.
          Nevertheless, the elliptical model of <ENAMEX TYPE="ORGANIZATION">IMM</ENAMEX> consistently
          produced the most accurate clusters over different
          numbers of simulated repeated measurements and different
          noise levels.
        
      
      
        Discussion
        
          Limitations
          In all the above results, we produced clustering
          results in which the number of clusters was set equal to
          the number of classes. In agglomerative hierarchical
          clustering algorithms (for example, average linkage), we
          <ENAMEX TYPE="ORGANIZATION">successively</ENAMEX> merged clusters until the desired number of
          <ENAMEX TYPE="PERSON">clusters</ENAMEX>, <ENAMEX TYPE="ORGANIZATION">K</ENAMEX>, is reached, and considered the <ENAMEX TYPE="ORGANIZATION">K subtrees</ENAMEX> as
          our <ENAMEX TYPE="ORGANIZATION">K</ENAMEX> clusters, whereas in other algorithms the number of
          <ENAMEX TYPE="PERSON">clusters</ENAMEX> was provided as input. A <ENAMEX TYPE="ORG_DESC">concern</ENAMEX> is that using a
          fixed number of clusters will force different classes
          into the same cluster owing to <NUMEX TYPE="CARDINAL">one</NUMEX> or more outliers
          occupying a cluster. In such cases, the adjusted Rand
          index might improve with a larger number of clusters.
          However, we chose to use a fixed number of clusters
          for several reasons. First, with the exception of the
          model-based algorithms, all other clustering algorithms
          (directly or indirectly) require the number of clusters
          as input. Even with the <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX>-based algorithms, the
          number of clusters can only be estimated. In <ENAMEX TYPE="ORGANIZATION">MCLUST-HC</ENAMEX>,
          the number of clusters can be estimated using a
          statistical score (see [ <TIMEX TYPE="DATE">29</TIMEX> ] ). In the <ENAMEX TYPE="ORGANIZATION">IMM</ENAMEX> approach, the
          number of clusters can be estimated from the posterior
          distribution of clustering results (see [ <TIMEX TYPE="DATE">18</TIMEX> ] ). <NUMEX TYPE="ORDINAL">Second</NUMEX>,
          it is very difficult, if not impossible, to compare
          cluster quality over a range of different clustering
          <ENAMEX TYPE="ORGANIZATION">algorithms</ENAMEX> when the number of clusters is not fixed.
          Finally, increasing the number of clusters does not
          always yield better clusters or higher Rand indices (data
          not shown).
          There are also some limitations with the external
          criteria for the real datasets used in our empirical
          study. With the yeast galactose data, we used a subset of
          <NUMEX TYPE="CARDINAL">205</NUMEX> <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX>, which contains many <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> previously shown to
          be strongly co-regulated and which reflect <NUMEX TYPE="CARDINAL">four</NUMEX>
          functional categories in the GO listings [ <TIMEX TYPE="DATE">23</TIMEX> ] . This
          subset of <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> may be biased in the sense that they are
          not chosen entirely independently of their expression
          patterns. In addition, there may be good biological
          reasons why some genes in the chosen set of <NUMEX TYPE="CARDINAL">205</NUMEX> should
          not cluster into <ENAMEX TYPE="ORG_DESC">groups</ENAMEX> segregated by the GO
          classifications.
        
        
          Distributions of variability-weighted similarity
          measures
          The essence of the variability-weighted similarity
          approach is that the pairwise similarities take into
          account the variability in repeated measurements. In an
          attempt to understand the effect of variability between
          repeated measurements on these similarity measures, we
          computed the correlation coefficients between all pairs
          of <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> in the yeast galactose data and plotted the
          distribution of the fraction of gene pairs against
          correlation coefficient by averaging over repeated
          measurements and against <ENAMEX TYPE="ORGANIZATION">SD</ENAMEX>-weighted correlation in
          Figure <NUMEX TYPE="CARDINAL">2</NUMEX>. The distribution of CV-weighted correlation is
          similar to that of SD-weighted.
          Figure <TIMEX TYPE="DATE">2shows</TIMEX> that when <ENAMEX TYPE="ORGANIZATION">SD</ENAMEX> is used in
          variability-weighted correlation, there are more gene
          pairs with correlation coefficients around <NUMEX TYPE="CARDINAL">0</NUMEX> and fewer
          gene pairs with correlation coefficients near <NUMEX TYPE="CARDINAL">1</NUMEX>. Figure
          3shows the distribution of Euclidean distance by
          averaging over the repeated measurements and the
          SD-weighted distance on the same data. There are more
          gene pairs with distance <NUMEX TYPE="CARDINAL">close to zero</NUMEX> when variability
          estimates are used to weigh distance. This shows that
          weighing similarity measures with variability estimates
          produces more conservative estimates of pairwise
          similarities.
          Moreover, we showed that on average,
          variability-weighted similarity measures (both
          <ENAMEX TYPE="PERSON">correlation</ENAMEX> and distance) computed from repeated
          measurements produced pairwise similarities closer to the
          true similarity than similarity measures computed from
          <ENAMEX TYPE="ORGANIZATION">data</ENAMEX> with no repeated measurement. In our simulation
          experiment, we computed the true pairwise correlation and
          distance between all pairs of genes on the estimated true
          mean yeast galactose data (using the error model in
          Ideker 
          et al. [ <TIMEX TYPE="DATE">24</TIMEX> ] ). We also computed
          the variability-weighted correlation and distance between
          <NUMEX TYPE="CARDINAL">all</NUMEX> pairs of genes on the synthetic remeasured data
          generated from the same error parameters and mean
          intensities as the yeast galactose data. In addition, we
          computed correlation and distance using <NUMEX TYPE="CARDINAL">only one</NUMEX> of the
          repeated measurements in the remeasured data. Then, we
          compared the average deviation of the
          variability-weighted similarity measures from the truth,
          and the average deviation of the similarity measures on
          the data with no repeated measurements to the truth (see
          Materials and methods for detailed results).
        
        
          Modified variability-weighted approach
          <NUMEX TYPE="CARDINAL">One</NUMEX> of the drawbacks of the current definitions of the
          variability-weighted similarity approach is that only
          noisy experiments are down-weighted, whereas noisy genes
          are not. Suppose we have a dataset in which some genes
          are noisier than others, but the noise levels across the
          experiments stay relatively constant. In this scenario,
          the variability-weighted approach would not improve
          cluster quality. Genes expressed at low levels are
          frequently expressed at low levels across all experiments
          and usually have higher variability (see Figure <NUMEX TYPE="CARDINAL">4</NUMEX>).
          Hence, unless we filter out low-intensity <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX>, the
          weighting methods developed by <ENAMEX TYPE="ORGANIZATION">Hughes</ENAMEX> 
          <ENAMEX TYPE="ORGANIZATION">et al.</ENAMEX> ( [ <ENAMEX TYPE="LAW">2</ENAMEX> ] and see Materials
          and methods) will not down-weight these genes. We
          attempted to correct for this effect by removing the
          normalizing factor in the definition of
          variability-weighted distance (see <ENAMEX TYPE="PRODUCT">Materials</ENAMEX> and methods
          for mathematical definitions). This improved the
          clustering accuracy when Euclidean distance was used.
          However, we did not see improvement using this method
          with correlation as the similarity measure.
        
      
      
        Conclusions
        Our work shows that clustering array data with repeated
        measurements can significantly improve cluster quality,
        especially when the appropriate clustering approach is
        <ENAMEX TYPE="ORGANIZATION">applied</ENAMEX>. Different clustering algorithms and different
        methods to take advantage of repeated measurements (not
        surprisingly) yield different clusters with different
        quality. In practice, many clustering algorithms are
        frequently run on the same dataset and the results most
        consistent with previous beliefs are published. A better
        approach would be to use a clustering algorithm shown to be
        the most accurate and stable when applied to data with
        similar signal-to-noise and other characteristics as the
        <ENAMEX TYPE="ORGANIZATION">data</ENAMEX> of interest. In this work, we analyzed both real and
        completely synthetic data with many algorithms to assess
        cluster accuracy and stability. In general, the <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX>-based
        <ENAMEX TYPE="PER_DESC">clustering</ENAMEX> approaches produce higher-quality clusters,
        especially the elliptical model of the <ENAMEX TYPE="ORGANIZATION">IMM</ENAMEX>. In particular,
        the higher the noise level, the greater the performance
        difference between the <ENAMEX TYPE="ORGANIZATION">IMM</ENAMEX> approach and other methods.
        For the heuristically based approaches, average linkage
        hierarchical clustering algorithm combined with SD-weighted
        Euclidean distance also produces relatively stable and
        accurate clusters. On the completely synthetic data, we
        showed that the infinite mixture approach works amazingly
        well with <NUMEX TYPE="CARDINAL">only four</NUMEX> repeated measurements, even at high
        noise levels. The variability-weighted approach works
        almost as well as the <ENAMEX TYPE="ORGANIZATION">IMM</ENAMEX> with <NUMEX TYPE="CARDINAL">20</NUMEX> repeated measurements.
        From our results on the synthetic data, we showed that
        there is significant improvement in cluster accuracy from
        <NUMEX TYPE="CARDINAL">one</NUMEX> to <NUMEX TYPE="CARDINAL">four</NUMEX> repeated measurements using <ENAMEX TYPE="ORGANIZATION">IMM</ENAMEX> at both low and
        high noise levels (<ENAMEX TYPE="PRODUCT">Table 5</ENAMEX>). However, there is no
        substantial improvement in cluster accuracy from <NUMEX TYPE="CARDINAL">4 to 20</NUMEX>
        repeated measurements with the <ENAMEX TYPE="ORGANIZATION">IMM</ENAMEX> approach (<ENAMEX TYPE="PRODUCT">Table 5</ENAMEX>).
        There are many possible directions of future work, both
        <ENAMEX TYPE="ORGANIZATION">methodological</ENAMEX> and experimental. Because the elliptical
        model of <ENAMEX TYPE="ORGANIZATION">IMM</ENAMEX> produces very high-quality clusters, it would
        be interesting to develop a similar error model in the
        <ENAMEX TYPE="PRODUCT">finite</ENAMEX> <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX>-based framework on <ENAMEX TYPE="ORGANIZATION">MCLUST</ENAMEX> and to compare the
        performance of the finite versus infinite mixture
        approaches. Another practical methodological development
        would be to incorporate the estimation of missing data
        values into the <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX>-based approaches. It would also be
        interesting to develop other variability-weighted
        similarity measures that would down-weight both noisy genes
        and noisy experiments.
        In terms of future experimental work, we would like to
        evaluate the performance of various clustering algorithms
        on array data with repeated measurements on more real
        <ENAMEX TYPE="ORGANIZATION">datasets</ENAMEX>. <NUMEX TYPE="CARDINAL">One</NUMEX> of the difficulties we encountered is that
        there are very few public datasets that have both repeated
        measurements and external criteria available. We would
        greatly appreciate it if <ENAMEX TYPE="PER_DESC">readers</ENAMEX> would provide us with
        access to such datasets as they become available.
      
      
        Materials and methods
        
          Datasets
          
            Yeast galactose data
            Ideker 
            et al. [ <TIMEX TYPE="DATE">22</TIMEX> ] studied galactose
            utilization in <ENAMEX TYPE="SUBSTANCE">yeast</ENAMEX> using cDNA arrays by deleting <NUMEX TYPE="CARDINAL">nine</NUMEX>
            genes on the galactose utilization pathway in the
            presence or absence of galactose and raffinose. There
            are a total of <NUMEX TYPE="CARDINAL">20</NUMEX> experiments (<NUMEX TYPE="CARDINAL">nine</NUMEX> single-gene
            <ENAMEX TYPE="PERSON">deletions</ENAMEX> and <TIMEX TYPE="TIME">one wild</TIMEX>-type experiment with galactose
            and raffinose, <NUMEX TYPE="CARDINAL">nine</NUMEX> deletions and <TIMEX TYPE="TIME">one wild</TIMEX>-type without
            <ENAMEX TYPE="ORGANIZATION">galactose</ENAMEX> and raffinose). <NUMEX TYPE="CARDINAL">Four</NUMEX> replicate hybridizations
            were performed for each experiment. We used a subset of
            <NUMEX TYPE="CARDINAL">205</NUMEX> <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> from this data, whose expression patterns
            reflect <NUMEX TYPE="CARDINAL">four</NUMEX> functional categories in the GO [ <TIMEX TYPE="DATE">23</TIMEX> ]
            .
          
          
            Synthetic remeasured cDNA data
            Let 
            x 
            
              ijr 
             and 
            <ENAMEX TYPE="ORGANIZATION">y</ENAMEX> 
            
              ijr 
             be the fluorescent intensities of the <NUMEX TYPE="CARDINAL">two</NUMEX> channels
            (fluorescent dyes) for gene 
            i , experiment 
            <ENAMEX TYPE="PERSON">j</ENAMEX> and repeated measurement 
            r , where 
            i = <NUMEX TYPE="CARDINAL">1</NUMEX>, ..., <ENAMEX TYPE="ORGANIZATION">G</ENAMEX>, 
            <ENAMEX TYPE="PRODUCT">j = 1</ENAMEX>, .., E, 
            r = <NUMEX TYPE="CARDINAL">1</NUMEX>, .., <ENAMEX TYPE="NATIONALITY">R.</ENAMEX> For the yeast
            galactose data, <ENAMEX TYPE="ORGANIZATION">G</ENAMEX> is <NUMEX TYPE="CARDINAL">approximately 6,000</NUMEX>, <ENAMEX TYPE="ORGANIZATION">E</ENAMEX> is <NUMEX TYPE="CARDINAL">20 and</NUMEX> R
            is <NUMEX TYPE="CARDINAL">4</NUMEX>. Ideker 
            <ENAMEX TYPE="ORGANIZATION">et al</ENAMEX> . [ <TIMEX TYPE="DATE">24</TIMEX> ] proposed an error
            model for replicated cDNA array data in which the
            observed fluorescent intensity levels are related to
            their true expression levels by the following
            model:
            
            where ( 
            μ 
            
              xij 
             , 
            μ 
            
              yij 
             ) are the true mean intensity levels for gene 
            i under experiment 
            <ENAMEX TYPE="PERSON">j</ENAMEX> in the <NUMEX TYPE="CARDINAL">two</NUMEX> channels. The
            multiplicative error parameters in the <NUMEX TYPE="CARDINAL">two</NUMEX> channels ( 
            ε 
            
              xijr 
             , 
            ε 
            
              yijr 
             ) are assumed to follow the bivariate normal
            distribution with mean <NUMEX TYPE="CARDINAL">0</NUMEX>, <ENAMEX TYPE="ORGANIZATION">SDs</ENAMEX> σ 
            ε 
            xj  , 
            σ 
            
              ε 
              yj 
             and correlation 
            ρ 
            
              εj 
             . Similarly, the <ENAMEX TYPE="SUBSTANCE">additive</ENAMEX> error parameters ( 
            δ 
            
              xijr 
             , 
            δ 
            
              yijr 
             ) are assumed to follow the bivariate normal
            distribution with mean <NUMEX TYPE="CARDINAL">0</NUMEX>, <ENAMEX TYPE="ORGANIZATION">SDs</ENAMEX> σ 
            δ 
            xj  , 
            σ 
            
              δyj 
             and correlation 
            ρ 
            
              δj 
             . The gene-independent parameters ( 
            σ 
            
              εxj 
             , 
            σ 
            
              ε 
              yj 
             , 
            ρ 
            
              εj 
             , 
            σ 
            
              δxj 
             , 
            σ 
            
              δyj 
             , 
            ρ 
            
              δj 
             ) and the gene-dependent parameters ( 
            μ 
            
              xij 
             , 
            μ 
            
              yij 
             ), where 
            i = <NUMEX TYPE="CARDINAL">1</NUMEX>, ..., <ENAMEX TYPE="ORGANIZATION">G</ENAMEX> and 
            <ENAMEX TYPE="PRODUCT">j = 1</ENAMEX>, ..., E, are estimated by
            maximum likelihood [ <TIMEX TYPE="DATE">24</TIMEX> ] .
            Using this error model, we estimate the true
            expression intensities for each gene and the
            <ENAMEX TYPE="PERSON">gene-independent</ENAMEX> parameters for each of the <NUMEX TYPE="CARDINAL">20</NUMEX>
            experiments in the yeast galactose data. From the gene
            independent parameters ( 
            σ 
            
              ε 
              xj 
             , 
            σ 
            
              εyj 
             , 
            ρ 
            
              εj 
             , 
            σ 
            
              δ 
              xj 
             , 
            σ 
            
              δyj 
             , 
            ρ 
            
              δj 
             ), we generate random ( 
            ε 
            
              xijr 
             , 
            ε 
            
              yijr 
             ) and ( 
            δ 
            
              xijr 
             , 
            δ 
            
              yijr 
             ) from the bivariate normal distributions. Hence,
            we can generate random remeasured data (and log ratios)
            using the estimated true mean intensities ( 
            μ 
            
              xij 
             , 
            μ 
            
              yij 
             ).
          
          
            Completely synthetic data
            The completely synthetic <ENAMEX TYPE="SUBSTANCE">datasets</ENAMEX> consist of <NUMEX TYPE="CARDINAL">400</NUMEX>
            <ENAMEX TYPE="ORGANIZATION">data</ENAMEX> points (<ENAMEX TYPE="SUBSTANCE">genes</ENAMEX>), <NUMEX TYPE="CARDINAL">20</NUMEX> attributes (experiments) and <NUMEX TYPE="CARDINAL">6</NUMEX>
            classes. Let 
            <ENAMEX TYPE="ORGANIZATION">φ</ENAMEX> ( 
            i,j ) be the artificial pattern
            of gene 
            i and experiment 
            <ENAMEX TYPE="PERSON">j</ENAMEX> before error is added, and
            suppose gene 
            i belongs to class 
            <ENAMEX TYPE="ORGANIZATION">k</ENAMEX> . <NUMEX TYPE="CARDINAL">Four</NUMEX> of the <NUMEX TYPE="CARDINAL">six</NUMEX> classes
            follow the periodic sine function ( 
            <ENAMEX TYPE="ORGANIZATION">φ</ENAMEX> ( 
            i,j ) = sin (<ENAMEX TYPE="CONTACT_INFO">2</ENAMEX> 
            π 
            <ENAMEX TYPE="PRODUCT">j /10</ENAMEX> - 
            ω 
            
              <ENAMEX TYPE="ORGANIZATION">k</ENAMEX> 
             )), and the remaining <NUMEX TYPE="CARDINAL">two</NUMEX> classes follow the
            non-periodic linear function ( 
            <ENAMEX TYPE="ORGANIZATION">φ</ENAMEX> ( 
            i,j ) = 
            <ENAMEX TYPE="PRODUCT">j /20</ENAMEX> or 
            <ENAMEX TYPE="ORGANIZATION">φ</ENAMEX> ( 
            i,j ) = - 
            <ENAMEX TYPE="PRODUCT">j /20</ENAMEX>), where 
            i = <NUMEX TYPE="CARDINAL">1</NUMEX>, <TIMEX TYPE="DATE">2, 3</TIMEX>, ..., <ENAMEX TYPE="CONTACT_INFO">400,</ENAMEX> 
            <ENAMEX TYPE="CONTACT_INFO">j = 1, 2, 3</ENAMEX>, ..., <TIMEX TYPE="DATE">20</TIMEX>, 
            <ENAMEX TYPE="ORGANIZATION">k</ENAMEX> = <NUMEX TYPE="CARDINAL">1</NUMEX>, <ENAMEX TYPE="CONTACT_INFO">2, 3, 4</ENAMEX> and 
            ω 
            
              <ENAMEX TYPE="ORGANIZATION">k</ENAMEX> 
             is a random phase shift between <NUMEX TYPE="CARDINAL">0 and 2</NUMEX> 
            <ENAMEX TYPE="ORGANIZATION">π .</ENAMEX> Let 
            X ( 
            i,j,r ) be the error-added value
            for gene 
            i , experiment 
            <ENAMEX TYPE="PERSON">j</ENAMEX> and repeated measurement 
            r . Let the randomly sampled
            error be 
            σ 
            
              ij 
             for gene 
            i and experiment 
            <ENAMEX TYPE="PERSON">j</ENAMEX> , and 
            X ( 
            i,j,r ) is generated from a
            <ENAMEX TYPE="ORGANIZATION">random</ENAMEX> normal distribution with mean equal to 
            <ENAMEX TYPE="ORGANIZATION">φ</ENAMEX> ( 
            i,j ), and <ENAMEX TYPE="ORGANIZATION">SD</ENAMEX> equal to 
            σ 
            
              ij 
             .
            We define the signal-to-noise ratio of a synthetic
            <ENAMEX TYPE="ORGANIZATION">dataset</ENAMEX> to be the ratio of the range of signals (in our
            case, <NUMEX TYPE="CARDINAL">1</NUMEX>-(<ENAMEX TYPE="CONTACT_INFO">-1</ENAMEX>) = <NUMEX TYPE="CARDINAL">2</NUMEX>) to the average sampled error. For the
            completely synthetic data shown in Figure 1a,1bthe
            signal-to-noise ratios are <NUMEX TYPE="CARDINAL">14.3</NUMEX> and <NUMEX TYPE="CARDINAL">2.5</NUMEX>
            respectively.
          
        
        
          Missing data
          The <ENAMEX TYPE="SUBSTANCE">yeast galactose dataset</ENAMEX> [ <TIMEX TYPE="DATE">22</TIMEX> ] contains
          <NUMEX TYPE="PERCENT">approximately 8%</NUMEX> of missing data values. There are many
          possible <ENAMEX TYPE="PER_DESC">sources</ENAMEX> of missing data values, for example, low
          signal-to-noise ratios, dust or scratches on slides. As
          the current versions of <ENAMEX TYPE="ORGANIZATION">MCLUST</ENAMEX> [ <TIMEX TYPE="DATE">30</TIMEX> ] and the IMM
          implementation [ <TIMEX TYPE="DATE">18</TIMEX> ] do not handle missing data values,
          we impute the missing data values. We experimented with
          <NUMEX TYPE="CARDINAL">two</NUMEX> imputation methods, namely model-based multiple
          <ENAMEX TYPE="ORGANIZATION">imputation</ENAMEX> [ <TIMEX TYPE="DATE">31</TIMEX> ] as implemented in <ENAMEX TYPE="GPE">Splus</ENAMEX>, and weighted
          <ENAMEX TYPE="ORGANIZATION">k</ENAMEX>-nearest <ENAMEX TYPE="PER_DESC">neighbors</ENAMEX> (KNNimpute) [ <TIMEX TYPE="DATE">32</TIMEX> ] . We found that
          <ENAMEX TYPE="ORGANIZATION">data</ENAMEX> after <ENAMEX TYPE="ORGANIZATION">KNNimpute</ENAMEX> produce higher-quality clusters than
          <ENAMEX TYPE="ORGANIZATION">data</ENAMEX> after model-based multiple imputation. Therefore, we
          <ENAMEX TYPE="ORGANIZATION">applied KNNimpute</ENAMEX> to the yeast galactose data before
          applying the <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX>-based approaches.
        
        
          <ENAMEX TYPE="ORGANIZATION">Notations</ENAMEX> and similarity measures
          Suppose there are 
          G genes, 
          E experiments, and 
          R repeated measurements. Denote the
          measured expression level from repeated measurement 
          r of gene 
          g under experiment 
          e as 
          X 
          
            <ENAMEX TYPE="CONTACT_INFO">ger</ENAMEX> 
           , where 
          g = <NUMEX TYPE="CARDINAL">1</NUMEX>, ..., 
          G , 
          e = <NUMEX TYPE="CARDINAL">1</NUMEX>, ..., 
          E and 
          r = <NUMEX TYPE="CARDINAL">1</NUMEX>, ..., 
          R . Let 
          D be the raw data matrix such that 
          D ( 
          g,e ) represents the average
          expression level over 
          R repeated measurements for gene 
          g under experiment 
          e , that is,
          ,
          where 
          g = <NUMEX TYPE="CARDINAL">1</NUMEX>, ..., 
          G , 
          e = <NUMEX TYPE="CARDINAL">1</NUMEX>, ..., 
          E . The correlation coefficient
          between a pair of genes 
          i and 
          <ENAMEX TYPE="PERSON">j</ENAMEX> ( 
          i,<ENAMEX TYPE="PER_DESC">j</ENAMEX> = <ENAMEX TYPE="CONTACT_INFO">1, ..,</ENAMEX> 
          <ENAMEX TYPE="ORGANIZATION">G</ENAMEX> ) is defined as
          
          where
          
          is the average expression level of gene 
          i over all 
          E experiments. The Euclidean
          distance between a pair of genes 
          i and 
          <ENAMEX TYPE="PERSON">j</ENAMEX> ( 
          i,<ENAMEX TYPE="PER_DESC">j</ENAMEX> = <ENAMEX TYPE="CONTACT_INFO">1, ..,</ENAMEX> 
          <ENAMEX TYPE="ORGANIZATION">G</ENAMEX> ) is defined as
          .
          Similarly, we can define correlation and Euclidean
          distance between a pair of experiments by swapping the
          positions of the gene and experiment indices.
        
        
          Variability-weighted similarity measures
          <ENAMEX TYPE="ORGANIZATION">Hughes</ENAMEX> 
          <ENAMEX TYPE="ORGANIZATION">et al</ENAMEX> . [ <ENAMEX TYPE="LAW">2</ENAMEX> ] defined
          error-weighted similarity measures that weight expression
          values with error estimates such that expression values
          with relatively high errors are down-weighted. Let 
          σ 
          
            <ENAMEX TYPE="ORGANIZATION">ge</ENAMEX> 
           be the error estimate of the expression level of
          gene 
          g under experiment 
          e , where 
          g = <NUMEX TYPE="CARDINAL">1</NUMEX>, ..., 
          G and 
          e = <NUMEX TYPE="CARDINAL">1</NUMEX>, ..., 
          E . The error-weighted correlation
          between a pair of genes 
          i and 
          <ENAMEX TYPE="PERSON">j</ENAMEX> is defined as
          
          where
          
          is the weighted average expression level of gene i.
          Similarly, the error-weighted Euclidean distance [ <ENAMEX TYPE="LAW">2</ENAMEX> ] is
          defined as
          .
          In our empirical study, variability estimates are used
          instead of error estimates. In particular, we use either
          the <ENAMEX TYPE="ORGANIZATION">SD</ENAMEX> or CV over the 
          R repeated measurements as 
          σ 
          
            <ENAMEX TYPE="ORGANIZATION">ge</ENAMEX> 
           . These variability-weighted similarity measures
          serve as inputs to many clustering algorithms.
        
        
          Modified variability-weighted distance
          The above definitions of variability-weighted
          <ENAMEX TYPE="PERSON">correlation</ENAMEX> and distance down-weight noisy experiments in
          computing the pairwise similarity, but would not work in
          the case of noisy genes. Consider <NUMEX TYPE="CARDINAL">two</NUMEX> pairs of genes, ( 
          X, <ENAMEX TYPE="ORGANIZATION">Y</ENAMEX> ) and ( 
          <ENAMEX TYPE="ORGANIZATION">W</ENAMEX>, <ENAMEX TYPE="WORK_OF_ART">Z</ENAMEX> ), such that 
          D ( 
          X,e ) = 
          D ( 
          <ENAMEX TYPE="ORGANIZATION">W</ENAMEX>,e ) and 
          D ( 
          <ENAMEX TYPE="ORGANIZATION">Y</ENAMEX>,e ) = 
          D ( 
          Z,e ) and 
          σ 
          
            Xe 
           = 
          σ 
          
            Ye 
           << 
          σ 
          
            We 
           = 
          σ 
          
            Ze 
           for all experiments 
          e . In other words, the expression
          patterns of gene 
          X and gene 
          <ENAMEX TYPE="ORGANIZATION">W</ENAMEX> are identical, so are the
          patterns of gene 
          <ENAMEX TYPE="ORGANIZATION">Y</ENAMEX> and gene 
          Z . The levels of noise (or
          <ENAMEX TYPE="ORGANIZATION">variability</ENAMEX>) are constant across all the experiments for
          each pair of genes, but <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> ( 
          <ENAMEX TYPE="ORGANIZATION">W</ENAMEX>,<ENAMEX TYPE="WORK_OF_ART">Z</ENAMEX> ) are much more noisy than
          genes ( 
          X,<ENAMEX TYPE="ORGANIZATION">Y</ENAMEX> ). Using the above definitions
          of variability-weighted similarity, 
          
            XY 
           = 
          
            WZ 
           and 
          
            XY 
           = 
          
            WZ 
           . Intuitively, one would expect the pairwise
          similarity between <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> ( 
          <ENAMEX TYPE="ORGANIZATION">W</ENAMEX>,<ENAMEX TYPE="WORK_OF_ART">Z</ENAMEX> ) to be lower than that of
          genes ( 
          X,<ENAMEX TYPE="ORGANIZATION">Y</ENAMEX> ) because <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> ( 
          <ENAMEX TYPE="ORGANIZATION">W</ENAMEX>,<ENAMEX TYPE="WORK_OF_ART">Z</ENAMEX> ) are more noisy. We
          experimented with a modified definition of
          variability-weighted distance by removing the scaling
          factor in the denominator:
          
          This modified definition tends to give slightly better
          <ENAMEX TYPE="PERSON">clusters</ENAMEX> (see Additional data files and [ <TIMEX TYPE="DATE">33</TIMEX> ] ).
        
        
          Clustering algorithms
          
            Agglomerative hierarchical algorithms
            In agglomerative hierarchical clustering algorithms
            [ <ENAMEX TYPE="LAW">5</ENAMEX> ] , each object is initially assigned to its own
            <ENAMEX TYPE="ORGANIZATION">cluster</ENAMEX> (subtree), and the number of initial clusters
            is equal to the number of <ENAMEX TYPE="PER_DESC">objects</ENAMEX>. Similar clusters
            (<NUMEX TYPE="MONEY">subtrees</NUMEX>) are successively merged to form a
            <ENAMEX TYPE="ORGANIZATION">dendrogram</ENAMEX>. In each merging step, the number of
            <ENAMEX TYPE="PERSON">clusters</ENAMEX> (subtrees) is reduced by <NUMEX TYPE="CARDINAL">one</NUMEX>. This merging
            process is repeated until the desired number of
            <ENAMEX TYPE="PERSON">clusters</ENAMEX>, <ENAMEX TYPE="ORGANIZATION">K</ENAMEX>, is produced. The <ENAMEX TYPE="PER_DESC">objects</ENAMEX> in these <ENAMEX TYPE="ORGANIZATION">K</ENAMEX>
            subtrees form the resulting <ENAMEX TYPE="ORGANIZATION">K</ENAMEX> clusters, and the
            hierarchical structures of the subtrees are
            ignored.
            Different definitions of cluster similarity yield
            different clustering algorithms. In a single linkage
            hierarchical algorithm, the cluster similarity of <NUMEX TYPE="CARDINAL">two</NUMEX>
            <ENAMEX TYPE="PERSON">clusters</ENAMEX> is the maximum similarity between a pair of
            genes, one from each of the two clusters. In a complete
            linkage hierarchical algorithm, the cluster similarity
            is defined as the minimum similarity between a pair of
            genes, one from each of the two clusters. In an average
            linkage hierarchical algorithm, the cluster similarity
            of <NUMEX TYPE="CARDINAL">two</NUMEX> clusters is the average pairwise similarity
            between genes in the two clusters. In a centroid
            linkage hierarchical algorithm, clusters (subtrees) are
            represented by the mean vectors of the clusters, and
            cluster similarity is defined as the similarity between
            the mean vectors.
          
          
            <ENAMEX TYPE="ORGANIZATION">k</ENAMEX>-means
            <ENAMEX TYPE="ORGANIZATION">K</ENAMEX>-means [ <ENAMEX TYPE="LAW">7</ENAMEX> ] is a classic iterative clustering
            algorithm, in which the number of clusters is an input
            to the algorithm. Clusters are represented by
            centroids, which are cluster <ENAMEX TYPE="FAC_DESC">centers</ENAMEX>. The goal of
            <ENAMEX TYPE="ORGANIZATION">k</ENAMEX>-means is to minimize the sum of distances from each
            object to its corresponding centroid. In each
            <ENAMEX TYPE="PERSON">iteration</ENAMEX>, each gene is assigned to its closest
            <ENAMEX TYPE="ORGANIZATION">centroid</ENAMEX>. After the gene reassignment, new centroids
            are computed. The steps of <ENAMEX TYPE="SUBSTANCE">assigning genes</ENAMEX> to centroids
            and computing new centroids are repeated until no genes
            are moved between clusters. In our implementation, we
            use the clusters from average linkage hierarchical
            <ENAMEX TYPE="ORGANIZATION">algorithm</ENAMEX> to compute initial centroids to start
            <ENAMEX TYPE="ORGANIZATION">k</ENAMEX>-means.
          
          
            MCLUST
            The finite Gaussian mixture model-based approach
            assumes that each cluster follows the multivariate
            normal distribution with model parameters that specify
            the location and shape of each cluster. <ENAMEX TYPE="ORGANIZATION">MCLUST</ENAMEX> [ <ENAMEX TYPE="LAW">8</ENAMEX> ]
            implements the expectation-maximization (EM) algorithm
            for <ENAMEX TYPE="SUBSTANCE">clustering</ENAMEX> via finite Gaussian mixture models, as
            well as <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX>-based hierarchical clustering algorithms,
            with optional cross-cluster constraints. <ENAMEX TYPE="ORGANIZATION">MCLUST</ENAMEX> also
            includes a clustering function (hcVVV) that uses
            model-based hierarchical clustering to initialize the
            <ENAMEX TYPE="ORGANIZATION">EM</ENAMEX> algorithm. Because the current version of MCLUST
            does not have any mechanism to incorporate repeated
            measurements, but does allow <ENAMEX TYPE="PER_DESC">initializations</ENAMEX> at
            nontrivial partitions, we initialize the hierarchical
            model-based algorithm with subtrees containing repeated
            measurements. We use the most general model
            (<NUMEX TYPE="MONEY">unconstrained</NUMEX>) for hierarchical clustering, which
            allows each cluster to have different volume,
            <ENAMEX TYPE="PERSON">orientation</ENAMEX> and shape. This approach is abbreviated as
            <ENAMEX TYPE="ORGANIZATION">MCLUST-HC</ENAMEX>.
          
          
            IMM
            The <ENAMEX TYPE="ORGANIZATION">IMM</ENAMEX> approach uses a <ENAMEX TYPE="ORGANIZATION">Gibbs</ENAMEX> sampler to estimate
            the posterior pairwise probabilities. The <ENAMEX TYPE="ORGANIZATION">Gibbs</ENAMEX> sampler
            requires <NUMEX TYPE="CARDINAL">two</NUMEX> sets of parameters for input:
            initialization parameters (random <ENAMEX TYPE="SUBSTANCE">seed</ENAMEX> and the initial
            number of mixture components) and convergence
            <ENAMEX TYPE="PERSON">parameters</ENAMEX> (initial annealing coefficient, the rate of
            <ENAMEX TYPE="ORGANIZATION">'cooling</ENAMEX>' and the <ENAMEX TYPE="PER_DESC">'burn</ENAMEX>-in' period). A posterior
            distribution with multiple peaks could result in Gibbs
            <ENAMEX TYPE="ORGANIZATION">samplers</ENAMEX>' inability to escape from <NUMEX TYPE="QUANTITY">a suboptimal</NUMEX> peak.
            The role of the annealing coefficient [ <TIMEX TYPE="DATE">34</TIMEX> ] is to
            flatten the posterior distribution of clustering
            results and thus alleviate the difficulty in
            transitioning between high-probability regions that are
            separated by regions of low probability, which is a
            common problem of <ENAMEX TYPE="ORGANIZATION">Gibbs</ENAMEX> samplers in general [ <TIMEX TYPE="DATE">35</TIMEX> ] .
            Burn-in corresponds to the number of initial iterations
            that the <ENAMEX TYPE="ORGANIZATION">Gibbs</ENAMEX> <ENAMEX TYPE="PER_DESC">sampler</ENAMEX> takes to converge to the
            posterior distribution, and the burn-in iterations are
            not used in calculating posterior pairwise
            <ENAMEX TYPE="PERSON">probabilities</ENAMEX>. We tuned the convergence parameters by
            running independent samplers with different
            initialization parameters, and chose the set of
            convergence parameters that yielded the highest
            correlation between pairwise probabilities over
            different runs and over different random perturbations
            of the data. Using this simple principle, we identified
            a single combination of the annealing parameters that
            resulted in excellent convergence in all datasets we
            analyzed, including some not reported in this <ENAMEX TYPE="ORG_DESC">paper</ENAMEX>.
            This combination consisted of the initial annealing
            coefficient of <NUMEX TYPE="MONEY">0.01</NUMEX>, rate of cooling of <NUMEX TYPE="CARDINAL">0.999</NUMEX> and the
            burn-in of <NUMEX TYPE="CARDINAL">10,000</NUMEX> iterations. For <ENAMEX TYPE="PER_DESC">investigators</ENAMEX>
            analyzing their own data, we suggest that they run at
            least <NUMEX TYPE="CARDINAL">five</NUMEX> independent <ENAMEX TYPE="GPE">Gibbs</ENAMEX> samplers with this
            combination of parameters from <NUMEX TYPE="CARDINAL">five</NUMEX> different initial
            numbers of clusters and establish that <NUMEX TYPE="CARDINAL">all five</NUMEX>
            <ENAMEX TYPE="ORGANIZATION">converge</ENAMEX> to the same posterior distribution. This can
            be done by calculating correlation between posterior
            pairwise probabilities from different runs.
            Alternatively, the adjusted <ENAMEX TYPE="ORGANIZATION">Rand</ENAMEX> index can be used for
            comparing clustering results generated by different
            runs of the <ENAMEX TYPE="ORGANIZATION">Gibbs</ENAMEX> sampler. If the correlations or
            adjusted Rand indices suggest that <NUMEX TYPE="CARDINAL">all five</NUMEX> samplers
            did not converge to the same solution, <ENAMEX TYPE="PER_DESC">investigators</ENAMEX>
            should try increasing the annealing coefficient (to say
            <NUMEX TYPE="MONEY">0.9995</NUMEX>) and the burn-in number of iterations (to say
            <NUMEX TYPE="CARDINAL">20,000</NUMEX>), and repeat the process. The <ENAMEX TYPE="ORGANIZATION">Readme</ENAMEX>.txt file
            that accompanies the IMM software describes these
            parameters in detail.
          
          
            <ENAMEX TYPE="ORGANIZATION">CAST</ENAMEX>
            The cluster affinity search technique (<ENAMEX TYPE="ORGANIZATION">CAST</ENAMEX>) [ <TIMEX TYPE="DATE">21</TIMEX> ]
            is an iterative algorithm, in which <ENAMEX TYPE="PER_DESC">objects</ENAMEX> are added
            to or removed from the current cluster until there are
            no more similar objects to be added and no more
            dissimilar objects to be removed. At this point, the
            current cluster is assumed to be done. A new cluster is
            started and the iterative process of adding and
            removing objects is repeated until all <ENAMEX TYPE="PER_DESC">objects</ENAMEX> are
            assigned to clusters. The inputs to the algorithm
            include the pairwise similarities and a parameter that
            indirectly controls the number of clusters.
          
          
            <ENAMEX TYPE="ORGANIZATION">DIANA</ENAMEX>
            <ENAMEX TYPE="ORGANIZATION">DIANA</ENAMEX> [ <TIMEX TYPE="DATE">20</TIMEX> ] is a hierarchical divisive clustering
            algorithm, in which we start with all <ENAMEX TYPE="PER_DESC">objects</ENAMEX> in one
            <ENAMEX TYPE="ORGANIZATION">cluster</ENAMEX>. In each step, clusters are successively split
            to form <NUMEX TYPE="CARDINAL">two</NUMEX> clusters until the desired number of
            <ENAMEX TYPE="PERSON">clusters</ENAMEX> is reached. The cluster with maximum diameter
            (maximum pairwise dissimilarity) is split in each step.
            Let us call this the current cluster. The most
            dissimilar element in the current cluster is identified
            to start a new cluster. An object in the current
            <ENAMEX TYPE="ORGANIZATION">cluster</ENAMEX> is moved to the new cluster if the average
            similarity with the new cluster is higher than that
            with the current cluster.
          
        
        
          Completely synthetic data with different numbers of
          repeated measurements
          Table 5shows <NUMEX TYPE="CARDINAL">some</NUMEX> selected results produced using
          average linkage hierarchical algorithm on the completely
          synthetic data over varying numbers of repeated
          measurements and different noise levels. In general,
          increasing the number of repeated measurements increases
          cluster accuracy (average adjusted <ENAMEX TYPE="ORGANIZATION">Rand</ENAMEX> index with
          respect to the <NUMEX TYPE="CARDINAL">six</NUMEX> classes). The elliptical model of IMM
          produced superior quality of clusters, especially at high
          noise levels.
        
        
          <ENAMEX TYPE="PERSON">Simulation</ENAMEX> experiment: variability-weighted
          similarity
          We computed the true pairwise correlation and
          Euclidean distance between all pairs of genes on the
          estimated true mean <ENAMEX TYPE="SUBSTANCE">yeast</ENAMEX> galactose data. Denote the
          correlation between estimated true means for gene 
          i and gene 
          <ENAMEX TYPE="PERSON">j</ENAMEX> as . We generated synthetic
          <ENAMEX TYPE="ORGANIZATION">re</ENAMEX>-measured datasets using the same error parameters and
          true mean intensities of the yeast galactose data. Let
          the variability-weighted correlation for gene 
          i and gene 
          <ENAMEX TYPE="PERSON">j be</ENAMEX> , and the correlation computed
          using <NUMEX TYPE="CARDINAL">only one</NUMEX> of the repeated measurements, 
          r (no repeated data) be , where 
          <ENAMEX TYPE="ORGANIZATION">k</ENAMEX> is one of the randomly generated
          synthetic remeasured data.
          The column | - 
          ρ 
          true | in <ENAMEX TYPE="PRODUCT">Table 6shows</ENAMEX> the average
          of over all pairs of genes 
          i , 
          <ENAMEX TYPE="PERSON">j</ENAMEX> , and all randomly remeasured
          datasets 
          <ENAMEX TYPE="ORGANIZATION">k</ENAMEX> , while the column | 
          ρ 
          r - 
          ρ 
          true | shows the average of over
          <NUMEX TYPE="CARDINAL">all</NUMEX> pairs of genes 
          i , 
          <ENAMEX TYPE="PERSON">j</ENAMEX> , all randomly remeasured
          datasets 
          <ENAMEX TYPE="ORGANIZATION">k</ENAMEX> , and all repeated measurements 
          r . The corresponding results using
          distance are also shown in <ENAMEX TYPE="PRODUCT">Table 6</ENAMEX>, which shows that on
          average the variability-weighted similarities are closer
          to the 'truth' than similarities computed from data with
          no repeated measurement.
        
      
      
        Additional data files
        <ENAMEX TYPE="ORGANIZATION">Datasets</ENAMEX> (both real and synthetic) and the software
        (executables and documentation) used in this work are
        available as additional files and from our website [ <TIMEX TYPE="DATE">33</TIMEX> ] .
        They comprise additional results (Additional data file <NUMEX TYPE="CARDINAL">1</NUMEX>),
        <ENAMEX TYPE="PERSON">documentation</ENAMEX> (Additional data file <NUMEX TYPE="CARDINAL">2</NUMEX>), bytecode files for
        hierarchical agglomerative algorithms (Additional data file
        <NUMEX TYPE="CARDINAL">3</NUMEX>), bytecode files for k-means (Additional data file <NUMEX TYPE="CARDINAL">4</NUMEX>),
        <ENAMEX TYPE="ORGANIZATION">bytecode</ENAMEX> files for hierarchical agglomerative algorithms
        using <ENAMEX TYPE="ORGANIZATION">FITSS</ENAMEX> (Additional data file <NUMEX TYPE="CARDINAL">5</NUMEX>), subset of <NUMEX TYPE="CARDINAL">205</NUMEX> <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX>
        from yeast data (Additional data files <ENAMEX TYPE="CONTACT_INFO">6, 7, 8</ENAMEX>), and the
        completely synthetic <ENAMEX TYPE="SUBSTANCE">datasets</ENAMEX> (Additional data files <TIMEX TYPE="DATE">9, 10</TIMEX>,
        <TIMEX TYPE="DATE">11, 12, 13, 14</TIMEX>). Our website [ <TIMEX TYPE="DATE">33</TIMEX> ] also has external links
        to publicly available software, <ENAMEX TYPE="SUBSTANCE">yeast</ENAMEX> galactose data and
        lung <ENAMEX TYPE="DISEASE">cancer</ENAMEX> data.
        Additional data file 1
        Additional results
        Additional results
        Click here for additional data file
        Additional data file 2
        Documentation
        Documentation
        Click here for additional data file
        Additional data file 3
        <ENAMEX TYPE="ORGANIZATION">Bytecode</ENAMEX> files for hierarchical agglomerative
        algorithms
        <ENAMEX TYPE="ORGANIZATION">Bytecode</ENAMEX> files for hierarchical agglomerative
        algorithms
        Click here for additional data file
        Additional data file 4
        <ENAMEX TYPE="ORGANIZATION">Bytecode</ENAMEX> files for k-means
        <ENAMEX TYPE="ORGANIZATION">Bytecode</ENAMEX> files for k-means
        Click here for additional data file
        Additional data file 5
        <ENAMEX TYPE="ORGANIZATION">Bytecode</ENAMEX> files for hierarchical agglomerative algorithms
        using FITSS
        <ENAMEX TYPE="ORGANIZATION">Bytecode</ENAMEX> files for hierarchical agglomerative algorithms
        using FITSS
        Click here for additional data file
        Additional data file 6
        A subset of <NUMEX TYPE="CARDINAL">205</NUMEX> <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> from <ENAMEX TYPE="SUBSTANCE">yeast</ENAMEX> data
        A subset of <NUMEX TYPE="CARDINAL">205</NUMEX> <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> from <ENAMEX TYPE="SUBSTANCE">yeast</ENAMEX> data
        Click here for additional data file
        Additional data file 7
        A subset of <NUMEX TYPE="CARDINAL">205</NUMEX> <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> from <ENAMEX TYPE="SUBSTANCE">yeast</ENAMEX> data
        A subset of <NUMEX TYPE="CARDINAL">205</NUMEX> <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> from <ENAMEX TYPE="SUBSTANCE">yeast</ENAMEX> data
        Click here for additional data file
        Additional data file 8
        A subset of <NUMEX TYPE="CARDINAL">205</NUMEX> <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> from <ENAMEX TYPE="SUBSTANCE">yeast</ENAMEX> data
        A subset of <NUMEX TYPE="CARDINAL">205</NUMEX> <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> from <ENAMEX TYPE="SUBSTANCE">yeast</ENAMEX> data
        Click here for additional data file
        Additional data file 9
        A completely synthetic dataset
        A completely synthetic dataset
        Click here for additional data file
        Additional data file <NUMEX TYPE="CARDINAL">10</NUMEX>
        A completely synthetic dataset
        A completely synthetic dataset
        Click here for additional data file
        Additional data file <NUMEX TYPE="CARDINAL">11</NUMEX>
        A completely synthetic dataset
        A completely synthetic dataset
        Click here for additional data file
        Additional data file <NUMEX TYPE="CARDINAL">12</NUMEX>
        A completely synthetic dataset
        A completely synthetic dataset
        Click here for additional data file
        Additional data file <NUMEX TYPE="CARDINAL">13</NUMEX>
        A completely synthetic dataset
        A completely synthetic dataset
        Click here for additional data file
        Additional data file <NUMEX TYPE="CARDINAL">14</NUMEX>
        A completely synthetic dataset
        A completely synthetic dataset
        Click here for additional data file
      
    
  
