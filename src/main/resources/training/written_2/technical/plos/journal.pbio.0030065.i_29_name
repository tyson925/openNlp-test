
  
    
      
        
        Biological databases offer access to formalized facts about many aspects of
        <ENAMEX TYPE="PERSON">biology—genes</ENAMEX> and <ENAMEX TYPE="SUBSTANCE">gene products</ENAMEX>, <ENAMEX TYPE="SUBSTANCE">protein</ENAMEX> structure, metabolic pathways, <ENAMEX TYPE="DISEASE">diseases</ENAMEX>,
        <ENAMEX TYPE="PERSON">organisms</ENAMEX>, and so on. These databases are becoming increasingly important to <ENAMEX TYPE="PER_DESC">researchers</ENAMEX>.
        The information that populates databases is generated by research <ENAMEX TYPE="ORG_DESC">teams</ENAMEX> and is usually
        published in peer-reviewed <ENAMEX TYPE="ORG_DESC">journals</ENAMEX>. As part of the publication process, some <ENAMEX TYPE="PER_DESC">authors</ENAMEX>
        deposit data into a database but, more often, it is extracted from the published literature
        and deposited into the databases by human <ENAMEX TYPE="PER_DESC">curators</ENAMEX>, a painstaking process.
        Research literature and scientific databases fulfil different needs. Literature provides
        ideas and new hypotheses, but is not constrained to provide facts in formats suitable for
        use in databases. By contrast, databases efficiently provide large quantities of data and
        information in a standardised <ENAMEX TYPE="PER_DESC">schema</ENAMEX> representing a predefined interpretation of the data.
        While the acceptance of a <ENAMEX TYPE="ORG_DESC">paper</ENAMEX> can enforce the submission of data to a central data
        repository, such as <ENAMEX TYPE="ORGANIZATION">EMBL</ENAMEX> (<ENAMEX TYPE="CONTACT_INFO">www.</ENAMEX><ENAMEX TYPE="ORGANIZATION">ebi</ENAMEX>.<ENAMEX TYPE="ORGANIZATION">ac</ENAMEX>.<ENAMEX TYPE="CONTACT_INFO">uk/embl/</ENAMEX>) or ArrayExpress
        (<ENAMEX TYPE="CONTACT_INFO">www.</ENAMEX><ENAMEX TYPE="ORGANIZATION">ebi</ENAMEX>.<ENAMEX TYPE="ORGANIZATION">ac</ENAMEX>.<ENAMEX TYPE="CONTACT_INFO">uk/arrayexpress/</ENAMEX>), nobody receives credit for the submission of a fact to a
        database without an associated <ENAMEX TYPE="ORG_DESC">publication</ENAMEX>. As long as this <ENAMEX TYPE="ORG_DESC">practice</ENAMEX> continues, curation
        will be necessary to add the (re)formalised facts to biological databases.
        Given that <ENAMEX TYPE="ORG_DESC">publications</ENAMEX> are not about to be replaced with routine deposition of data
        into databases, is it possible to develop software tools to support the work of the
        <ENAMEX TYPE="PER_DESC">curator</ENAMEX>? Could we automatically analyse new scientific <ENAMEX TYPE="ORG_DESC">publications</ENAMEX> routinely to extract
        facts, which could then be inserted into scientific databases? Could we tag gene and
        protein names, as well as other terms in the document, so that they are easier to
        <ENAMEX TYPE="ORGANIZATION">recognise</ENAMEX>? How can we use controlled vocabularies and ontologies to identify biological
        <ENAMEX TYPE="PERSON">concepts</ENAMEX> and phenomena? Fortunately, there are many <ENAMEX TYPE="ORG_DESC">groups</ENAMEX> that are now seeking to answer
        these questions, precisely with a view to extracting facts from text.
        Part of the motivation for this effort in text mining technology is the inexorable rise
        in the amount of published literature (Figure <NUMEX TYPE="CARDINAL">1</NUMEX>). This massive growth, coupled with the
        current inefficiencies in transferring facts into other data resources, leads to the
        unfortunate state that biological <ENAMEX TYPE="PER_DESC">databases</ENAMEX> tend to be incomplete (for example, DNA
        <ENAMEX TYPE="ORGANIZATION">sequences</ENAMEX> without known function in genetic databases), and there are inconsistencies
        between databases and literature.
        In theory, text mining is the perfect solution to transforming factual knowledge from
        <ENAMEX TYPE="ORGANIZATION">publications</ENAMEX> into database entries. But computational linguists have not yet developed
        tools that can analyse <NUMEX TYPE="PERCENT">more than 30%</NUMEX> of English sentences correctly and transform them into
        a structured formal representation [<NUMEX TYPE="CARDINAL">1,2</NUMEX>]. We can analyse part of a sentence, such as a
        <ENAMEX TYPE="PERSON">subphrase</ENAMEX> describing a protein–protein interaction or part of a sentence containing a gene
        and a <ENAMEX TYPE="SUBSTANCE">protein</ENAMEX> name, but we always run into <ENAMEX TYPE="LAW">Zipf's law</ENAMEX> whenever we write down the rules for
        how the extraction is done (Figure <NUMEX TYPE="CARDINAL">2</NUMEX>) [<ENAMEX TYPE="LAW">3</ENAMEX>]. A small number of patterns describe a reasonable
        portion of protein–protein interactions, gene names, or mutations, but many of those
        <ENAMEX TYPE="ORG_DESC">entities</ENAMEX> are described by a pattern of words that's only ever used once. Even if we could
        collect them all—which is impossible—we can't stop new phrases from being used.
      
      
        <ENAMEX TYPE="ORGANIZATION">Curators—The Gold Standard</ENAMEX>
        Hand-curated data is precise, because the <ENAMEX TYPE="PER_DESC">curator</ENAMEX> is trained to inspect literature and
        databases, select only high-quality data, and reformat the facts according to the schema of
        the database. In addition, <ENAMEX TYPE="PER_DESC">curators</ENAMEX> select citations from the text as evidence for the
        identified fact, and those citations are also added to the database.
        <ENAMEX TYPE="PER_DESC">Curators</ENAMEX> read and interpret the text at the same time, and if they don't understand the
        meaning of a sentence, they can go back and pick a new strategy to analyse it—they can even
        call the <ENAMEX TYPE="PER_DESC">authors</ENAMEX> to iron out any ambiguities. <ENAMEX TYPE="PER_DESC">Curators</ENAMEX> can also cope with the high
        variability of language described by <ENAMEX TYPE="LAW">Zipf's law</ENAMEX>. At present, no computer-based system comes
        close to matching these capabilities. In particular, it is difficult to convert all the
        <ENAMEX TYPE="PER_DESC">curators</ENAMEX>' domain knowledge into a structured training set for the purposes of machine
        learning approaches.
        <ENAMEX TYPE="PERSON">Curators</ENAMEX> fulfil a second important task: they know how to define standards for data
        <ENAMEX TYPE="PERSON">consistency</ENAMEX>, in particular, the most relevant <ENAMEX TYPE="SUBSTANCE">terminology</ENAMEX>, which has led to the design of
        standardised ontologies and controlled vocabularies (see <ENAMEX TYPE="CONTACT_INFO">Box 1</ENAMEX> for an explanation of these
        and related terms). Examples of these include <ENAMEX TYPE="PERSON">Gene Ontology</ENAMEX> (GO; <ENAMEX TYPE="CONTACT_INFO">www.</ENAMEX><ENAMEX TYPE="ORGANIZATION">geneontology</ENAMEX>.<ENAMEX TYPE="CONTACT_INFO">org/</ENAMEX>),
        <ENAMEX TYPE="ORGANIZATION">Unified Medical Language System</ENAMEX> (<ENAMEX TYPE="CONTACT_INFO">www.</ENAMEX><ENAMEX TYPE="ORGANIZATION">nlm</ENAMEX>.<ENAMEX TYPE="ORGANIZATION">nih</ENAMEX>.<ENAMEX TYPE="CONTACT_INFO">gov/research/umls/</ENAMEX>), and MedDRA
        (<ENAMEX TYPE="CONTACT_INFO">www.</ENAMEX><ENAMEX TYPE="ORGANIZATION">meddramsso</ENAMEX>.<ENAMEX TYPE="CONTACT_INFO">com/NewWeb2003/</ENAMEX>index.<ENAMEX TYPE="CONTACT_INFO">htm</ENAMEX>) [<ENAMEX TYPE="LAW">4</ENAMEX>]. These terminological resources help to
        relate entries in bioinformatics databases to concepts mentioned in scientific publications
        and to link related information in databases using different schemas. Text <ENAMEX TYPE="PER_DESC">miners</ENAMEX> would
        love such standards to be used in text, but there is an understandable reluctance to impose
        and use standards that might limit the expressiveness of natural language.
      
      
        <ENAMEX TYPE="ORGANIZATION">Curation</ENAMEX> and <ENAMEX TYPE="ORGANIZATION">Text Mining—In Partnership</ENAMEX>
        The problem with curation of data is that it is time consuming and costly, and therefore
        has to focus on the most relevant facts. This compromises the completeness of the curated
        <ENAMEX TYPE="ORGANIZATION">data</ENAMEX>, and curation <ENAMEX TYPE="ORG_DESC">teams</ENAMEX> are doomed to stay behind the latest <ENAMEX TYPE="ORG_DESC">publications</ENAMEX>. So, is it
        possible for curation and text mining to work together for rapid retrieval and analysis of
        facts with precise postprocessing and standardisation of the extracted information?
        There are several software tools that perform well in the identification of standardised
        terms from the literature. Examples include <ENAMEX TYPE="ORGANIZATION">Textpresso</ENAMEX> and <ENAMEX TYPE="ORGANIZATION">Whatizit</ENAMEX> [<NUMEX TYPE="CARDINAL">5,6,7,8</NUMEX>]. Extensive
        term lists come from the <ENAMEX TYPE="ORGANIZATION">Human Genome Organization</ENAMEX> (<ENAMEX TYPE="CONTACT_INFO">www.</ENAMEX><ENAMEX TYPE="PERSON">gene</ENAMEX>.<ENAMEX TYPE="ORGANIZATION">ucl</ENAMEX>.<ENAMEX TYPE="ORGANIZATION">ac</ENAMEX>.<ENAMEX TYPE="CONTACT_INFO">uk/hugo</ENAMEX>; <NUMEX TYPE="CARDINAL">20,000</NUMEX> gene
        and <ENAMEX TYPE="SUBSTANCE">protein</ENAMEX> names), GO (<NUMEX TYPE="CARDINAL">almost 20,000</NUMEX> terms), Uniprot/<ENAMEX TYPE="NATIONALITY">Swiss</ENAMEX>-Prot
        (<ENAMEX TYPE="CONTACT_INFO">www.</ENAMEX><ENAMEX TYPE="ORGANIZATION">ebi</ENAMEX>.<ENAMEX TYPE="ORGANIZATION">uniprot</ENAMEX>.<ENAMEX TYPE="CONTACT_INFO">org/</ENAMEX>index.<ENAMEX TYPE="ORGANIZATION">shtml</ENAMEX>; <NUMEX TYPE="CARDINAL">about 200,000</NUMEX> terms), and other databases. In addition,
        terms describing diseases, syndromes, and <ENAMEX TYPE="SUBSTANCE">drugs</ENAMEX> are available from the <ENAMEX TYPE="ORGANIZATION">Unified Medical</ENAMEX>
        <ENAMEX TYPE="ORGANIZATION">Language System</ENAMEX>. Altogether, <NUMEX TYPE="CARDINAL">about 500,000</NUMEX> terms constitute the basis of domain knowledge
        in life sciences. To gain some perspective of this figure: an average individual handles
        <NUMEX TYPE="CARDINAL">2,000 to 20,000</NUMEX> terms in his or her <TIMEX TYPE="DATE">daily</TIMEX> language, and 
        <ENAMEX TYPE="ORGANIZATION">Merriam-Webster</ENAMEX>'s <ENAMEX TYPE="WORK_OF_ART">Collegiate Dictionary</ENAMEX> provides definitions for <NUMEX TYPE="CARDINAL">225,000</NUMEX>
        terms (<ENAMEX TYPE="CONTACT_INFO">www.</ENAMEX>merriam-webstercollegiate.<ENAMEX TYPE="CONTACT_INFO">com/</ENAMEX>).
        The identification of all terms by a text mining <ENAMEX TYPE="ORG_DESC">system</ENAMEX> still sets challenging demands.
        All variants of a term have to be taken into account, including syntactical variants and
        <ENAMEX TYPE="ORGANIZATION">synonyms</ENAMEX>. In the case of ambiguities, relevant findings have to be distinguished from other
        findings—a process referred to as disambiguation. Depending on the curation task, it might
        therefore be advantageous to select only part of the terminological resources and thus
        restrict the domain of the terminology to the <ENAMEX TYPE="PER_DESC">curators</ENAMEX>' needs (Figure <NUMEX TYPE="CARDINAL">3</NUMEX>).
        Available text mining solutions are concerned with named <ENAMEX TYPE="ORG_DESC">entity</ENAMEX> (NE) recognition
        (<ENAMEX TYPE="ORG_DESC">entities</ENAMEX> are, for example, <ENAMEX TYPE="SUBSTANCE">proteins</ENAMEX>, <ENAMEX TYPE="ANIMAL">species</ENAMEX>, and cell lines), with identification of
        relationships between <ENAMEX TYPE="ORGANIZATION">NEs</ENAMEX> (such as <ENAMEX TYPE="SUBSTANCE">protein interactions</ENAMEX>), and with the classification of
        text subphrases according to annotation schemata in general (<ENAMEX TYPE="SUBSTANCE">thyroid receptor</ENAMEX> is a thyroid
        <ENAMEX TYPE="SUBSTANCE">hormone receptor</ENAMEX>) [<NUMEX TYPE="CARDINAL">9,10,11,12,13,14,15</NUMEX>]. Whilst the identification of a curation <ENAMEX TYPE="ORG_DESC">team</ENAMEX>'s
        <ENAMEX TYPE="ORGANIZATION">terminology</ENAMEX> in the scientific text under scrutiny is immensely valuable, there is still a
        long way to go before this becomes routine.
      
      
        Some <ENAMEX TYPE="WORK_OF_ART">Immediate Challenges</ENAMEX>
        Not all terms used in the literature (NEs) can actually be found in some kind of
        <ENAMEX TYPE="PERSON">database</ENAMEX> (perhaps because of an <ENAMEX TYPE="PER_DESC">author</ENAMEX> error, or an alternative name for an <ENAMEX TYPE="ORG_DESC">entity</ENAMEX> adopted
        by the <ENAMEX TYPE="PER_DESC">community</ENAMEX>). Text mining methods therefore have to detect new terms and map the term
        to known terminology [<TIMEX TYPE="DATE">16</TIMEX>]. If several mappings are possible, the correct version has to be
        selected (disambiguation).
        Over <TIMEX TYPE="DATE">the past several years</TIMEX> text mining research <ENAMEX TYPE="ORG_DESC">teams</ENAMEX> have presented various approaches
        that train a software tool to locate representations of gene or <ENAMEX TYPE="SUBSTANCE">protein</ENAMEX> names (for example,
        BioCreative, <ENAMEX TYPE="CONTACT_INFO">www.</ENAMEX><ENAMEX TYPE="ORGANIZATION">pdg</ENAMEX>.<ENAMEX TYPE="ORGANIZATION">cnb</ENAMEX>.<ENAMEX TYPE="ORGANIZATION">uam</ENAMEX>.<ENAMEX TYPE="ORGANIZATION">es/BioLINK/BioCreative</ENAMEX>.<ENAMEX TYPE="ORGANIZATION">eval</ENAMEX>.html, and <ENAMEX TYPE="ORGANIZATION">JNLPBA</ENAMEX>,
        <ENAMEX TYPE="CONTACT_INFO">www.</ENAMEX><ENAMEX TYPE="ORGANIZATION">genisis</ENAMEX>.<ENAMEX TYPE="CONTACT_INFO">ch/~natlang/JNLPBA04/</ENAMEX>) [<NUMEX TYPE="CARDINAL">17,18</NUMEX>]. These tools are scored with a statistic known
        as the <ENAMEX TYPE="PRODUCT">F-</ENAMEX>measure, with the best methods scoring <NUMEX TYPE="CARDINAL">about 0.85</NUMEX>. At the level of <NUMEX TYPE="MONEY">0.85</NUMEX>, <ENAMEX TYPE="PER_DESC">curators</ENAMEX>
        still tend to be unhappy. However, analyses have shown that this score is in the range of
        curator–curator variation (unpublished data, measured as part of the project work for
        [<TIMEX TYPE="DATE">19</TIMEX>]), which suggests that such methods produce useful results.
        Additional information-extraction methods have been proposed, for example, for the
        documentation of mutations in specific genes and for the extraction of the subcellular
        location of <ENAMEX TYPE="SUBSTANCE">proteins</ENAMEX> [<NUMEX TYPE="CARDINAL">11,13</NUMEX>]. An even larger number of tools focus on the identification of
        appropriate terminology for the annotation of <ENAMEX TYPE="SUBSTANCE">genes</ENAMEX> (GO terms) [<ENAMEX TYPE="LAW">7</ENAMEX>]. The evaluation of their
        usefulness depends on the demands of the <ENAMEX TYPE="PER_DESC">user groups</ENAMEX>. Finally, another way to support
        <ENAMEX TYPE="ORGANIZATION">curation</ENAMEX> <ENAMEX TYPE="PER_DESC">teams</ENAMEX> would be to provide information-retrieval methods to guide the <ENAMEX TYPE="ORG_DESC">team</ENAMEX> members
        towards documents containing relevant information. For example, in <TIMEX TYPE="DATE">2002</TIMEX>, the <ENAMEX TYPE="GPE_DESC">participants</ENAMEX>
        in the <ENAMEX TYPE="PRODUCT">Knowledge Discovery</ENAMEX> and <ENAMEX TYPE="ORGANIZATION">Data-Mining Challenge Cup</ENAMEX>
        (<ENAMEX TYPE="CONTACT_INFO">www.</ENAMEX><ENAMEX TYPE="ORGANIZATION">cs</ENAMEX>.<ENAMEX TYPE="PERSON">cornell</ENAMEX>.<ENAMEX TYPE="CONTACT_INFO">edu/projects/kddcup/</ENAMEX>) had to select documents from a given corpus that
        contained relevant experimental results about 
        <ENAMEX TYPE="ORGANIZATION">Drosophila</ENAMEX> [<TIMEX TYPE="DATE">20</TIMEX>].
      
      
        How <ENAMEX TYPE="WORK_OF_ART">Can Publishers Contribute</ENAMEX>?
        For all automated information-extraction methods, it is obvious that access to
        literature is crucial. Electronic access has, of course, already had a huge impact, but the
        structure and organisation of manuscripts could also be improved. For example, semantic
        tags could be integrated into the text. The markup would not appear on web pages or when
        the document is printed, but it would help software to deal with semantic aspects of the
        document. Inserting tags, for example, to mark protein names would allow retrieval software
        to find documents about <ENAMEX TYPE="SUBSTANCE">proteins</ENAMEX> even if they look like common <ENAMEX TYPE="NATIONALITY">English</ENAMEX> words, such as “you”
        or “and”. Retrieval engines currently often ignore such terms. In addition, explicit tags
        would enable text mining methods, for example, when looking for protein–protein
        <ENAMEX TYPE="PERSON">interactions</ENAMEX>, to use the correct semantic interpretation.
        Text mining systems already available <TIMEX TYPE="DATE">today</TIMEX>, such as <ENAMEX TYPE="PERSON">Whatizit</ENAMEX>, can integrate semantic
        tags during submission, which have to be verified by the <ENAMEX TYPE="PER_DESC">author</ENAMEX>. Text mining is ready to
        deliver tools whereby information is passed back to the <ENAMEX TYPE="PER_DESC">authors</ENAMEX> about the proper use of
        <ENAMEX TYPE="ORGANIZATION">terminology</ENAMEX> within their documents. If the use of a term raises conflicts or ambiguities or
        if the use of a term is wrong, the <ENAMEX TYPE="PER_DESC">author</ENAMEX> is asked to provide feedback. The curation effort
        is resolved at the earliest possible time-point. <ENAMEX TYPE="PER_DESC">Author</ENAMEX>, <ENAMEX TYPE="PER_DESC">publisher</ENAMEX>, reviewer, and reader
        profit from consistent information representation, which leads to better dissemination of
        documents and <ENAMEX TYPE="ORG_DESC">journals</ENAMEX> and easily offsets the additional cost in the generation of an
        <ENAMEX TYPE="LAW">article</ENAMEX>. <ENAMEX TYPE="ORGANIZATION">Publishers</ENAMEX> and <ENAMEX TYPE="PER_DESC">authors</ENAMEX> have to agree on standards though.
      
      
        Is <ENAMEX TYPE="WORK_OF_ART">Text Mining Ready to Deliver</ENAMEX>?
        Text mining solutions have found their way into <TIMEX TYPE="DATE">daily</TIMEX> work, wherever fast and precise
        extraction of details from a large volume of text is needed. We have to keep in mind,
        however, that any text mining tool, just like other bioinformatics resources, will only be
        suitable for a limited number of tasks. For example, the same text may serve <ENAMEX TYPE="PER_DESC">curators</ENAMEX> from
        different <ENAMEX TYPE="PER_DESC">communities</ENAMEX> who extract different types of facts, depending on their domain
        knowledge. Furthermore, different <ENAMEX TYPE="GPE_DESC">communities</ENAMEX> have different expectations for accuracy. For
        example, <ENAMEX TYPE="PER_DESC">curators</ENAMEX> dealing with a small set of <ENAMEX TYPE="SUBSTANCE">proteins</ENAMEX> prefer tools with high recall,
        whereas <ENAMEX TYPE="PER_DESC">curators</ENAMEX> dealing with a large number of <ENAMEX TYPE="SUBSTANCE">proteins</ENAMEX> prefer tools with high
        <ENAMEX TYPE="ORGANIZATION">precision</ENAMEX>.
        Although text mining cannot dissect English sentences completely, and cannot extract the
        meaning and put the facts into a database, text mining tools are becoming increasingly used
        and valued. Text mining is ready to deliver handling of <ENAMEX TYPE="FAC_DESC">complex</ENAMEX> terminology and
        nomenclature as a mature service. It is only a matter of time and effort before we are able
        to extract facts automatically. The consequences are likely to be profound. Not only will
        we have a more effective approach for the mining of knowledge from the literature, our
        approach to the publication process itself might change. If a fact is clear enough for
        automatic extraction, it could be reported in a fact database instead of a publication. As
        methods improve, <ENAMEX TYPE="PER_DESC">authors</ENAMEX> will see more and more of their text being analysed and formalised
        in a database. If appropriate quality control is provided, and if <ENAMEX TYPE="PER_DESC">authors</ENAMEX> receive due
        credit for their deposition of facts into databases, we might well see a shift towards
        original papers describing new creative ideas and visions rather than just listing
        facts.
      
    
  
