
  
    
      
        
        Is <ENAMEX TYPE="PERSON">Michael Moore</ENAMEX> liberal <ENAMEX TYPE="GPE">America</ENAMEX>'s <ENAMEX TYPE="PERSON">Rush Limbaugh</ENAMEX>? If so, is he filling a much needed, or
        a much lamented, gap in turning issues that are really cast in pastel shades into <TIMEX TYPE="DATE">Day</TIMEX>-Glo
        relief? In this <ENAMEX TYPE="SUBSTANCE">hale monograph</ENAMEX>, <ENAMEX TYPE="PERSON">Jeff Hawkins</ENAMEX> (rendered <ENAMEX TYPE="PRODUCT">by Sandra Blakeslee</ENAMEX>) plays exactly
        this role for theoretical neuroscience. As a pastel practitioner myself, but furtively
        sharing many of <ENAMEX TYPE="PERSON">Hawkins</ENAMEX>' prejudices and hunches about computational modelling in
        neuroscience, I am caught between commendation and consternation.
        <ENAMEX TYPE="PERSON">Hawkins</ENAMEX> is an <ENAMEX TYPE="PER_DESC">engineer</ENAMEX>, <ENAMEX TYPE="PER_DESC">entrepreneur</ENAMEX>, and <ENAMEX TYPE="PER_DESC">scientist</ENAMEX> who founded and led the <ENAMEX TYPE="ORGANIZATION">companies</ENAMEX>
        <ENAMEX TYPE="GPE">Palm</ENAMEX> and then <ENAMEX TYPE="ORGANIZATION">Handspring</ENAMEX>. He created, against what must have been considerable obstacles,
        the first widely successful <ENAMEX TYPE="ORGANIZATION">PDA</ENAMEX>, and continued the development of this <ENAMEX TYPE="FAC_DESC">platform</ENAMEX>. He has
        thus amply earned a bully pulpit. The autobiographical segments of this book detail that,
        throughout his career, he has been interested in understanding how the brain works, using
        his substantial knowledge and intuition about the architecture and design of conventional
        computers as a counterpoint.
        More recently, <ENAMEX TYPE="PERSON">Hawkins</ENAMEX> has generously put his money where his ideas about mentation
        <ENAMEX TYPE="PERSON">dictate</ENAMEX>, founding the <ENAMEX TYPE="GPE">Redwood</ENAMEX> <ENAMEX TYPE="ORGANIZATION">Neuroscience Institute</ENAMEX> and also funding various conferences
        and workshops. The <ENAMEX TYPE="ORG_DESC">institute</ENAMEX> is dedicated to <NUMEX TYPE="MONEY">â</NUMEX>€˜studying and promoting biologically accurate
        mathematical models of memory and cognition.â€™Despite its <ENAMEX TYPE="PER_DESC">youth</ENAMEX>, the <ENAMEX TYPE="ORGANIZATION">Institute</ENAMEX> already has
        attracted notable attention as a centre for theoretical neuroscience. <ENAMEX TYPE="PERSON">Hawkins</ENAMEX>' quest,
        and-depending on which statements of the book you read-its <ENAMEX TYPE="SUBSTANCE">endpoint (â€˜â€¦</ENAMEX> a comprehensive
        theory of how the brain <ENAMEX TYPE="SUBSTANCE">works â€¦ describ</ENAMEX>[ing] what intelligence is and how your brain
        <ENAMEX TYPE="CONTACT_INFO">creates itâ€™</ENAMEX>) or just its tipping point (â€˜join me, along with others who take up the
        <ENAMEX TYPE="ORGANIZATION">challengeâ€™</ENAMEX>), are the subject here.
        There are really <NUMEX TYPE="CARDINAL">three</NUMEX> books jostling inside the covers. One is the (highly abbreviated)
        autobiography. The history of modern computing is very brief and (at least judging by the
        sales) very glorious, and this story is most entertaining. Don't miss the wonderfully faux
        naive letter from <ENAMEX TYPE="PERSON">Hawkins to Gordon Moore</ENAMEX> asking, in <TIMEX TYPE="DATE">1980</TIMEX>, to set up a research group
        within <ENAMEX TYPE="ORGANIZATION">Intel</ENAMEX> devoted to the brain. That <ENAMEX TYPE="PERSON">Hawkins</ENAMEX> prospered in clear opposition to accepted
        wisdom is perhaps one of the key subtexts of the book.
        The <NUMEX TYPE="ORDINAL">second</NUMEX>, and rather less satisfying, book is about the philosophy of mind and the
        history of artificial intelligence and neural network approaches to understanding the brain
        and replicating cognition. With respect to the fields of artificial intelligence and neural
        <ENAMEX TYPE="ORGANIZATION">nets</ENAMEX>, the text seems rather to be fighting <TIMEX TYPE="DATE">yesterday</TIMEX>'s battles. The importance of learning,
        flexibility in representation and inference, and even decentralisation of control has been
        more than amply recognised in the inexorable rise of probabilistic approaches in both
        fields.
        With respect to the philosophy of mind, there seems to be something of an <ENAMEX TYPE="PER_DESC">enthusiast</ENAMEX>'s
        disdain for the niceties of philosophical pettifogging, even arguing by assertion. The
        discussions at <TIMEX TYPE="DATE">the end</TIMEX> on creativity and consciousness all seem a bit gossamer. The book is
        somewhat careless about functionalism, a key doctrine for computational <ENAMEX TYPE="PER_DESC">theorists</ENAMEX> about how
        brains give rise to minds. According to this doctrine, at least <NUMEX TYPE="CARDINAL">roughly</NUMEX>, it is the
        functional roles of, and functional interactions among, the physical elements of brain that
        matter, and not their precise physical nature. If you can capture those functional aspects
        correctly, for instance, in a computer program, then you can (re-)create what's important
        about mental <ENAMEX TYPE="GPE_DESC">states</ENAMEX>. <ENAMEX TYPE="ORGANIZATION">Functionalism</ENAMEX> licenses a form of inquiry into the computational jobs
        played by <ENAMEX TYPE="FAC_DESC">structures</ENAMEX> in the brain. However, although formally agreeing that â€˜there's
        nothing inherently special or magical about the brain that allows it to be intelligent,â€™the
        book slips into statements such as â€˜brains and computers do fundamentally different
        things,â€™which are, at best, unfortunate shorthand.
        The book is a little apt to sneak plausible, but misleading, claims under the radar.
        Just to give one instance, it compellingly compares a <TIMEX TYPE="DATE">six year old</TIMEX> hopping from rock to
        <ENAMEX TYPE="ORGANIZATION">rock</ENAMEX> in a streambed with a lumbering robot failing to do the same task. However, this is a
        bit unfair. <NUMEX TYPE="CARDINAL">One</NUMEX> of <ENAMEX TYPE="PERSON">Hawkins</ENAMEX>' self-denying ordinances is to consider the cortex pretty much
        by itself. As aficionados of the cerebellum (an evolutionarily ancient brain region with a
        special role in the organisation of smooth, precise, well-timed, and task-sensitive motor
        output) would be quick to point out, the singular role for the cortex in such graceful
        <ENAMEX TYPE="ORGANIZATION">behaviour</ENAMEX> is rather questionable.
        The <NUMEX TYPE="ORDINAL">third</NUMEX> book is what I think is intended to be the real contribution. This contains a
        (not wholly convincing) attempt to conceptualise the definition of intelligence in terms of
        prediction rather than behaviour, and then to describe its possible instantiation in the
        <ENAMEX TYPE="ORGANIZATION">anatomy</ENAMEX> (and mostly only the anatomy) of the cortex.
      
      
        <ENAMEX TYPE="ORGANIZATION">Unsupervised Learning</ENAMEX>
        To situate <ENAMEX TYPE="PERSON">Hawkins</ENAMEX>' suggestions, it is instructive to consider current models of how the
        cerebral cortex represents, and learns to represent, information about the world without
        being explicitly taught. Being a popular account, the book fairly breezes by these
        so-called unsupervised learning models (see <ENAMEX TYPE="PERSON">Hinton</ENAMEX> and <ENAMEX TYPE="PRODUCT">Ghahramani</ENAMEX> <TIMEX TYPE="DATE">1997</TIMEX>; <ENAMEX TYPE="ORGANIZATION">Rao et</ENAMEX> al. <TIMEX TYPE="DATE">2002</TIMEX>),
        in which the neocortex is treated as a general device for finding relationships or
        structure in its input. The algorithms are called unsupervised since they have to work
        without detailed information from a <ENAMEX TYPE="PER_DESC">teacher</ENAMEX> or a <ENAMEX TYPE="PER_DESC">supervisor</ENAMEX> about the actual structure in
        each input. Rather, they must rely on general, statistical characteristics.
        First, where does the <ENAMEX TYPE="FAC_DESC">structure</ENAMEX> in the inputs come from? For the sake of concreteness,
        think of the input as being something like movies on a television screen. Movies don't look
        like white noise, or <NUMEX TYPE="MONEY">â</NUMEX>€˜snowâ€™, because of their statistical structure. For instance, in
        movies, pixel activities tend to change rather slowly over time, and pixels that are close
        to each other on the screen tend to have relatively similar activities at any given time.
        Neither of these is true of white noise. More technically, movies constitute only a tiny
        fraction of the space of all possible activations of all the pixels on your screen. They
        (and indeed real visual scenes) have a particular statistical structure that the cortex is
        supposed to extract.
        What is the cortex supposed to do with this <ENAMEX TYPE="FAC_DESC">structure</ENAMEX>? The idea is that the cortex
        learns to model, or <NUMEX TYPE="MONEY">â</NUMEX>€˜parameterizeâ€™, it. Then, the activities of cortical cells over time
        for a particular input, for example, a particular face in a movie, indicate the values of
        the parameters associated with that face. Thereby the cortical activities represent the
        input. The parameters for a face might include <NUMEX TYPE="CARDINAL">one</NUMEX> set for its physical structure (e.g.,
        the separation between the eyes and whether it is more round or more <ENAMEX TYPE="FAC_DESC">square</ENAMEX>), another set
        for the expression, and yet others, too.
        Cortical representations are thus intended to reflect directly the statistical structure
        in the input. <ENAMEX TYPE="PERSON">Importantly</ENAMEX>, for inputs such as movies, this <ENAMEX TYPE="FAC_DESC">structure</ENAMEX> is thought to be
        <ENAMEX TYPE="PERSON">hierarchical</ENAMEX> and, concomitantly, to provide an account of the observed hierarchical
        structure of sensory cortical areas. <NUMEX TYPE="CARDINAL">One</NUMEX> <ENAMEX TYPE="PER_DESC">source</ENAMEX> of hierarchical <ENAMEX TYPE="FAC_DESC">structure</ENAMEX> in movies is the
        simple fact that <ENAMEX TYPE="PER_DESC">objects</ENAMEX> (such as the faces) have parts (such as eyes and cheeks) whose
        form and changes in form over time are interdependent. Another source of hierarchical
        structure is that the same face can appear in many different poses, under many different
        forms of illumination, and so on. Pattern theory (<ENAMEX TYPE="PRODUCT">Grenander 1995</ENAMEX>), one of the parent
        disciplines of the field, calls these dimensions of variation deformations. <ENAMEX TYPE="PERSON">Loosely</ENAMEX>, the
        deformations are independent of the objects themselves, and we might expect this
        independence to be reflected in the cortical representations. Indeed, there is
        neurophysiological evidence for just such invariant neural responses to deformations of a
        <ENAMEX TYPE="PERSON">stimulus</ENAMEX>.
        How does the cortex do all this? Of course, some fraction of this <ENAMEX TYPE="FAC_DESC">structure</ENAMEX> was built in
        over evolution. However, the unsupervised learning tradition concentrates on ontogenic
        <ENAMEX TYPE="PERSON">adaptation</ENAMEX>, based on multiple presented input movies. An additional facet of the lack of
        <ENAMEX TYPE="ORGANIZATION">supervision</ENAMEX> is that this adaptation is taken as not depending on any particular behavioural
        task.
        Finally, what does this process allow the cortex to do? The whole representational
        structure is intended to support inference. Crudely, this involves turning partial or noisy
        inputs into the completed, cleaned-up patterns they imply, using connections between areas
        in the cortical hierarchy. Construed this way, probabilistic inference actually
        instantiates a very general form of computation. <ENAMEX TYPE="PERSON">Crucially</ENAMEX>, over the course of the
        development of unsupervised learning methods, it has been realised that the best way to
        approach the extraction of input structure, and inference with it, is through the language
        and tools of probability theory and statistics. The same realisation has driven substantial
        developments in artificial intelligence, machine learning, computer vision, and a host of
        other disciplines.
      
      
        <ENAMEX TYPE="ORGANIZATION">Predictive Auto-Association</ENAMEX>
        We can now return to the book. <ENAMEX TYPE="PERSON">Hawkins</ENAMEX> compactly sums up his thesis in the following
        <ENAMEX TYPE="PERSON">way</ENAMEX>. â€˜To make predictions of future events, your neocortex has to store <ENAMEX TYPE="ORG_DESC">sequences</ENAMEX> of
        patterns. To recall appropriate memories, it has to retrieve patterns by their similarity
        to past patterns (<ENAMEX TYPE="PRODUCT_DESC">auto</ENAMEX>-associative recall). And finally, memories have to be stored in an
        invariant form so that the knowledge of past events can be applied to new situations that
        are similar but not identical to the past.â€™In fact, to take the latter points <NUMEX TYPE="ORDINAL">first</NUMEX>, the
        sort of auto-associative storage and recall to which <ENAMEX TYPE="PERSON">Hawkins</ENAMEX> refers is a theoretically and
        practically hobbled version of unsupervised learning's probabilistic inference. Invariance
        is closely related to the deformations we described above in the context of pattern
        theory.
        Unsupervised learning has certainly paid substantial attention to sequences of inputs
        and prediction, and to some good effect. For instance, (artificial) speech recognition
        programs are based on a probabilistic device called a hidden <ENAMEX TYPE="PRODUCT">Markov</ENAMEX> <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX>, which is a key
        element in a wealth of unsupervised learning approaches to prediction. However, despite
        heroic efforts, these modelling methods are incapable of capturing the sort of complex
        structure seen in inputs such as natural languages. They fail on phenomena like
        long-distance dependencies, for example, the agreement between the cases of subjects and
        verbs, which are rife. This does tend to offer a vaccine against <ENAMEX TYPE="PERSON">Hawkins</ENAMEX>' otherwise
        infectious optimism.
        Once place in which <ENAMEX TYPE="PERSON">Hawkins</ENAMEX> goes beyond existing unsupervised learning <ENAMEX TYPE="PRODUCT_DESC">models</ENAMEX> is in an
        extension to actions and control, and in an ascription of parts of the <ENAMEX TYPE="PRODUCT_DESC">model</ENAMEX> to cortical
        <ENAMEX TYPE="ORGANIZATION">anatomy</ENAMEX>. The hierarchical conception of cortex here goes all the way down to primary motor
        <ENAMEX TYPE="ORGANIZATION">cortex</ENAMEX> (the neocortical area most directly associated with motor output). This allows
        auto-associative recall of sequences of past inputs and outputs to be used to specify
        actions that have formerly been successful. The discussion of this possibility is,
        unfortunately, rather brief. Central issues are omitted, such as the way that planning over
        multiple actions might happen. Also, the way that value is assigned to outcomes to
        determine success or failure is not discussed. The latter is widely believed to involve the
        <ENAMEX TYPE="ORGANIZATION">neuromodulatory</ENAMEX> systems that lie below the cortex and that the book's cortical chauvinism
        leads it cheerfully to ignore.
        By contrast, the book has a rather detailed description of how the model should map onto
        the anatomy of the cerebral cortex. Like many <ENAMEX TYPE="PER_DESC">unsupervised</ENAMEX> learning modellers, <ENAMEX TYPE="PERSON">Hawkins</ENAMEX> is a
        <ENAMEX TYPE="CONTACT_INFO">self-confessed â€˜lumperâ€™.</ENAMEX> He ignores huge swathes of complexity and specificity in cortical
        structure and <ENAMEX TYPE="FAC_DESC">connections</ENAMEX> in favour of a scheme of crystalline regularity. Though this will
        doubtless irk many <ENAMEX TYPE="PER_DESC">readers</ENAMEX> (as will the lack of citations to some influential prior
        <ENAMEX TYPE="PER_DESC">proponents</ENAMEX> such as <ENAMEX TYPE="PERSON">Douglas</ENAMEX> and <ENAMEX TYPE="PERSON">Martin</ENAMEX> [<TIMEX TYPE="DATE">1991</TIMEX>]), some (though not necessarily this) strong
        form of abstraction and omission is necessary to get to clear functional ideas. This part
        has interesting suggestions, such as a neat solution for a persistent dilemma for
        <ENAMEX TYPE="PER_DESC">proponents</ENAMEX> of hierarchical models. The battle comes between cases in which information in a
        higher cortical area, acting as prior information, boosts activities in a lower cortical
        area, and cases of predictive <ENAMEX TYPE="DISEASE">coding</ENAMEX>, in which the higher cortical area informs the lower
        <ENAMEX TYPE="GPE">cortical</ENAMEX> area about what it already knows and therefore suppresses the information that the
        lower area would otherwise just repeat up the hierarchy. The proposed solution involves the
        <ENAMEX TYPE="PERSON">invention</ENAMEX> (or rather prediction) of <NUMEX TYPE="CARDINAL">two</NUMEX> different sorts of neurons in a particular layer of
        <ENAMEX TYPE="ORGANIZATION">cortex</ENAMEX>.
        Unsupervised learning models of cortex are without doubt very elegant. However, if
        pushed, purveyors of this approach will often admit to being kept awake at <TIMEX TYPE="TIME">night</TIMEX> by a
        number of critical concerns even apart from the difficulty of getting the <ENAMEX TYPE="PRODUCT_DESC">models</ENAMEX> to work in
        interestingly rich sensory domains. Does the book provide computational <ENAMEX TYPE="PERSON">Halcyon</ENAMEX>? First, the
        <ENAMEX TYPE="ORGANIZATION">representations</ENAMEX> acquired by unsupervised learning are intended to be used for
        something-such as accomplishing more specific learning tasks, for example, making
        predictions of reward. However, most aspects of the statistical structure of inputs are
        irrelevant. This might be called the <ENAMEX TYPE="ANIMAL">â€˜carpetâ€™problem</ENAMEX>: there is a wealth of statistical
        structure in the visual texture of carpets; however, this <ENAMEX TYPE="FAC_DESC">structure</ENAMEX> is irrelevant for
        almost any task. Capturing it might therefore (a) constitute a terrible waste of cortical
        representational power, or, worse, (b) interfere with, or warp, the parameterization of the
        aspects of the input that are important, making it harder to extract critical distinctions.
        The book does not address this issue, relying on there being enough predictive power to
        capture any and all predictions, including predictive characterisation of motor
        <ENAMEX TYPE="ORGANIZATION">control</ENAMEX>.
        <NUMEX TYPE="ORDINAL">Second</NUMEX>, although our subjective sense is that we build a sophisticated predictive model
        of the entire sensory input, experiments into such phenomena as change blindness (Rensink
        <TIMEX TYPE="DATE">2002</TIMEX>) show this probably isn't true. A classic example involves alternating the
        presentation of <NUMEX TYPE="CARDINAL">two</NUMEX> pictures, which differ in some significant way (e.g., the colour of the
        trousers of <NUMEX TYPE="CARDINAL">one</NUMEX> of the main protagonists). <ENAMEX TYPE="PER_DESC">Subjects</ENAMEX> have great difficulty in identifying
        the difference between the pictures, even though (a) they are explicitly told to look for
        it, (b) they have the subjective sense that they have represented all the information in
        each picture, and (c) if the location of the change is pointed out, they see it as
        blindingly obvious. This, and other attentional phenomena, suggests that substantially less
        is actually represented than we might naively think. In fact, elaborate computations go
        into selecting aspects of the input to which the <ENAMEX TYPE="PER_DESC">models</ENAMEX> might be applied, and sophisticated
        models of these computations, such as <ENAMEX TYPE="PERSON">Li</ENAMEX>'s salience circuit (<TIMEX TYPE="DATE">2002</TIMEX>), involve aspects of
        cortical anatomy and physiology ignored in the book.
        As a final example of a spur to insomnia, unsupervised learners worry that Damasio
        (<TIMEX TYPE="DATE">1994</TIMEX>) might be somewhat right. That is, cool logic and hot emotion may be tightly coupled
        in a way that a model such as this that is rigidly confined to cortical processing,
        ignoring key subcortical contributions to practical decision making, will find hard to
        capture.
        To sum up, in terms of the adage that <ENAMEX TYPE="PER_DESC">genius</ENAMEX> is <NUMEX TYPE="PERCENT">1%</NUMEX> inspiration and <NUMEX TYPE="PERCENT">99%</NUMEX> perspiration, the
        book's enthymematic nature suggests that not quite enough sweat has been broken. Were it <NUMEX TYPE="PERCENT">1%</NUMEX>
        inspiration and <NUMEX TYPE="PERCENT">99%</NUMEX> aspiration, though, then the appealing call to arms for a new
        <ENAMEX TYPE="PER_DESC">generation</ENAMEX> of modellers should more than suffice.
      
    
  
