
   
   
      
         
             Cooking the <ENAMEX TYPE="ORGANIZATION">School Books</ENAMEX>

            

               
                  Click for a response to this article by the <ENAMEX TYPE="PER_DESC">editors</ENAMEX> of <ENAMEX TYPE="GPE">U.S.</ENAMEX>
News .
               
            

            According to the <TIMEX TYPE="DATE">annual</TIMEX> "<ENAMEX TYPE="WORK_OF_ART">America's Best Colleges</ENAMEX>"
issue of <ENAMEX TYPE="ORGANIZATION">U.S. News & World Report</ENAMEX> , published <TIMEX TYPE="DATE">Aug. 30</TIMEX>, the best
<ENAMEX TYPE="ORGANIZATION">college</ENAMEX> in the <ENAMEX TYPE="GPE">United States</ENAMEX> is the <ENAMEX TYPE="ORGANIZATION">California Institute of Technology</ENAMEX>. This
was dramatic, since <ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX>, while highly regarded, is not normally thought of
as No. <NUMEX TYPE="CARDINAL">1</NUMEX>. <TIMEX TYPE="DATE">Last year</TIMEX> <ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX> was rated <NUMEX TYPE="CARDINAL">No. 9</NUMEX>, while the top spot was an
<ENAMEX TYPE="ORGANIZATION">uninteresting</ENAMEX> three-way tie among <ENAMEX TYPE="ORGANIZATION">Harvard</ENAMEX>, <ENAMEX TYPE="ORGANIZATION">Yale</ENAMEX>, and <ENAMEX TYPE="GPE">Princeton</ENAMEX>.

            "Why does <ENAMEX TYPE="ORGANIZATION">U.S. News</ENAMEX> rank <ENAMEX TYPE="ORG_DESC">colleges</ENAMEX>?" asks
<ENAMEX TYPE="ORGANIZATION">U.S. News</ENAMEX> . The "simple answer," the <ENAMEX TYPE="ORG_DESC">magazine</ENAMEX> says, is, "We do it to help
you make one of the most important decisions of your life." Perhaps. Another
simple answer is that the annual <ENAMEX TYPE="ORG_DESC">college</ENAMEX> rankings (and similar rankings of
graduate <ENAMEX TYPE="ORG_DESC">schools</ENAMEX> and <ENAMEX TYPE="ORG_DESC">hospitals</ENAMEX>) are lucrative and influential unlike anything
else the No. <NUMEX TYPE="CARDINAL">3</NUMEX> newsmag does. Newsstand sales are almost double those of a
normal issue, and a paperback-book version sells <NUMEX TYPE="CARDINAL">a million</NUMEX> copies. <ENAMEX TYPE="ORG_DESC">Colleges</ENAMEX>
brag or complain loudly about their scores, enhancing the 'Snooze
either way.

            Whatever their validity as measures of academic
<ENAMEX TYPE="PERSON">excellence</ENAMEX>, the <TIMEX TYPE="DATE">annual</TIMEX> rankings are a brilliant gimmick for <ENAMEX TYPE="ORGANIZATION">U.S. News</ENAMEX> .
But there's a problem. A successful feature like this requires surprise, which
means volatility. Nobody's going to pay much attention if it's <ENAMEX TYPE="ORGANIZATION">Harvard</ENAMEX>, <ENAMEX TYPE="ORGANIZATION">Yale</ENAMEX>,
and <ENAMEX TYPE="GPE">Princeton</ENAMEX> again and again, <TIMEX TYPE="DATE">year after year</TIMEX>. Yet the relative merits of
<ENAMEX TYPE="GPE">America</ENAMEX>'s top <ENAMEX TYPE="ORG_DESC">universities</ENAMEX> surely change slowly, if at all. Naturally, <ENAMEX TYPE="GPE">U.S.</ENAMEX>
News does not just make up its ratings. It uses a weighted average of <NUMEX TYPE="CARDINAL">16</NUMEX>
numerical factors such as average <ENAMEX TYPE="PER_DESC">class</ENAMEX> size, acceptance rate (<TIMEX TYPE="TIME">fraction of</TIMEX>
<ENAMEX TYPE="PER_DESC">applicants</ENAMEX> who are admitted), and amount of <ENAMEX TYPE="PER_DESC">alumni</ENAMEX> giving. Trouble is, any
combination of these factors just isn't going to change enough from <TIMEX TYPE="DATE">year</TIMEX> to
<TIMEX TYPE="DATE">year</TIMEX> to keep things interesting.

            So how on earth can
<ENAMEX TYPE="ORGANIZATION">U.S. News</ENAMEX> explain <ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX>'s <TIMEX TYPE="DATE">one-year</TIMEX> rise?

            The <ENAMEX TYPE="ORG_DESC">magazine</ENAMEX> tries to deny that there's anything odd about
a college improving so quickly. The "best <ENAMEX TYPE="ORG_DESC">colleges</ENAMEX>" story argues: "<ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX> has
always been within striking distance of the top of the chart. In <TIMEX TYPE="DATE">1989</TIMEX>, <ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX>
was the No. <NUMEX TYPE="CARDINAL">3</NUMEX> school, ahead of <ENAMEX TYPE="ORGANIZATION">Harvard</ENAMEX>. ... <TIMEX TYPE="DATE">Last year</TIMEX>, <ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX> had the
<NUMEX TYPE="ORDINAL">fourth</NUMEX>-highest score among national <ENAMEX TYPE="ORG_DESC">universities</ENAMEX>." The first assertion is
irrelevant: We're not interested in <TIMEX TYPE="DATE">the 10-year</TIMEX> rise from <NUMEX TYPE="ORDINAL">third</NUMEX> but rather in
the <TIMEX TYPE="DATE">one-year</TIMEX> rise from <NUMEX TYPE="ORDINAL">ninth</NUMEX>. The <NUMEX TYPE="ORDINAL">second</NUMEX> assertion is technically true but
practically dishonest: <ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX> had the "<NUMEX TYPE="ORDINAL">fourth</NUMEX>-highest score" <TIMEX TYPE="DATE">last year</TIMEX> only
because there were <NUMEX TYPE="CARDINAL">two</NUMEX> three-way ties and <NUMEX TYPE="CARDINAL">one</NUMEX> two-way tie among the <NUMEX TYPE="CARDINAL">eight</NUMEX>
<ENAMEX TYPE="ORG_DESC">schools</ENAMEX> that beat it.

            But the real reason <ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX> jumped <NUMEX TYPE="CARDINAL">eight</NUMEX> spaces this
<TIMEX TYPE="DATE">year</TIMEX> is that the <ENAMEX TYPE="PER_DESC">editors</ENAMEX> at <ENAMEX TYPE="ORGANIZATION">U.S. News</ENAMEX> fiddled with the rules. The lead
story of the "best <ENAMEX TYPE="ORG_DESC">colleges</ENAMEX>" package says that a change in "methodology ...
helped" make <ENAMEX TYPE="ORGANIZATION">Caltech No. 1</ENAMEX>. Buried in a sidebar is the flat-out concession that
"[t]he effect of the change ... was to move [<ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX>] into <NUMEX TYPE="ORDINAL">first</NUMEX> place." No
"helped" about it. In other words, <ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX> didn't improve <TIMEX TYPE="DATE">this year</TIMEX>, and
<ENAMEX TYPE="ORGANIZATION">Harvard</ENAMEX>, <ENAMEX TYPE="ORGANIZATION">Yale</ENAMEX>, and <ENAMEX TYPE="GPE">Princeton</ENAMEX> didn't get any worse. If the rules hadn't changed,
<ENAMEX TYPE="ORGANIZATION">HYP</ENAMEX> would still be ahead. If the rules had changed <TIMEX TYPE="DATE">last year</TIMEX>, <ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX> would
have been on top <TIMEX TYPE="DATE">a year earlier</TIMEX>.

            (In fact, if the <ENAMEX TYPE="GPE">U.S.</ENAMEX>
News criteria are taken seriously, and if they held steady, <ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX> may
actually have slipped in quality <TIMEX TYPE="DATE">this past year</TIMEX>. Most indicators did not change
compared to <TIMEX TYPE="DATE">last year</TIMEX>. But graduation rate, number of <ENAMEX TYPE="PER_DESC">classes</ENAMEX> with fewer than
<NUMEX TYPE="CARDINAL">20</NUMEX> <ENAMEX TYPE="PER_DESC">students</ENAMEX>, and percentage of <ENAMEX TYPE="PER_DESC">faculty members</ENAMEX> who work full-time actually
declined . <NUMEX TYPE="CARDINAL">Only two</NUMEX> indicators showed small improvements: percentage of
accepted <ENAMEX TYPE="PER_DESC">students</ENAMEX> in top <NUMEX TYPE="PERCENT">10 percent</NUMEX> of their high-school <ENAMEX TYPE="PER_DESC">classes</ENAMEX> went from <NUMEX TYPE="CARDINAL">99</NUMEX>
<NUMEX TYPE="PERCENT">percent to 100 percent</NUMEX> [big deal!], and <ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX>'s acceptance rate fell from <NUMEX TYPE="CARDINAL">23</NUMEX>
percent to <NUMEX TYPE="PERCENT">18 percent</NUMEX>.)

            <ENAMEX TYPE="GPE">U. S. News</ENAMEX> denies that it changes the rules--as it
does <TIMEX TYPE="DATE">every year</TIMEX>--simply to change the results. <ENAMEX TYPE="PERSON">Robert Morse</ENAMEX>, <ENAMEX TYPE="ORGANIZATION">U.S. News</ENAMEX> '
<ENAMEX TYPE="ORGANIZATION">statistical</ENAMEX> <ENAMEX TYPE="PER_DESC">guru</ENAMEX>, explained to me that <TIMEX TYPE="DATE">this year</TIMEX>'s ranking procedures are an
"improvement" over <TIMEX TYPE="DATE">last year</TIMEX>'s. Doesn't that imply, I said, that <TIMEX TYPE="DATE">last year</TIMEX>'s
rankings were inferior? And shouldn't <ENAMEX TYPE="ORGANIZATION">U.S. News</ENAMEX> apologize to anyone who
made "one of the most important decisions of your life"--possibly turning down
<ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX> for <ENAMEX TYPE="GPE">Princeton</ENAMEX>--based on rankings the <ENAMEX TYPE="ORG_DESC">magazine</ENAMEX> itself now regards as
inaccurate? <ENAMEX TYPE="PERSON">Morse</ENAMEX> replied that he hadn't said the earlier ratings were
inferior. But if something improves, I pressed him, doesn't that mean that it
was less excellent before the improvement? Morse grudgingly allowed that I was
free to make that inference.

            I can't prove that
<ENAMEX TYPE="ORGANIZATION">U.S. News</ENAMEX> keeps changing the rules simply in order to change the
results. But if not, <ENAMEX TYPE="ORGANIZATION">U.S. News</ENAMEX> ought to shy away from <ENAMEX TYPE="ANIMAL">horse</ENAMEX>-race
headlines such as "<ENAMEX TYPE="WORK_OF_ART">Caltech Comes out on Top</ENAMEX>." A more honest summary might be
"<ENAMEX TYPE="WORK_OF_ART">We Finally Realize That Caltech Is Tops</ENAMEX>." Or "<ENAMEX TYPE="WORK_OF_ART">Caltech on Top (Until We Fiddle</ENAMEX>
With <ENAMEX TYPE="ORGANIZATION">Rules Again</ENAMEX>)."

            So, how did <ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX> come out on top? Well, one
variable in a <ENAMEX TYPE="ORG_DESC">school</ENAMEX>'s ranking has long been educational expenditures per
student, and <ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX> has traditionally been tops in this category. But until
<TIMEX TYPE="DATE">this year</TIMEX>, <ENAMEX TYPE="ORGANIZATION">U.S. News</ENAMEX> considered only a <ENAMEX TYPE="ORG_DESC">school</ENAMEX>'s ranking in this
category--<NUMEX TYPE="ORDINAL">first</NUMEX>, <NUMEX TYPE="ORDINAL">second</NUMEX>, etc.--rather than how much it spent relative to other
<ENAMEX TYPE="ORG_DESC">schools</ENAMEX>. It didn't matter whether <ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX> beat <ENAMEX TYPE="ORGANIZATION">Harvard</ENAMEX> by <NUMEX TYPE="MONEY">$1</NUMEX> or by <NUMEX TYPE="MONEY">$100,000</NUMEX>.
<NUMEX TYPE="CARDINAL">Two</NUMEX> other <ENAMEX TYPE="ORG_DESC">schools</ENAMEX> that rose in their rankings <TIMEX TYPE="DATE">this year</TIMEX> were <ENAMEX TYPE="ORGANIZATION">MIT</ENAMEX> (from <NUMEX TYPE="ORDINAL">fourth</NUMEX>
to <NUMEX TYPE="ORDINAL">third</NUMEX>) and <ENAMEX TYPE="ORGANIZATION">Johns Hopkins</ENAMEX> (from <NUMEX TYPE="MONEY">14 th</NUMEX> to <NUMEX TYPE="ORDINAL">seventh</NUMEX>). <NUMEX TYPE="CARDINAL">All three</NUMEX> have
high per-student expenditures and <NUMEX TYPE="CARDINAL">all three</NUMEX> are especially strong in the hard
<ENAMEX TYPE="ORGANIZATION">sciences</ENAMEX>. <ENAMEX TYPE="ORG_DESC">Universities</ENAMEX> are allowed to count their research budgets in their
per-student expenditures, though <ENAMEX TYPE="PER_DESC">students</ENAMEX> get no direct benefit from costly
research their <ENAMEX TYPE="PER_DESC">professors</ENAMEX> are doing outside of class.

            In its "best <ENAMEX TYPE="ORG_DESC">colleges</ENAMEX>"
issue <TIMEX TYPE="DATE">two years ago</TIMEX>, <ENAMEX TYPE="ORGANIZATION">U.S. News</ENAMEX> made precisely this point, saying it
considered only the rank ordering of per-student expenditures, rather than the
actual amounts, on the grounds that "expenditures at <ENAMEX TYPE="ORG_DESC">institutions</ENAMEX> with large
research programs and medical <ENAMEX TYPE="ORG_DESC">schools</ENAMEX> are substantially higher than those at
the rest of the <ENAMEX TYPE="ORG_DESC">schools</ENAMEX> in the category." In other words, <TIMEX TYPE="DATE">just two years ago</TIMEX>,
the <ENAMEX TYPE="ORG_DESC">magazine</ENAMEX> felt it unfair to give <ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX>, <ENAMEX TYPE="ORGANIZATION">MIT</ENAMEX>, and <ENAMEX TYPE="ORGANIZATION">Johns Hopkins</ENAMEX> credit for
having lots of fancy <ENAMEX TYPE="FAC_DESC">laboratories</ENAMEX> that don't actually improve undergraduate
<ENAMEX TYPE="ORGANIZATION">education</ENAMEX>.

            Each of <ENAMEX TYPE="ORGANIZATION">U.S. News</ENAMEX> ' criteria can generate a quibble
like this one. But there is a larger philosophical flaw in the "best <ENAMEX TYPE="ORG_DESC">colleges</ENAMEX>"
rankings. Consider this analogy: Suppose you wanted to rank baseball <ENAMEX TYPE="PER_DESC">teams</ENAMEX>. You
might choose some plausible criteria such as <ENAMEX TYPE="PER_DESC">players</ENAMEX>' lifetime batting averages
and salaries, the <ENAMEX TYPE="PER_DESC">coaches</ENAMEX>' years of professional experience, and so on. To
decide whether these criteria were valid, and what relative weights to give
them, you would look at the figures for winning and losing <ENAMEX TYPE="PER_DESC">teams</ENAMEX> of the past.
Because you know which <ENAMEX TYPE="PER_DESC">teams</ENAMEX> are successful before you begin your
analysis--those that win--you can use mathematics to identify similarities
among those winning <ENAMEX TYPE="PER_DESC">teams</ENAMEX>.

            But with the <ENAMEX TYPE="GPE">U.S.</ENAMEX>
<ENAMEX TYPE="ORGANIZATION">News</ENAMEX> rankings there is no objective way to know which <ENAMEX TYPE="ORG_DESC">schools</ENAMEX> are winners
before you begin your analysis. In fact, determining the <ENAMEX TYPE="ORG_DESC">winners</ENAMEX> is the point
of the exercise. So you sit around and brainstorm about whether faculty
<ENAMEX TYPE="ORGANIZATION">resources</ENAMEX> (<ENAMEX TYPE="PER_DESC">class</ENAMEX> size, <ENAMEX TYPE="PER_DESC">faculty</ENAMEX> salaries, etc.) or student graduation rates, for
instance, are more important to educational "quality." Right now, <ENAMEX TYPE="GPE">U.S.</ENAMEX>
News gives the two characteristics equal weight, which seems reasonable.
But if you told me that faculty resources are twice as important as student
graduation rates, that would seem reasonable too.

            <ENAMEX TYPE="PERSON">Mel Elfin</ENAMEX>, the retired <ENAMEX TYPE="ORGANIZATION">U.S. News</ENAMEX> <ENAMEX TYPE="PER_DESC">editor</ENAMEX> who more or
less created the current college rankings, explained to <ENAMEX TYPE="ORGANIZATION">Lingua Franca</ENAMEX> :
"We've come up with a list that underscores intuitive judgments. We did not set
out to underscore [those] judgments; we set out with a methodology. That it
wound up this way is to me both a justification and a discovery that we're on
the right track." This is a masterpiece of circular logic. <ENAMEX TYPE="ORGANIZATION">Elfin</ENAMEX> is saying: <ENAMEX TYPE="CONTACT_INFO">1</ENAMEX>)
We trust our methodology because it confirms our intuition; and <NUMEX TYPE="CARDINAL">2</NUMEX>) we are
confirmed in our intuition because it is supported by our methodology.

            And the truth is that the rankings' success actually
depends on confounding most <ENAMEX TYPE="PER_DESC">people</ENAMEX>'s intuition. For example, by declaring that
<ENAMEX TYPE="ORGANIZATION">Caltech</ENAMEX> is superior to <ENAMEX TYPE="ORGANIZATION">Harvard</ENAMEX>, <ENAMEX TYPE="ORGANIZATION">Yale</ENAMEX>, <ENAMEX TYPE="ORGANIZATION">Stanford</ENAMEX>, <ENAMEX TYPE="GPE">Princeton</ENAMEX>, <ENAMEX TYPE="ORGANIZATION">MIT</ENAMEX>, and so on. And
why should <ENAMEX TYPE="PER_DESC">people</ENAMEX> find that so hard to believe? Maybe because you told them the
opposite just <TIMEX TYPE="DATE">last year</TIMEX>.

         
      
   
