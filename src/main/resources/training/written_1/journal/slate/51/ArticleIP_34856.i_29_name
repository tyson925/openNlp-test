
   
   
      
         
             Too <ENAMEX TYPE="WORK_OF_ART">True To Be Good</ENAMEX>

            Not everything you read on the <ENAMEX TYPE="ORGANIZATION">World Wide Web</ENAMEX> is
true. Not everything you read in the <ENAMEX TYPE="ORGANIZATION">New York Times</ENAMEX> is true, either. So
when you read about scientific breakthroughs, how do you know what to
believe?

            Partly, you trust your instincts: A theory that
life evolved from <ENAMEX TYPE="SUBSTANCE">clay</ENAMEX> is more inherently plausible than a theory that life
evolved from <ENAMEX TYPE="EVENT">Play-Doh</ENAMEX>. Partly, you consider the <ENAMEX TYPE="ORG_DESC">source</ENAMEX>: A <ENAMEX TYPE="ORGANIZATION">Harvard</ENAMEX> <ENAMEX TYPE="PER_DESC">professor</ENAMEX> is
more credible than a <ENAMEX TYPE="ORGANIZATION">Dartmouth</ENAMEX> dishwasher. And partly you rely on expert
judgments: If a prestigious <ENAMEX TYPE="ORG_DESC">journal</ENAMEX> has agreed to publish the clay theory, it's
probably wrong.

            Yes, I meant to say that: If a prestigious
<ENAMEX TYPE="ORGANIZATION">journal</ENAMEX> publishes a theory, it's probably wrong . Given <NUMEX TYPE="CARDINAL">two</NUMEX> equally
plausible theories from equally credible <ENAMEX TYPE="PER_DESC">sources</ENAMEX> that have passed equally
strict scrutiny, the one that makes it into a top <ENAMEX TYPE="ORG_DESC">journal</ENAMEX> has a smaller
chance of being right. Here's why: <ENAMEX TYPE="PER_DESC">Editors</ENAMEX> like to publish theories they find
surprising. And the best way to surprise an <ENAMEX TYPE="PER_DESC">editor</ENAMEX> is to be wrong.

            That's not to say that
<ENAMEX TYPE="PER_DESC">editors</ENAMEX> are reckless. At least in mathematics and economics (the <NUMEX TYPE="CARDINAL">two</NUMEX> fields
where I can testify from personal experience), the editorial process is
rigorously demanding. Long before an article is submitted for <ENAMEX TYPE="ORG_DESC">publication</ENAMEX>, the
<ENAMEX TYPE="PER_DESC">author</ENAMEX> is expected to circulate drafts among <ENAMEX TYPE="PER_DESC">experts</ENAMEX> in the field and to
respond to their <ENAMEX TYPE="PER_DESC">criticisms</ENAMEX> and comments--a process that typically takes <TIMEX TYPE="DATE">years</TIMEX>.
Only then is the (now heavily revised) article formally submitted, whereupon
the <ENAMEX TYPE="PER_DESC">editor</ENAMEX> handpicks an <ENAMEX TYPE="PER_DESC">expert referee</ENAMEX> to examine it line by line--a process
that can easily take <TIMEX TYPE="DATE">another year</TIMEX> or more. Are <ENAMEX TYPE="PER_DESC">referees</ENAMEX> ever lax and careless?
Surely. Are they lax and careless with articles of genuine importance? In my
<ENAMEX TYPE="PERSON">observation</ENAMEX>, essentially never. Through multiple rounds of correspondence,
referees demand satisfaction regarding every important detail. In many cases,
the <ENAMEX TYPE="PER_DESC">author</ENAMEX> will visit the <ENAMEX TYPE="PER_DESC">referee</ENAMEX>'s home <ENAMEX TYPE="ORG_DESC">institution</ENAMEX> for a <TIMEX TYPE="DATE">semester</TIMEX> or a year
to be available for periodic grilling.

            That's exactly what's so damning about the hoax perpetrated
in <TIMEX TYPE="DATE">1996</TIMEX> by <ENAMEX TYPE="PERSON">Alan Sokal</ENAMEX>. <ENAMEX TYPE="ORGANIZATION">Sokal</ENAMEX>'s paper, intentionally stripped of logic,
evidence, and even meaning, was accepted for <ENAMEX TYPE="ORG_DESC">publication</ENAMEX> in the cultural
studies journal <ENAMEX TYPE="ORGANIZATION">Social Text</ENAMEX> . True, this was a one-time event, but it was
an event so far removed from anything that could possibly occur in a legitimate
academic enterprise that it converted agnostics like me, who had doubted the
status of cultural studies as an intellectual discipline, into hard-core cynics
with no doubt whatsoever.

            In a serious economics
<ENAMEX TYPE="ORGANIZATION">journal</ENAMEX>, it would be impossible to publish an article like <ENAMEX TYPE="ORGANIZATION">Sokal's</ENAMEX>. But it
would not be impossible, or even unusual, to publish a carefully reasoned
<ENAMEX TYPE="LAW">article</ENAMEX> that's still wrong. That's because of the bias I mentioned earlier:
Given <NUMEX TYPE="CARDINAL">two</NUMEX> <ENAMEX TYPE="ORG_DESC">papers</ENAMEX> that have both survived the vetting process, <ENAMEX TYPE="PER_DESC">editors</ENAMEX> tend to
prefer the more surprising, which means that on average they prefer the one
that's wrong.

            It's easy to see how the same dynamic could work at a
newspaper. "Man bites dog" is a better story than "dog bites <ENAMEX TYPE="PER_DESC">man</ENAMEX>," but it's
also more likely to be wrong, even if both stories are reported by equally
reliable <ENAMEX TYPE="PER_DESC">witnesses</ENAMEX>. In general--and this observation is a mainstay of college
statistics <ENAMEX TYPE="FAC_DESC">courses</ENAMEX>--when you think you've seen something unusual, you're more
likely to be mistaken than when you think you've seen something ordinary. But
it's the unusual that makes the front page.

            <TIMEX TYPE="DATE">A few years ago</TIMEX>,
<ENAMEX TYPE="ORGANIZATION">economics</ENAMEX> <ENAMEX TYPE="PER_DESC">professors</ENAMEX> <ENAMEX TYPE="PERSON">J. Bradford De Long</ENAMEX> and <ENAMEX TYPE="PERSON">Kevin Lang</ENAMEX> devised an ingenious
<ENAMEX TYPE="PERSON">way</ENAMEX> to determine just how many published economic hypotheses are actually true.
They looked through <TIMEX TYPE="DATE">several years</TIMEX>' worth of issues of the top economics
<ENAMEX TYPE="ORGANIZATION">journals</ENAMEX> and found <NUMEX TYPE="CARDINAL">78</NUMEX> hypotheses that were confirmed by strong evidence--the
sort of evidence that led the <ENAMEX TYPE="PER_DESC">authors</ENAMEX> to accept their own hypotheses. (Another
<NUMEX TYPE="CARDINAL">198</NUMEX> hypotheses were rejected by their <ENAMEX TYPE="PER_DESC">authors</ENAMEX>.) In exactly none of the <NUMEX TYPE="CARDINAL">78</NUMEX> cases
could the confirmation be called overwhelming.

            But that's OK. Strong evidence is, after all, strong
evidence, even when it's a little shy of overwhelming. In most cases,
overwhelming evidence is too much to ask for, because evidence can be hard to
collect and hard to interpret. So no individual article can be criticized for
failing to live up to an unattainable standard.

            But, said <ENAMEX TYPE="PERSON">De Long</ENAMEX> and
<ENAMEX TYPE="PERSON">Lang</ENAMEX>, out of <NUMEX TYPE="CARDINAL">78</NUMEX> true hypotheses, surely there should be at least a few
that are overwhelmingly confirmed. In fact, they gave a precise definition of
the word overwhelming, according to which <NUMEX TYPE="PERCENT">roughly 10 percent</NUMEX> of all true
<ENAMEX TYPE="ORGANIZATION">hypotheses</ENAMEX> should come packaged with overwhelming evidence. So if all <NUMEX TYPE="CARDINAL">78</NUMEX>
hypotheses are true, then <NUMEX TYPE="CARDINAL">roughly 7.8</NUMEX> of them--call it <NUMEX TYPE="CARDINAL">eight</NUMEX>-- should be
confirmed overwhelmingly. And they're not.

            OK, so maybe that's because not all <NUMEX TYPE="CARDINAL">78</NUMEX> are true. Maybe only
<NUMEX TYPE="CARDINAL">50</NUMEX> are true. In that case, <NUMEX TYPE="CARDINAL">five</NUMEX> should be confirmed overwhelmingly. Or maybe
<NUMEX TYPE="CARDINAL">only 30</NUMEX> are true, in which case <NUMEX TYPE="CARDINAL">three</NUMEX> should be confirmed overwhelmingly. The
problem is that exactly <NUMEX TYPE="CARDINAL">zero</NUMEX> are confirmed overwhelmingly, and zero is
<NUMEX TYPE="PERCENT">10 percent</NUMEX> of--<NUMEX TYPE="CARDINAL">zero</NUMEX>! So out of <NUMEX TYPE="CARDINAL">78</NUMEX> "confirmed" hypotheses, it seems that
<NUMEX TYPE="PERCENT">approximately</NUMEX> <NUMEX TYPE="CARDINAL">zero</NUMEX> are true.

            Using a more sophisticated version of the same
techniques, <ENAMEX TYPE="PERSON">De Long</ENAMEX> and <ENAMEX TYPE="PERSON">Lang</ENAMEX> concluded that some of the <TIMEX TYPE="DATE">78</TIMEX> "confirmed"
<ENAMEX TYPE="ORGANIZATION">hypotheses</ENAMEX> might be true, but probably not <NUMEX TYPE="CARDINAL">more than about a third</NUMEX> of them. In
other words, when a published article in a top <ENAMEX TYPE="ORG_DESC">journal</ENAMEX> presents evidence that
its hypothesis is true, its hypothesis is probably false. It would be very
interesting to perform the same experiment with, say, medical <ENAMEX TYPE="ORG_DESC">journals</ENAMEX> instead
of economics <ENAMEX TYPE="ORG_DESC">journals</ENAMEX>. I'd be very surprised if the results were substantially
different.

            If this makes you feel pessimistic about the
progress of science, keep in mind that we can learn a lot from even a very few
true hypotheses submerged in a sea of false ones. And here's another ray of
hope: <ENAMEX TYPE="PERSON">De Long</ENAMEX> and <ENAMEX TYPE="PERSON">Lang</ENAMEX>'s results were published in the prestigious <ENAMEX TYPE="ORGANIZATION">Journal</ENAMEX>
of <ENAMEX TYPE="ORGANIZATION">Political Economy</ENAMEX> , so they're probably wrong to begin with. And this
account of them was published in 
                  Slate
               , so it's probably wrong,
too.

         
      
   
